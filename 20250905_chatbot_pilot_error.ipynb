{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹æ•´é«”çš„uiä»‹é¢ï¼Œè®Šæˆä¸€å€‹å•ç­”æ©Ÿå™¨äºº\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA,  ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain-community pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "ç›®çš„ï¼šå»ºç«‹langchain + openai çš„åŸºç¤ç’°å¢ƒ\n",
    "'''\n",
    "import os # ä½œæ¥­ç³»çµ±ç›¸é—œåŠŸèƒ½ï¼ˆè®€å–ç’°å¢ƒè®Šæ•¸ï¼‰\n",
    "from openai import OpenAI # openai api å®¢æˆ¶ç«¯\n",
    "from dotenv import load_dotenv, find_dotenv # dotenv æ˜¯å°ˆé–€ç”¨ä¾†è®€å–.envå¥—ä»¶çš„å¥—ä»¶ï¼Œä¸¦æ¥ä¸Šç’°å¢ƒ\n",
    "_ = load_dotenv(find_dotenv()) # è®€å–.envæª”æ¡ˆ\n",
    "client = OpenAI(\n",
    "    api_key=os.environ['OPENAI_API_KEY']\n",
    ")\n",
    "# openai.api_key  = os.environ['OPENAI_API_KEY'] èˆŠç‰ˆ\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬ä¸€æ­¥æ¸¬è©¦ï¼šè¼‰å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Success! API key loaded\n",
      "Key starts with: sk-proj-FWUUter...\n",
      "Key length: 164 characters\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# æ¸…é™¤èˆŠçš„ç’°å¢ƒè®Šæ•¸\n",
    "if 'OPENAI_API_KEY' in os.environ:\n",
    "    del os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# é‡æ–°è¼‰å…¥\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.environ.get('OPENAI_API_KEY')\n",
    "if api_key:\n",
    "    print(\"âœ… Success! API key loaded\")\n",
    "    print(f\"Key starts with: {api_key[:15]}...\")\n",
    "    print(f\"Key length: {len(api_key)} characters\")\n",
    "else:\n",
    "    print(\"âŒ Still not working\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder exists: True\n",
      "Found 23 PDF files:\n",
      "  - 2023_LLM limitation_Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions.pdf\n",
      "  - 2025_LLM limitation_The Order Effect- Investigating Prompt Sensitivity to Input Order in LLMs.pdf\n",
      "  - 2024_RAG_Evaluation of Retrieval-Augmented Generation- A Survey.pdf\n",
      "  - 2020_scaling laws_Scaling Laws for Neural Language Models.pdf\n",
      "  - 2022_LLM limitation_Robustness of Learning from Task Instructions.pdf\n"
     ]
    }
   ],
   "source": [
    "# æª¢æŸ¥è³‡æ–™å¤¾æ˜¯å¦å­˜åœ¨\n",
    "import os\n",
    "folder_path = \"/Users/mangtinglee/Desktop/2025_gap_careerpath/RAG_LLM/pdfs\"\n",
    "\n",
    "print(f\"Folder exists: {os.path.exists(folder_path)}\")\n",
    "# çœ‹çœ‹æœ‰å“ªäº›PDFæª”æ¡ˆ\n",
    "pdf_files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
    "print(f\"Found {len(pdf_files)} PDF files:\")\n",
    "for pdf in pdf_files[:5]:  # é¡¯ç¤ºå‰5å€‹æª”å\n",
    "    print(f\"  - {pdf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test file loaded: 11 pages\n",
      "å…§å®¹ï¼šFigure 2: Order sensitivity in few-shot setting:The error bars represent the range of minimum and maximum\n",
      "accuracy achievable in each task through oracle reordering. Our observations are as follows: (1) The sensitivity\n",
      "gap consistently remains substantial even with the addition of more demonstrations in the few-shot setting. (2) As\n",
      "performances improve, the sensitivity gap shrinks. (3) Adding more demonstrations does not necessarily result in a\n",
      "reduction of the sensitivity gap.\n",
      "4 Why Do LLMs Sho...\n",
      "metadataï¼š{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-23T01:15:54+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-23T01:15:54+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/mangtinglee/Desktop/2025_gap_careerpath/RAG_LLM/pdfs/2023_LLM limitation_Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "# æ¸¬è©¦1.è¼‰å…¥ä¸€ç¯‡\n",
    "test_file = os.path.join(folder_path, pdf_files[0])\n",
    "loader = PyPDFLoader(test_file)\n",
    "documents = loader.load()\n",
    "print(f\"Test file loaded: {len(documents)} pages\")\n",
    "print(f\"å…§å®¹ï¼š{documents[3].page_content[:500]}...\")\n",
    "print(f\"metadataï¼š{documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬äºŒæ­¥æ¸¬è©¦ï¼šåˆ†å‰²æª”æ¡ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# æ¸¬è©¦2. åˆ†å‰²æª”æ¡ˆ\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=150,\n",
    "    separators=[ \"\\n\\n\", \". \", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
    "    )    \n",
    "docs = text_splitter.split_documents(documents)\n",
    "print(len(docs)) # chunks\n",
    "print(len(documents)) # test 11 pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Language Models Sensitivity to The Order of Options in\n",
      "Multiple-Choice Questions\n",
      "Pouya Pezeshkpour\n",
      "Megagon Labs\n",
      "pouya@megagon.ai\n",
      "Estevam Hruschka\n",
      "Megagon Labs\n",
      "estevam@megagon.ai\n",
      "Abstract\n",
      "Large Language Models (LLMs) have demon-\n",
      "strated remarkable capabilities in various NLP\n",
      "tasks. However, previous works have shown\n",
      "these models are sensitive towards prompt\n",
      "wording, and few-shot demonstrations and\n",
      "their order, posing challenges to fair assess-\n",
      "ment of these models. As these models be-\n",
      "come more powerful, it becomes imperative\n",
      "to understand and address these limitations.\n",
      "In this paper, we focus on LLMs robust-\n",
      "ness on the task of multiple-choice questionsâ€”\n",
      "commonly adopted task to study reasoning and\n",
      "fact-retrieving capability of LLMs\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "separators=[\n",
    "    \"\\n\\n\",        # æ®µè½åˆ†éš”ï¼ˆæœ€å„ªå…ˆï¼‰\n",
    "    \". \",          # å¥è™Ÿ\n",
    "    \"\\n\",          # è¡Œåˆ†éš”\n",
    "    \"(?<=\\. )\",    # å¥è™Ÿå¾Œåˆ†éš”ï¼ˆç”¨æ­£å‰‡è¡¨é”å¼ï¼‰\n",
    "    \" \",           # ç©ºæ ¼åˆ†éš”\n",
    "    \"\"             # å­—å…ƒåˆ†éš”ï¼ˆæœ€å¾Œæ‰‹æ®µï¼‰\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### å•é¡Œï¼šå¦‚ä½•æ±ºå®šå¡Šçš„å¤§å°å’Œé‡ç–Šåº¦ï¼Ÿ\n",
    "- å…©ç¨®æŒ‡æ¨™ï¼š\n",
    "    - çµ±è¨ˆæŒ‡æ¨™ï¼š\n",
    "        - å¹³å‡é•·åº¦ï¼šæ¥è¿‘è¨­å®šçš„ `chunk _size`ï¼Œä»£è¡¨æ¥è¿‘ç›®æ¨™ \n",
    "        - maxï¼šè©•ä¼°æ˜¯å¦è¶…éé™åˆ¶\n",
    "        - minï¼šè©•ä¼°æœ€å°å¡Šæ˜¯å¦èƒ½æ¥å—\n",
    "        - stdï¼šchunkçš„å¤§å°ç¯„åœå¤§æ¦‚è½åœ¨ mean Â± std ä¹‹é–“ï¼Œæœ‰é»å¤§çš„è©±å¤§æ¦‚é•·åº¦ä¸å¤ªä¸€è‡´ï¼Œ<200 (é•·åº¦è¼ƒä¸€è‡´)\n",
    "        - ä¸€å€‹æœ€çŸ­çš„chunkï¼šé€šå¸¸è¶Šå°‘è¶Šå¥½ï¼Œ<5%  \n",
    "\n",
    "    - èªæ„å®Œæ•´æ€§ï¼ˆæ›´é‡è¦ï¼‰ï¼š\n",
    "        - ä¸€å€‹å¡Šæ˜¯å¦åŒ…å«å®Œæ•´æ¦‚å¿µï¼Ÿ\n",
    "        - é‡è¦è¡“èªæ˜¯å¦æœ‰è¢«åˆ‡æ–·ï¼Ÿ\n",
    "        - è©•ä¼°æ–¹æ³•ï¼šäººå·¥æª¢è¦–ï¼ˆçœ‹å®Œæ–‡å­—æ˜¯å¦èƒ½ç†è§£ï¼‰ã€å•ç­”æ¸¬è©¦æ³•ï¼ˆå…ˆæ¸¬è©¦åˆ°æœ€å¾Œï¼Œé aiæª¢ç´¢æ˜¯å¦èƒ½å¾æº–ç¢ºå›æ‡‰ç­”æ¡ˆï¼‰\n",
    "- çµè«–ï¼šç¾åœ¨çœ‹çœ‹ä¸æº–ï¼Œè¦ç­‰runéä¸€éå†ä¾†çœ‹llmçš„æª¢ç´¢èƒ½åŠ›\n",
    "- trade-offè§£æ±ºï¼šèªæ„å®Œæ•´æ€§ > çµ±è¨ˆæŒ‡æ¨™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 54\n",
      "Average length: 784\n",
      "Min length: 125\n",
      "Max length: 998\n",
      "Length std dev: 215\n",
      "Warning: 2 chunks are very short\n"
     ]
    }
   ],
   "source": [
    "# æª¢æŸ¥chunkæ˜¯å¦åˆç†\n",
    "def analyze_chunks(docs):\n",
    "    lengths = [len(d.page_content) for d in docs]\n",
    "    \n",
    "    print(f\"Total chunks: {len(docs)}\")\n",
    "    print(f\"Average length: {sum(lengths)/len(lengths):.0f}\")\n",
    "    print(f\"Min length: {min(lengths)}\")  \n",
    "    print(f\"Max length: {max(lengths)}\")\n",
    "    print(f\"Length std dev: {(sum((x-sum(lengths)/len(lengths))**2 for x in lengths)/len(lengths))**0.5:.0f}\")\n",
    "    \n",
    "    # æª¢æŸ¥æ˜¯å¦æœ‰å¤ªçŸ­çš„chunkï¼ˆå¯èƒ½æ˜¯åˆ‡å‰²éŒ¯èª¤ï¼‰\n",
    "    short_chunks = [i for i, l in enumerate(lengths) if l < 200]\n",
    "    if short_chunks:\n",
    "        print(f\"Warning: {len(short_chunks)} chunks are very short\")\n",
    "        \n",
    "analyze_chunks(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬ä¸‰æ­¥æ¸¬è©¦ï¼šembedding ä¸¦æ”¾å…¥è³‡æ–™åº«"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embedding trade-offs:\n",
    "\n",
    "- å›ç­”ç‹€æ³\n",
    "- Embeddingå“è³ªæª¢æŸ¥\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£æ–°å¥—ä»¶\n",
    "%pip install -U langchain-openai\n",
    "%pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finishï¼\n",
      "è³‡æ–™å¤¾å­˜åœ¨å—ï¼ŸTrue\n"
     ]
    }
   ],
   "source": [
    "# å»ºç«‹è³‡æ–™åº«è·¯å¾‘ï¼Œå·²æœ‰è·¯å¾‘å‰‡å¯å¿½ç•¥\n",
    "import os\n",
    "\n",
    "# å»ºç«‹è³‡æ–™å¤¾\n",
    "os.makedirs('./chroma_db', exist_ok=True)\n",
    "print(\"finishï¼\")\n",
    "\n",
    "# æª¢æŸ¥æ˜¯å¦æˆåŠŸ\n",
    "print(f\"è³‡æ–™å¤¾å­˜åœ¨å—ï¼Ÿ{os.path.exists('./chroma_db')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define embedding\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ³¨æ„ï¼Œéœ€è¦å…ˆåœ¨è‡ªå·±çš„ç’°å¢ƒä¸­å»ºç«‹è³‡æ–™åº«è·¯å¾‘\n",
    "persist_directory = './chroma_db' # æŒ‡å®šè³‡æ–™åº«è·¯å¾‘\n",
    "!rm -rf ./chroma_db  # remove old database files if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹æ–°çš„å‘é‡è³‡æ–™åº«ï¼Œä¸¦å°‡æ–‡ä»¶æ”¾é€²å»\n",
    "from langchain.vectorstores import Chroma\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n"
     ]
    }
   ],
   "source": [
    "# æª¢æŸ¥å‰›å‰›çš„å‘é‡è¡Œæ•¸æ˜¯å¦èˆ‡å¡Šæ•¸ç›¸ç­‰\n",
    "print(vectordb._collection.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      ". Through these investigations, we\n",
      "contribute to a deeper understanding of how the\n",
      "order of options affects LLMsâ€™ decision-making in\n",
      "multiple-choice questions (MCQ) and offer practi-\n",
      "cal solutions, which go beyond simple bootstrap-\n",
      "ping, to increase their robustness and accuracy in\n",
      "such scenarios.\n",
      "2 Background and Experimental Details\n",
      "This paper focuses on the task of multiple-choice\n",
      "question answering. In multiple-choice questions,\n",
      "the objective is to identify the correct answer to a\n",
      "given question from a set of possible options (an il-\n",
      "lustration is presented in Figure 1). To address this\n",
      "task using in-context learning models, we present\n",
      "a prompt in the following format: â€œChoose the\n",
      "answer to the question only from A, B, C, D,\n",
      "and E choices. Question: {question}. Choices:\n",
      "{options}. Answer:â€ to the models\n"
     ]
    }
   ],
   "source": [
    "question = \"can you explain the abstract in this article?\"\n",
    "ans_docs = vectordb.similarity_search(question,k=3)\n",
    "print(len(ans_docs))\n",
    "print(ans_docs[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_x/xcmm2w3d1hld6_nc6t0xyrch0000gn/T/ipykernel_85068/2355425439.py:2: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "# æ‰‹å‹•å„²å­˜å‰›å‰›å»ºç«‹çš„è³‡æ–™åº«\n",
    "vectordb.persist() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬å››æ­¥æ¸¬è©¦ï¼šæª¢ç´¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define retriever æ”¹æˆä½¿ç”¨mmr\n",
    "retriever = vectordb.as_retriever(\n",
    "    search_type=\"mmr\", \n",
    "    search_kwargs={\n",
    "        \"k\": 3,\n",
    "        \"fetch_k\":20\n",
    "        }\n",
    "    )\n",
    "# question = \"what is the conclusion of this paper?\"\n",
    "# docs_mmr = vectordb.max_marginal_relevance_search(question,k=1)\n",
    "# docs_mmr[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # æª¢ç´¢æ›´å¤šçµæœï¼Œçœ‹çœ‹æœ‰æ²’æœ‰å®šç¾©\n",
    "# question = \"what are the conclusions of this paper?\"\n",
    "# docs_more = vectordb.similarity_search(question, k=5)\n",
    "\n",
    "# print(\"ğŸ” æª¢ç´¢åˆ°çš„æ‰€æœ‰æ®µè½ï¼š\")\n",
    "# for i, doc in enumerate(docs_more):\n",
    "#     print(f\"\\n--- Chunk {i+1} ---\")\n",
    "#     print(doc.page_content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬äº”æ­¥æ¸¬è©¦ï¼šå›ç­”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_x/xcmm2w3d1hld6_nc6t0xyrch0000gn/T/ipykernel_85068/1656567243.py:12: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model_name=llm_name, temperature=0)\n",
      "/var/folders/_x/xcmm2w3d1hld6_nc6t0xyrch0000gn/T/ipykernel_85068/1656567243.py:13: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  llm.predict(\"Hello world!\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# é¸æ“‡æ¨¡å‹\n",
    "import datetime\n",
    "current_date = datetime.datetime.now().date()\n",
    "if current_date < datetime.date(2023, 9, 2):\n",
    "    llm_name = \"gpt-3.5-turbo-0301\"\n",
    "else:\n",
    "    llm_name = \"gpt-3.5-turbo\"\n",
    "print(llm_name)\n",
    "\n",
    "# åˆå§‹åŒ–èŠå¤©æ©Ÿå™¨äººè¦ç”¨åˆ°çš„llm\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=llm_name, temperature=0)\n",
    "llm.predict(\"Hello world!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ–°é …ç›®ï¼šè¼‰å…¥ConversationBufferMemory\n",
    "# æ­¤å¥—ä»¶èƒ½è®“å•ç­”æ©Ÿå™¨äººè¨˜ä½éå¾€æ­·å²å•ç­”\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\", # å‘Šè¨´ chain å»å“ªè£¡æ‰¾æ­·å²å°è©±\n",
    "    return_messages=True, # è¿”å›çš„æ˜¯ç‰©ä»¶ï¼ˆé•·å¾—åƒjson or meta dataï¼‰\n",
    "    output_key=\"answer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "        llm=ChatOpenAI(model_name=llm_name, temperature=0), \n",
    "        chain_type=\"map_reduce\", \n",
    "        retriever=retriever, \n",
    "        return_source_documents=True,\n",
    "        return_generated_question=True,\n",
    "        memory=memory\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text does not provide specific information about the conclusion of the paper.\n"
     ]
    }
   ],
   "source": [
    "question = \"what is the conclusion of this paper?\"\n",
    "result = qa({\"question\": question})\n",
    "print(result['answer'])\n",
    "\n",
    "# éŒ¯èª¤ï¼šllmæ‰¾ä¸åˆ°ã€Œçµè«–ã€ï¼Œdebug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ragé‚è¼¯\n",
    "def load_db(file, chain_type, k):\n",
    "    # load documents\n",
    "    loader = PyPDFLoader(file)\n",
    "    documents = loader.load()\n",
    "    # split documents\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    # define embedding\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    # create vector database from data\n",
    "    db = DocArrayInMemorySearch.from_documents(docs, embeddings)\n",
    "    # define retriever\n",
    "    retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
    "    # create a chatbot chain. Memory is managed externally.\n",
    "    qa = ConversationalRetrievalChain.from_llm(\n",
    "        llm=ChatOpenAI(model_name=llm_name, temperature=0), \n",
    "        chain_type=chain_type, \n",
    "        retriever=retriever, \n",
    "        return_source_documents=True,\n",
    "        return_generated_question=True,\n",
    "    )\n",
    "    return qa \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting imutils\n",
      "  Downloading imutils-0.5.4.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: imutils\n",
      "  Building wheel for imutils (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for imutils: filename=imutils-0.5.4-py3-none-any.whl size=25860 sha256=7b479d541b96342e75c3529427a9eea0c52bb150ce6c8188a74ef6c43f915ebd\n",
      "  Stored in directory: /Users/mangtinglee/Library/Caches/pip/wheels/4b/a5/2d/4a070a801d3a3d93f033d3ee9728f470f514826e89952df3ea\n",
      "Successfully built imutils\n",
      "Installing collected packages: imutils\n",
      "Successfully installed imutils-0.5.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½œç”¨ï¼šç®¡ç†å°è©±æµç¨‹ã€é€£æ¥ragå’Œæ“ä½œä»‹é¢\n",
    "    # ä¾‹å¦‚ï¼šè¨˜ä½å°è©±æ­·å²ã€å‘¼å«ragå›ç­”å•é¡Œã€æ•´ç†ç­”æ¡ˆçµ¦ä½¿ç”¨è€…çœ‹ã€æä¾›è¼”åŠ©åŠŸèƒ½\n",
    "import panel as pn\n",
    "import param\n",
    "\n",
    "class cbfs(param.Parameterized):\n",
    "    chat_history = param.List([])\n",
    "    answer = param.String(\"\")\n",
    "    db_query  = param.String(\"\")\n",
    "    db_response = param.List([])\n",
    "    \n",
    "    def __init__(self,  **params):\n",
    "        super(cbfs, self).__init__( **params)\n",
    "        self.panels = []\n",
    "        self.loaded_file = \"docs/cs229_lectures/MachineLearning-Lecture01.pdf\"\n",
    "        self.qa = load_db(self.loaded_file,\"stuff\", 4)\n",
    "    \n",
    "    def call_load_db(self, count):\n",
    "        if count == 0 or file_input.value is None:  # init or no file specified :\n",
    "            return pn.pane.Markdown(f\"Loaded File: {self.loaded_file}\")\n",
    "        else:\n",
    "            file_input.save(\"temp.pdf\")  # local copy\n",
    "            self.loaded_file = file_input.filename\n",
    "            button_load.button_style=\"outline\"\n",
    "            self.qa = load_db(\"temp.pdf\", \"stuff\", 4)\n",
    "            button_load.button_style=\"solid\"\n",
    "        self.clr_history()\n",
    "        return pn.pane.Markdown(f\"Loaded File: {self.loaded_file}\")\n",
    "\n",
    "    def convchain(self, query):\n",
    "        if not query:\n",
    "            return pn.WidgetBox(pn.Row('User:', pn.pane.Markdown(\"\", width=600)), scroll=True)\n",
    "        result = self.qa({\"question\": query, \"chat_history\": self.chat_history})\n",
    "        self.chat_history.extend([(query, result[\"answer\"])])\n",
    "        self.db_query = result[\"generated_question\"]\n",
    "        self.db_response = result[\"source_documents\"]\n",
    "        self.answer = result['answer'] \n",
    "        self.panels.extend([\n",
    "            pn.Row('User:', pn.pane.Markdown(query, width=600)),\n",
    "            pn.Row('ChatBot:', pn.pane.Markdown(self.answer, width=600, style={'background-color': '#F6F6F6'}))\n",
    "        ])\n",
    "        inp.value = ''  #clears loading indicator when cleared\n",
    "        return pn.WidgetBox(*self.panels,scroll=True)\n",
    "\n",
    "    @param.depends('db_query ', )\n",
    "    def get_lquest(self):\n",
    "        if not self.db_query :\n",
    "            return pn.Column(\n",
    "                pn.Row(pn.pane.Markdown(f\"Last question to DB:\", styles={'background-color': '#F6F6F6'})),\n",
    "                pn.Row(pn.pane.Str(\"no DB accesses so far\"))\n",
    "            )\n",
    "        return pn.Column(\n",
    "            pn.Row(pn.pane.Markdown(f\"DB query:\", styles={'background-color': '#F6F6F6'})),\n",
    "            pn.pane.Str(self.db_query )\n",
    "        )\n",
    "\n",
    "    @param.depends('db_response', )\n",
    "    def get_sources(self):\n",
    "        if not self.db_response:\n",
    "            return \n",
    "        rlist=[pn.Row(pn.pane.Markdown(f\"Result of DB lookup:\", styles={'background-color': '#F6F6F6'}))]\n",
    "        for doc in self.db_response:\n",
    "            rlist.append(pn.Row(pn.pane.Str(doc)))\n",
    "        return pn.WidgetBox(*rlist, width=600, scroll=True)\n",
    "\n",
    "    @param.depends('convchain', 'clr_history') \n",
    "    def get_chats(self):\n",
    "        if not self.chat_history:\n",
    "            return pn.WidgetBox(pn.Row(pn.pane.Str(\"No History Yet\")), width=600, scroll=True)\n",
    "        rlist=[pn.Row(pn.pane.Markdown(f\"Current Chat History variable\", styles={'background-color': '#F6F6F6'}))]\n",
    "        for exchange in self.chat_history:\n",
    "            rlist.append(pn.Row(pn.pane.Str(exchange)))\n",
    "        return pn.WidgetBox(*rlist, width=600, scroll=True)\n",
    "\n",
    "    def clr_history(self,count=0):\n",
    "        self.chat_history = []\n",
    "        return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¶²é ä»‹é¢è¨­è¨ˆ\n",
    "    # å–®ç´”æä¾›è¦–è¦ºä»‹é¢è®“å¤§å®¶å¯ä»¥æ“ä½œ\n",
    "cb = cbfs()\n",
    "\n",
    "file_input = pn.widgets.FileInput(accept='.pdf')\n",
    "button_load = pn.widgets.Button(name=\"Load DB\", button_type='primary')\n",
    "button_clearhistory = pn.widgets.Button(name=\"Clear History\", button_type='warning')\n",
    "button_clearhistory.on_click(cb.clr_history)\n",
    "inp = pn.widgets.TextInput( placeholder='Enter text hereâ€¦')\n",
    "\n",
    "bound_button_load = pn.bind(cb.call_load_db, button_load.param.clicks)\n",
    "conversation = pn.bind(cb.convchain, inp) \n",
    "\n",
    "jpg_pane = pn.pane.Image( './img/convchain.jpg')\n",
    "\n",
    "tab1 = pn.Column(\n",
    "    pn.Row(inp),\n",
    "    pn.layout.Divider(),\n",
    "    pn.panel(conversation,  loading_indicator=True, height=300),\n",
    "    pn.layout.Divider(),\n",
    ")\n",
    "tab2= pn.Column(\n",
    "    pn.panel(cb.get_lquest),\n",
    "    pn.layout.Divider(),\n",
    "    pn.panel(cb.get_sources ),\n",
    ")\n",
    "tab3= pn.Column(\n",
    "    pn.panel(cb.get_chats),\n",
    "    pn.layout.Divider(),\n",
    ")\n",
    "tab4=pn.Column(\n",
    "    pn.Row( file_input, button_load, bound_button_load),\n",
    "    pn.Row( button_clearhistory, pn.pane.Markdown(\"Clears chat history. Can use to start a new topic\" )),\n",
    "    pn.layout.Divider(),\n",
    "    pn.Row(jpg_pane.clone(width=400))\n",
    ")\n",
    "dashboard = pn.Column(\n",
    "    pn.Row(pn.pane.Markdown('# ChatWithYourData_Bot')),\n",
    "    pn.Tabs(('Conversation', tab1), ('Database', tab2), ('Chat History', tab3),('Configure', tab4))\n",
    ")\n",
    "dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. æº–å‚™LLMæ¨™è¨»å‡½æ•¸\n",
    "def classify_academic_sections(chunk_content, chunk_metadata):\n",
    "    \"\"\"ä½¿ç”¨LLMç‚ºchunkæ¨™è¨»å­¸è¡“ç« ç¯€æ¨™ç±¤\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    è«‹åˆ†æä»¥ä¸‹å­¸è¡“è«–æ–‡ç‰‡æ®µï¼Œåˆ¤æ–·å®ƒå±¬æ–¼å“ªå€‹ç« ç¯€ã€‚\n",
    "    å¯èƒ½çš„ç« ç¯€åŒ…æ‹¬ï¼šAbstract, Introduction, Method, Results, Conclusion, References, Other\n",
    "    \n",
    "    å¦‚æœå…§å®¹æ©«è·¨å…©å€‹ç« ç¯€ï¼Œè«‹ç”¨é€—è™Ÿåˆ†éš”ï¼Œä¾‹å¦‚ï¼š\"Introduction,Method\"\n",
    "    \n",
    "    å…§å®¹ï¼š\n",
    "    {chunk_content}\n",
    "    \n",
    "    è«‹åªå›å‚³ç« ç¯€æ¨™ç±¤ï¼Œä¸è¦å…¶ä»–è§£é‡‹ã€‚\n",
    "    \"\"\"\n",
    "    \n",
    "    # èª¿ç”¨ä½ çš„LLM API\n",
    "    response = your_llm_api_call(prompt)\n",
    "    \n",
    "    # è™•ç†å›æ‡‰ï¼Œç¢ºä¿æ ¼å¼æ­£ç¢º\n",
    "    sections = [s.strip() for s in response.split(',')]\n",
    "    return sections\n",
    "\n",
    "# 2. ç‚ºæ‰€æœ‰chunksæ·»åŠ ç« ç¯€æ¨™ç±¤\n",
    "def add_section_labels_to_chunks(docs):\n",
    "    \"\"\"ç‚ºæ‰€æœ‰chunksæ·»åŠ ç« ç¯€æ¨™ç±¤åˆ°metadata\"\"\"\n",
    "    \n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"è™•ç† chunk {i+1}/{len(docs)}...\")\n",
    "        \n",
    "        # ç²å–ç« ç¯€æ¨™ç±¤\n",
    "        section_labels = classify_academic_sections(\n",
    "            doc.page_content, \n",
    "            doc.metadata\n",
    "        )\n",
    "        \n",
    "        # æ·»åŠ åˆ°metadata\n",
    "        doc.metadata['sections'] = section_labels\n",
    "        doc.metadata['primary_section'] = section_labels[0] if section_labels else 'Other'\n",
    "        \n",
    "        # å¯é¸ï¼šæ·»åŠ å…¶ä»–æœ‰ç”¨çš„metadata\n",
    "        doc.metadata['chunk_id'] = i\n",
    "        doc.metadata['chunk_length'] = len(doc.page_content)\n",
    "    \n",
    "    return docs\n",
    "\n",
    "# 3. åŸ·è¡Œæ¨™è¨»\n",
    "labeled_docs = add_section_labels_to_chunks(docs)\n",
    "\n",
    "# 4. æª¢æŸ¥çµæœ\n",
    "for i in range(min(5, len(labeled_docs))): # é€™æ®µä»€éº¼æ„æ€ï¼Ÿå°æ–¼ç­‰æ–¼5å¡Š\n",
    "    print(f\"\\n--- Chunk {i} ---\")\n",
    "    print(f\"ç« ç¯€: {labeled_docs[i].metadata.get('sections')}\")\n",
    "    print(f\"ä¸»è¦ç« ç¯€: {labeled_docs[i].metadata.get('primary_section')}\")\n",
    "    print(f\"å…§å®¹: {labeled_docs[i].page_content[:100]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
