{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立整體的ui介面，變成一個問答機器人\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA,  ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain-community pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "目的：建立langchain + openai 的基礎環境\n",
    "'''\n",
    "import os # 作業系統相關功能（讀取環境變數）\n",
    "from openai import OpenAI # openai api 客戶端\n",
    "from dotenv import load_dotenv, find_dotenv # dotenv 是專門用來讀取.env套件的套件，並接上環境\n",
    "_ = load_dotenv(find_dotenv()) # 讀取.env檔案\n",
    "client = OpenAI(\n",
    "    api_key=os.environ['OPENAI_API_KEY']\n",
    ")\n",
    "# openai.api_key  = os.environ['OPENAI_API_KEY'] 舊版\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一步測試：載入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success! API key loaded\n",
      "Key starts with: sk-proj-FWUUter...\n",
      "Key length: 164 characters\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 清除舊的環境變數\n",
    "if 'OPENAI_API_KEY' in os.environ:\n",
    "    del os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# 重新載入\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.environ.get('OPENAI_API_KEY')\n",
    "if api_key:\n",
    "    print(\"✅ Success! API key loaded\")\n",
    "    print(f\"Key starts with: {api_key[:15]}...\")\n",
    "    print(f\"Key length: {len(api_key)} characters\")\n",
    "else:\n",
    "    print(\"❌ Still not working\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder exists: True\n",
      "Found 23 PDF files:\n",
      "  - 2023_LLM limitation_Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions.pdf\n",
      "  - 2025_LLM limitation_The Order Effect- Investigating Prompt Sensitivity to Input Order in LLMs.pdf\n",
      "  - 2024_RAG_Evaluation of Retrieval-Augmented Generation- A Survey.pdf\n",
      "  - 2020_scaling laws_Scaling Laws for Neural Language Models.pdf\n",
      "  - 2022_LLM limitation_Robustness of Learning from Task Instructions.pdf\n"
     ]
    }
   ],
   "source": [
    "# 檢查資料夾是否存在\n",
    "import os\n",
    "folder_path = \"/Users/mangtinglee/Desktop/2025_gap_careerpath/RAG_LLM/pdfs\"\n",
    "\n",
    "print(f\"Folder exists: {os.path.exists(folder_path)}\")\n",
    "# 看看有哪些PDF檔案\n",
    "pdf_files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
    "print(f\"Found {len(pdf_files)} PDF files:\")\n",
    "for pdf in pdf_files[:5]:  # 顯示前5個檔名\n",
    "    print(f\"  - {pdf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test file loaded: 11 pages\n",
      "內容：Figure 2: Order sensitivity in few-shot setting:The error bars represent the range of minimum and maximum\n",
      "accuracy achievable in each task through oracle reordering. Our observations are as follows: (1) The sensitivity\n",
      "gap consistently remains substantial even with the addition of more demonstrations in the few-shot setting. (2) As\n",
      "performances improve, the sensitivity gap shrinks. (3) Adding more demonstrations does not necessarily result in a\n",
      "reduction of the sensitivity gap.\n",
      "4 Why Do LLMs Sho...\n",
      "metadata：{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-23T01:15:54+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-23T01:15:54+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/mangtinglee/Desktop/2025_gap_careerpath/RAG_LLM/pdfs/2023_LLM limitation_Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "# 測試1.載入一篇\n",
    "test_file = os.path.join(folder_path, pdf_files[0])\n",
    "loader = PyPDFLoader(test_file)\n",
    "documents = loader.load()\n",
    "print(f\"Test file loaded: {len(documents)} pages\")\n",
    "print(f\"內容：{documents[3].page_content[:500]}...\")\n",
    "print(f\"metadata：{documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二步測試：分割檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# 測試2. 分割檔案\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=150,\n",
    "    separators=[ \"\\n\\n\", \". \", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
    "    )    \n",
    "docs = text_splitter.split_documents(documents)\n",
    "print(len(docs)) # chunks\n",
    "print(len(documents)) # test 11 pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Language Models Sensitivity to The Order of Options in\n",
      "Multiple-Choice Questions\n",
      "Pouya Pezeshkpour\n",
      "Megagon Labs\n",
      "pouya@megagon.ai\n",
      "Estevam Hruschka\n",
      "Megagon Labs\n",
      "estevam@megagon.ai\n",
      "Abstract\n",
      "Large Language Models (LLMs) have demon-\n",
      "strated remarkable capabilities in various NLP\n",
      "tasks. However, previous works have shown\n",
      "these models are sensitive towards prompt\n",
      "wording, and few-shot demonstrations and\n",
      "their order, posing challenges to fair assess-\n",
      "ment of these models. As these models be-\n",
      "come more powerful, it becomes imperative\n",
      "to understand and address these limitations.\n",
      "In this paper, we focus on LLMs robust-\n",
      "ness on the task of multiple-choice questions—\n",
      "commonly adopted task to study reasoning and\n",
      "fact-retrieving capability of LLMs\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "separators=[\n",
    "    \"\\n\\n\",        # 段落分隔（最優先）\n",
    "    \". \",          # 句號\n",
    "    \"\\n\",          # 行分隔\n",
    "    \"(?<=\\. )\",    # 句號後分隔（用正則表達式）\n",
    "    \" \",           # 空格分隔\n",
    "    \"\"             # 字元分隔（最後手段）\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 問題：如何決定塊的大小和重疊度？\n",
    "- 兩種指標：\n",
    "    - 統計指標：\n",
    "        - 平均長度：接近設定的 `chunk _size`，代表接近目標 \n",
    "        - max：評估是否超過限制\n",
    "        - min：評估最小塊是否能接受\n",
    "        - std：chunk的大小範圍大概落在 mean ± std 之間，有點大的話大概長度不太一致，<200 (長度較一致)\n",
    "        - 一個最短的chunk：通常越少越好，<5%  \n",
    "\n",
    "    - 語意完整性（更重要）：\n",
    "        - 一個塊是否包含完整概念？\n",
    "        - 重要術語是否有被切斷？\n",
    "        - 評估方法：人工檢視（看完文字是否能理解）、問答測試法（先測試到最後，靠ai檢索是否能從準確回應答案）\n",
    "- 結論：現在看看不準，要等run過一遍再來看llm的檢索能力\n",
    "- trade-off解決：語意完整性 > 統計指標"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 54\n",
      "Average length: 784\n",
      "Min length: 125\n",
      "Max length: 998\n",
      "Length std dev: 215\n",
      "Warning: 2 chunks are very short\n"
     ]
    }
   ],
   "source": [
    "# 檢查chunk是否合理\n",
    "def analyze_chunks(docs):\n",
    "    lengths = [len(d.page_content) for d in docs]\n",
    "    \n",
    "    print(f\"Total chunks: {len(docs)}\")\n",
    "    print(f\"Average length: {sum(lengths)/len(lengths):.0f}\")\n",
    "    print(f\"Min length: {min(lengths)}\")  \n",
    "    print(f\"Max length: {max(lengths)}\")\n",
    "    print(f\"Length std dev: {(sum((x-sum(lengths)/len(lengths))**2 for x in lengths)/len(lengths))**0.5:.0f}\")\n",
    "    \n",
    "    # 檢查是否有太短的chunk（可能是切割錯誤）\n",
    "    short_chunks = [i for i, l in enumerate(lengths) if l < 200]\n",
    "    if short_chunks:\n",
    "        print(f\"Warning: {len(short_chunks)} chunks are very short\")\n",
    "        \n",
    "analyze_chunks(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第三步測試：embedding 並放入資料庫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embedding trade-offs:\n",
    "\n",
    "- 回答狀況\n",
    "- Embedding品質檢查\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安裝新套件\n",
    "%pip install -U langchain-openai\n",
    "%pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish！\n",
      "資料夾存在嗎？True\n"
     ]
    }
   ],
   "source": [
    "# 建立資料庫路徑，已有路徑則可忽略\n",
    "import os\n",
    "\n",
    "# 建立資料夾\n",
    "os.makedirs('./chroma_db', exist_ok=True)\n",
    "print(\"finish！\")\n",
    "\n",
    "# 檢查是否成功\n",
    "print(f\"資料夾存在嗎？{os.path.exists('./chroma_db')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define embedding\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意，需要先在自己的環境中建立資料庫路徑\n",
    "persist_directory = './chroma_db' # 指定資料庫路徑\n",
    "!rm -rf ./chroma_db  # remove old database files if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立新的向量資料庫，並將文件放進去\n",
    "from langchain.vectorstores import Chroma\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n"
     ]
    }
   ],
   "source": [
    "# 檢查剛剛的向量行數是否與塊數相等\n",
    "print(vectordb._collection.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      ". Through these investigations, we\n",
      "contribute to a deeper understanding of how the\n",
      "order of options affects LLMs’ decision-making in\n",
      "multiple-choice questions (MCQ) and offer practi-\n",
      "cal solutions, which go beyond simple bootstrap-\n",
      "ping, to increase their robustness and accuracy in\n",
      "such scenarios.\n",
      "2 Background and Experimental Details\n",
      "This paper focuses on the task of multiple-choice\n",
      "question answering. In multiple-choice questions,\n",
      "the objective is to identify the correct answer to a\n",
      "given question from a set of possible options (an il-\n",
      "lustration is presented in Figure 1). To address this\n",
      "task using in-context learning models, we present\n",
      "a prompt in the following format: “Choose the\n",
      "answer to the question only from A, B, C, D,\n",
      "and E choices. Question: {question}. Choices:\n",
      "{options}. Answer:” to the models\n"
     ]
    }
   ],
   "source": [
    "question = \"can you explain the abstract in this article?\"\n",
    "ans_docs = vectordb.similarity_search(question,k=3)\n",
    "print(len(ans_docs))\n",
    "print(ans_docs[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_x/xcmm2w3d1hld6_nc6t0xyrch0000gn/T/ipykernel_85068/2355425439.py:2: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "# 手動儲存剛剛建立的資料庫\n",
    "vectordb.persist() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第四步測試：檢索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define retriever 改成使用mmr\n",
    "retriever = vectordb.as_retriever(\n",
    "    search_type=\"mmr\", \n",
    "    search_kwargs={\n",
    "        \"k\": 3,\n",
    "        \"fetch_k\":20\n",
    "        }\n",
    "    )\n",
    "# question = \"what is the conclusion of this paper?\"\n",
    "# docs_mmr = vectordb.max_marginal_relevance_search(question,k=1)\n",
    "# docs_mmr[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 檢索更多結果，看看有沒有定義\n",
    "# question = \"what are the conclusions of this paper?\"\n",
    "# docs_more = vectordb.similarity_search(question, k=5)\n",
    "\n",
    "# print(\"🔍 檢索到的所有段落：\")\n",
    "# for i, doc in enumerate(docs_more):\n",
    "#     print(f\"\\n--- Chunk {i+1} ---\")\n",
    "#     print(doc.page_content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第五步測試：回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_x/xcmm2w3d1hld6_nc6t0xyrch0000gn/T/ipykernel_85068/1656567243.py:12: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model_name=llm_name, temperature=0)\n",
      "/var/folders/_x/xcmm2w3d1hld6_nc6t0xyrch0000gn/T/ipykernel_85068/1656567243.py:13: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  llm.predict(\"Hello world!\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 選擇模型\n",
    "import datetime\n",
    "current_date = datetime.datetime.now().date()\n",
    "if current_date < datetime.date(2023, 9, 2):\n",
    "    llm_name = \"gpt-3.5-turbo-0301\"\n",
    "else:\n",
    "    llm_name = \"gpt-3.5-turbo\"\n",
    "print(llm_name)\n",
    "\n",
    "# 初始化聊天機器人要用到的llm\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=llm_name, temperature=0)\n",
    "llm.predict(\"Hello world!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新項目：載入ConversationBufferMemory\n",
    "# 此套件能讓問答機器人記住過往歷史問答\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\", # 告訴 chain 去哪裡找歷史對話\n",
    "    return_messages=True, # 返回的是物件（長得像json or meta data）\n",
    "    output_key=\"answer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "        llm=ChatOpenAI(model_name=llm_name, temperature=0), \n",
    "        chain_type=\"map_reduce\", \n",
    "        retriever=retriever, \n",
    "        return_source_documents=True,\n",
    "        return_generated_question=True,\n",
    "        memory=memory\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text does not provide specific information about the conclusion of the paper.\n"
     ]
    }
   ],
   "source": [
    "question = \"what is the conclusion of this paper?\"\n",
    "result = qa({\"question\": question})\n",
    "print(result['answer'])\n",
    "\n",
    "# 錯誤：llm找不到「結論」，debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rag邏輯\n",
    "def load_db(file, chain_type, k):\n",
    "    # load documents\n",
    "    loader = PyPDFLoader(file)\n",
    "    documents = loader.load()\n",
    "    # split documents\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    # define embedding\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    # create vector database from data\n",
    "    db = DocArrayInMemorySearch.from_documents(docs, embeddings)\n",
    "    # define retriever\n",
    "    retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
    "    # create a chatbot chain. Memory is managed externally.\n",
    "    qa = ConversationalRetrievalChain.from_llm(\n",
    "        llm=ChatOpenAI(model_name=llm_name, temperature=0), \n",
    "        chain_type=chain_type, \n",
    "        retriever=retriever, \n",
    "        return_source_documents=True,\n",
    "        return_generated_question=True,\n",
    "    )\n",
    "    return qa \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting imutils\n",
      "  Downloading imutils-0.5.4.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: imutils\n",
      "  Building wheel for imutils (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for imutils: filename=imutils-0.5.4-py3-none-any.whl size=25860 sha256=7b479d541b96342e75c3529427a9eea0c52bb150ce6c8188a74ef6c43f915ebd\n",
      "  Stored in directory: /Users/mangtinglee/Library/Caches/pip/wheels/4b/a5/2d/4a070a801d3a3d93f033d3ee9728f470f514826e89952df3ea\n",
      "Successfully built imutils\n",
      "Installing collected packages: imutils\n",
      "Successfully installed imutils-0.5.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 作用：管理對話流程、連接rag和操作介面\n",
    "    # 例如：記住對話歷史、呼叫rag回答問題、整理答案給使用者看、提供輔助功能\n",
    "import panel as pn\n",
    "import param\n",
    "\n",
    "class cbfs(param.Parameterized):\n",
    "    chat_history = param.List([])\n",
    "    answer = param.String(\"\")\n",
    "    db_query  = param.String(\"\")\n",
    "    db_response = param.List([])\n",
    "    \n",
    "    def __init__(self,  **params):\n",
    "        super(cbfs, self).__init__( **params)\n",
    "        self.panels = []\n",
    "        self.loaded_file = \"docs/cs229_lectures/MachineLearning-Lecture01.pdf\"\n",
    "        self.qa = load_db(self.loaded_file,\"stuff\", 4)\n",
    "    \n",
    "    def call_load_db(self, count):\n",
    "        if count == 0 or file_input.value is None:  # init or no file specified :\n",
    "            return pn.pane.Markdown(f\"Loaded File: {self.loaded_file}\")\n",
    "        else:\n",
    "            file_input.save(\"temp.pdf\")  # local copy\n",
    "            self.loaded_file = file_input.filename\n",
    "            button_load.button_style=\"outline\"\n",
    "            self.qa = load_db(\"temp.pdf\", \"stuff\", 4)\n",
    "            button_load.button_style=\"solid\"\n",
    "        self.clr_history()\n",
    "        return pn.pane.Markdown(f\"Loaded File: {self.loaded_file}\")\n",
    "\n",
    "    def convchain(self, query):\n",
    "        if not query:\n",
    "            return pn.WidgetBox(pn.Row('User:', pn.pane.Markdown(\"\", width=600)), scroll=True)\n",
    "        result = self.qa({\"question\": query, \"chat_history\": self.chat_history})\n",
    "        self.chat_history.extend([(query, result[\"answer\"])])\n",
    "        self.db_query = result[\"generated_question\"]\n",
    "        self.db_response = result[\"source_documents\"]\n",
    "        self.answer = result['answer'] \n",
    "        self.panels.extend([\n",
    "            pn.Row('User:', pn.pane.Markdown(query, width=600)),\n",
    "            pn.Row('ChatBot:', pn.pane.Markdown(self.answer, width=600, style={'background-color': '#F6F6F6'}))\n",
    "        ])\n",
    "        inp.value = ''  #clears loading indicator when cleared\n",
    "        return pn.WidgetBox(*self.panels,scroll=True)\n",
    "\n",
    "    @param.depends('db_query ', )\n",
    "    def get_lquest(self):\n",
    "        if not self.db_query :\n",
    "            return pn.Column(\n",
    "                pn.Row(pn.pane.Markdown(f\"Last question to DB:\", styles={'background-color': '#F6F6F6'})),\n",
    "                pn.Row(pn.pane.Str(\"no DB accesses so far\"))\n",
    "            )\n",
    "        return pn.Column(\n",
    "            pn.Row(pn.pane.Markdown(f\"DB query:\", styles={'background-color': '#F6F6F6'})),\n",
    "            pn.pane.Str(self.db_query )\n",
    "        )\n",
    "\n",
    "    @param.depends('db_response', )\n",
    "    def get_sources(self):\n",
    "        if not self.db_response:\n",
    "            return \n",
    "        rlist=[pn.Row(pn.pane.Markdown(f\"Result of DB lookup:\", styles={'background-color': '#F6F6F6'}))]\n",
    "        for doc in self.db_response:\n",
    "            rlist.append(pn.Row(pn.pane.Str(doc)))\n",
    "        return pn.WidgetBox(*rlist, width=600, scroll=True)\n",
    "\n",
    "    @param.depends('convchain', 'clr_history') \n",
    "    def get_chats(self):\n",
    "        if not self.chat_history:\n",
    "            return pn.WidgetBox(pn.Row(pn.pane.Str(\"No History Yet\")), width=600, scroll=True)\n",
    "        rlist=[pn.Row(pn.pane.Markdown(f\"Current Chat History variable\", styles={'background-color': '#F6F6F6'}))]\n",
    "        for exchange in self.chat_history:\n",
    "            rlist.append(pn.Row(pn.pane.Str(exchange)))\n",
    "        return pn.WidgetBox(*rlist, width=600, scroll=True)\n",
    "\n",
    "    def clr_history(self,count=0):\n",
    "        self.chat_history = []\n",
    "        return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 網頁介面設計\n",
    "    # 單純提供視覺介面讓大家可以操作\n",
    "cb = cbfs()\n",
    "\n",
    "file_input = pn.widgets.FileInput(accept='.pdf')\n",
    "button_load = pn.widgets.Button(name=\"Load DB\", button_type='primary')\n",
    "button_clearhistory = pn.widgets.Button(name=\"Clear History\", button_type='warning')\n",
    "button_clearhistory.on_click(cb.clr_history)\n",
    "inp = pn.widgets.TextInput( placeholder='Enter text here…')\n",
    "\n",
    "bound_button_load = pn.bind(cb.call_load_db, button_load.param.clicks)\n",
    "conversation = pn.bind(cb.convchain, inp) \n",
    "\n",
    "jpg_pane = pn.pane.Image( './img/convchain.jpg')\n",
    "\n",
    "tab1 = pn.Column(\n",
    "    pn.Row(inp),\n",
    "    pn.layout.Divider(),\n",
    "    pn.panel(conversation,  loading_indicator=True, height=300),\n",
    "    pn.layout.Divider(),\n",
    ")\n",
    "tab2= pn.Column(\n",
    "    pn.panel(cb.get_lquest),\n",
    "    pn.layout.Divider(),\n",
    "    pn.panel(cb.get_sources ),\n",
    ")\n",
    "tab3= pn.Column(\n",
    "    pn.panel(cb.get_chats),\n",
    "    pn.layout.Divider(),\n",
    ")\n",
    "tab4=pn.Column(\n",
    "    pn.Row( file_input, button_load, bound_button_load),\n",
    "    pn.Row( button_clearhistory, pn.pane.Markdown(\"Clears chat history. Can use to start a new topic\" )),\n",
    "    pn.layout.Divider(),\n",
    "    pn.Row(jpg_pane.clone(width=400))\n",
    ")\n",
    "dashboard = pn.Column(\n",
    "    pn.Row(pn.pane.Markdown('# ChatWithYourData_Bot')),\n",
    "    pn.Tabs(('Conversation', tab1), ('Database', tab2), ('Chat History', tab3),('Configure', tab4))\n",
    ")\n",
    "dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 準備LLM標註函數\n",
    "def classify_academic_sections(chunk_content, chunk_metadata):\n",
    "    \"\"\"使用LLM為chunk標註學術章節標籤\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    請分析以下學術論文片段，判斷它屬於哪個章節。\n",
    "    可能的章節包括：Abstract, Introduction, Method, Results, Conclusion, References, Other\n",
    "    \n",
    "    如果內容橫跨兩個章節，請用逗號分隔，例如：\"Introduction,Method\"\n",
    "    \n",
    "    內容：\n",
    "    {chunk_content}\n",
    "    \n",
    "    請只回傳章節標籤，不要其他解釋。\n",
    "    \"\"\"\n",
    "    \n",
    "    # 調用你的LLM API\n",
    "    response = your_llm_api_call(prompt)\n",
    "    \n",
    "    # 處理回應，確保格式正確\n",
    "    sections = [s.strip() for s in response.split(',')]\n",
    "    return sections\n",
    "\n",
    "# 2. 為所有chunks添加章節標籤\n",
    "def add_section_labels_to_chunks(docs):\n",
    "    \"\"\"為所有chunks添加章節標籤到metadata\"\"\"\n",
    "    \n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"處理 chunk {i+1}/{len(docs)}...\")\n",
    "        \n",
    "        # 獲取章節標籤\n",
    "        section_labels = classify_academic_sections(\n",
    "            doc.page_content, \n",
    "            doc.metadata\n",
    "        )\n",
    "        \n",
    "        # 添加到metadata\n",
    "        doc.metadata['sections'] = section_labels\n",
    "        doc.metadata['primary_section'] = section_labels[0] if section_labels else 'Other'\n",
    "        \n",
    "        # 可選：添加其他有用的metadata\n",
    "        doc.metadata['chunk_id'] = i\n",
    "        doc.metadata['chunk_length'] = len(doc.page_content)\n",
    "    \n",
    "    return docs\n",
    "\n",
    "# 3. 執行標註\n",
    "labeled_docs = add_section_labels_to_chunks(docs)\n",
    "\n",
    "# 4. 檢查結果\n",
    "for i in range(min(5, len(labeled_docs))): # 這段什麼意思？小於等於5塊\n",
    "    print(f\"\\n--- Chunk {i} ---\")\n",
    "    print(f\"章節: {labeled_docs[i].metadata.get('sections')}\")\n",
    "    print(f\"主要章節: {labeled_docs[i].metadata.get('primary_section')}\")\n",
    "    print(f\"內容: {labeled_docs[i].page_content[:100]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
