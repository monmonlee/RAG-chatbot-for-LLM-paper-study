{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 問題：「明明有結論章節，但找不到結論」\n",
    "\n",
    "##### 可能的原因：\n",
    "- 檢索失敗：向量檢索根本沒找到結論相關的chunks\n",
    "- 檢索成功但LLM誤判：找到了結論內容，但LLM說「沒有結論」\n",
    "- chunk邊界問題：結論被切得支離破碎，失去語義\n",
    "- embedding問題：「結論」和「conclusion」的embedding距離太遠"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、檢查 check laoder → 正常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mangtinglee/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 建立整體的ui介面，變成一個問答機器人\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA,  ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from openai import OpenAI \n",
    "from dotenv import load_dotenv, find_dotenv \n",
    "_ = load_dotenv(find_dotenv()) \n",
    "client = OpenAI(\n",
    "    api_key=os.environ['OPENAI_API_KEY']\n",
    ")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success! API key loaded\n",
      "Key starts with: sk-proj-FWUUter...\n",
      "Key length: 164 characters\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "if 'OPENAI_API_KEY' in os.environ:\n",
    "    del os.environ['OPENAI_API_KEY']\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.environ.get('OPENAI_API_KEY')\n",
    "if api_key:\n",
    "    print(\"✅ Success! API key loaded\")\n",
    "    print(f\"Key starts with: {api_key[:15]}...\")\n",
    "    print(f\"Key length: {len(api_key)} characters\")\n",
    "else:\n",
    "    print(\"❌ Still not working\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23 PDF files:\n",
      "Test file loaded: 11 pages\n",
      "內容：Large Language Models Sensitivity to The Order of Options in\n",
      "Multiple-Choice Questions\n",
      "Pouya Pezeshkpour\n",
      "Megagon Labs\n",
      "pouya@megagon.ai\n",
      "Estevam Hruschka\n",
      "Megagon Labs\n",
      "estevam@megagon.ai\n",
      "Abstract\n",
      "Large Language Models (LLMs) have demon-\n",
      "strated remarkable capabilities in various NLP\n",
      "tasks. However, previous works have shown\n",
      "these models are sensitive towards prompt\n",
      "wording, and few-shot demonstrations and\n",
      "their order, posing challenges to fair assess-\n",
      "ment of these models. As these models be-\n",
      "come ...\n",
      "metadata：{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-23T01:15:54+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-23T01:15:54+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/mangtinglee/Desktop/2025_gap_careerpath/RAG_LLM/pdfs/2023_LLM limitation_Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "# 載入一篇\n",
    "folder_path = \"/Users/mangtinglee/Desktop/2025_gap_careerpath/RAG_LLM/pdfs\"\n",
    "pdf_files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
    "print(f\"Found {len(pdf_files)} PDF files:\")\n",
    "\n",
    "test_file = os.path.join(folder_path, pdf_files[0])\n",
    "loader = PyPDFLoader(test_file)\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Test file loaded: {len(documents)} pages\")\n",
    "print(f\"內容：{documents[0].page_content[:500]}...\")\n",
    "print(f\"metadata：{documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、檢查 split → 已經切好，但不知道是否正確"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=150,\n",
    "    separators=[ \"\\n\\n\",  \"\\n\", \". \", \"(?<=\\. )\", \" \", \"\"]\n",
    "    )    \n",
    "docs = text_splitter.split_documents(documents)\n",
    "print(len(docs)) # chunks\n",
    "print(len(documents)) # test 11 pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 50\n",
      "Average length: 901\n",
      "Min length: 193\n",
      "Max length: 996\n",
      "Length std dev: 179\n",
      "Warning: 1 chunks are very short\n"
     ]
    }
   ],
   "source": [
    "# 檢查chunk是否合理\n",
    "def analyze_chunks(docs):\n",
    "    lengths = [len(d.page_content) for d in docs]\n",
    "    \n",
    "    print(f\"Total chunks: {len(docs)}\")\n",
    "    print(f\"Average length: {sum(lengths)/len(lengths):.0f}\")\n",
    "    print(f\"Min length: {min(lengths)}\")  \n",
    "    print(f\"Max length: {max(lengths)}\")\n",
    "    print(f\"Length std dev: {(sum((x-sum(lengths)/len(lengths))**2 for x in lengths)/len(lengths))**0.5:.0f}\")\n",
    "    \n",
    "    # 檢查是否有太短的chunk（可能是切割錯誤）\n",
    "    short_chunks = [i for i, l in enumerate(lengths) if l < 200]\n",
    "    if short_chunks:\n",
    "        print(f\"Warning: {len(short_chunks)} chunks are very short\")\n",
    "        \n",
    "analyze_chunks(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 三、embedding → "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish！\n",
      "資料夾存在嗎？True\n"
     ]
    }
   ],
   "source": [
    "# 建立資料庫路徑，已有路徑則可忽略\n",
    "import os\n",
    "\n",
    "# 建立資料夾\n",
    "os.makedirs('./chroma_db', exist_ok=True)\n",
    "print(\"finish！\")\n",
    "\n",
    "# 檢查是否成功\n",
    "print(f\"資料夾存在嗎？{os.path.exists('./chroma_db')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define embedding\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意，需要先在自己的環境中建立資料庫路徑\n",
    "persist_directory = './chroma_db' # 指定資料庫路徑\n",
    "!rm -rf ./chroma_db  # remove old database files if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立新的向量資料庫，並將文件放進去\n",
    "from langchain.vectorstores import Chroma\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "# 檢查剛剛的向量行數是否與塊數相等\n",
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "sensitivity gap coverage ranging from 20% to 72%,\n",
      "while the mitigating bias pattern ranges from 0.9%\n",
      "to 38%. These results validate the effectiveness\n",
      "of the identified pattern for both amplifying and\n",
      "mitigating bias. Additionally, in most cases, the\n",
      "amplifying pattern covers a considerably greater\n",
      "portion of the sensitivity gap comparing to the mit-\n",
      "igating pattern. It is important to highlight that the\n",
      "patterns we have identified for amplifying bias can\n",
      "serve as valuable insights for enhancing model per-\n",
      "formance or launching adversarial attacks against\n",
      "them. Furthermore, the patterns we have estab-\n",
      "lished for mitigating bias can play a crucial role in\n",
      "shaping benchmark design and guiding annotating\n",
      "efforts to create less biased benchmarks.\n",
      "5 Calibrating LLMs for MCQ Tasks\n",
      "We conduct an in-depth investigation into how large\n",
      "language models react to changes in the order of\n",
      "options, and investigate the reasons behind their\n",
      "sensitivity to such changes. Through our explo-\n"
     ]
    }
   ],
   "source": [
    "question = \"can you explain the abstract of this article?\"\n",
    "ans_docs = vectordb.similarity_search(question,k=3)\n",
    "print(len(ans_docs))\n",
    "print(ans_docs[2].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found abstract in document 0:\n",
      "Large Language Models Sensitivity to The Order of Options in\n",
      "Multiple-Choice Questions\n",
      "Pouya Pezeshkpour\n",
      "Megagon Labs\n",
      "pouya@megagon.ai\n",
      "Estevam Hruschka\n",
      "Megagon Labs\n",
      "estevam@megagon.ai\n",
      "Abstract\n",
      "Large Language Models (LLMs) have demon-\n",
      "strated remarkable capabilities in various NLP\n",
      "tasks. However, previous works have shown\n",
      "these models are sensitive towards prompt\n",
      "wording, and few-shot demonstrations and\n",
      "their order, posing challenges to fair assess-\n",
      "ment of these models. As these models be-\n",
      "come \n",
      "---\n",
      "Found abstract in document 1:\n",
      "In this paper, we investigating the sensitivity of\n",
      "LLMs to the order of options in multiple-choice\n",
      "questions; using it as a proxy to understand LLMs\n",
      "sensitivity to the order of prompt elements in\n",
      "instruction- or demonstration-based paradigm. We\n",
      "demonstrate an example of GPT-4’s sensitivity to\n",
      "options order in Figure 1, using a sample from the\n",
      "CSQA benchmark (Talmor et al., 2018). Notably,\n",
      "by merely rearranging the placement of options\n",
      "among choices A, C, and E, GPT-4 incorrectly pre-\n",
      "dicts the a\n",
      "---\n",
      "Found abstract in document 2:\n",
      "Tasks GPT-4 InstructGPT\n",
      "Vanila Min Max Vanila Min Max\n",
      "CSQA 84.3 -12.6 +10.3 72.3 -24.0 +19.1\n",
      "Logical Deduction 92.3 -8.1 +5.0 64.0 -39.4 +34.7\n",
      "Abstract Algebra 57.0 -30.0 +23.0 33.0 -31.0 +39.0\n",
      "High School Chemistry 71.9 -23.6 +18.2 44.8 -28.5 +38.0\n",
      "Professional Law 66.1 -12.7 +12.1 48.6 -24.9 +25.7\n",
      "Table 1: Zero-shot order sensitivity; both GPT-4 and InstructGPT exhibit a notable level of sensitivity to the order\n",
      "of options across various benchmarks.\n",
      "ber of provided options, we selected benchma\n",
      "---\n",
      "Found abstract in document 4:\n",
      "Tasks Sorted Options # Options\n",
      "Hits@1 Hits@2 Hits@3 Top-2 Top-3 All\n",
      "GPT-4\n",
      "CSQA 81.3 95.1 98.2 84.2 85.1 84.3\n",
      "Logical Deduction 85.3 95.7 97.9 94.8 92.3 92.3\n",
      "Abstract Algebra 55.0 72.0 88.0 57.0 52.0 57.0\n",
      "High School Chemistry 64.0 74.4 76.8 65.5 68.1 71.9\n",
      "Professional Law 51.7 62.9 74.1 65.3 65.1 66.1\n",
      "InstructGPT\n",
      "CSQA 63.4 82.3 90.3 70.6 72.1 72.3\n",
      "Logical Deduction 65.6 93.0 97.6 66.2 64.0 64.0\n",
      "Abstract Algebra 28.0 52.0 73.0 26.0 29.0 33.0\n",
      "High School Chemistry 30.0 51.7 66.9 37.9 40.1 44.8\n",
      "Pro\n",
      "---\n",
      "Found abstract in document 6:\n",
      "Tasks GPT-4 InstructGPT\n",
      "Amplifying-Bias Mitigating-Bias Amplifying-Bias Mitigating-Bias\n",
      "CSQA 62.9 22.7 71.7 38.3\n",
      "Logical Deduction 42.0 10.1 61.7 0.9\n",
      "Abstract Algebra 52.8 15.1 35.7 25.7\n",
      "High School Chemistry 21.5 22.9 25.7 25.7\n",
      "Professional Law 31.5 9.7 20.1 25.9\n",
      "Table 4: Percentage of initial sensitivity gap coveredusing the identified patterns to amplify and mitigate positional\n",
      "bias. A higher percentage in amplifying bias and a lower percentage in mitigating bias indicate better performance\n",
      "i\n",
      "---\n",
      "Found abstract in document 7:\n",
      "Tasks GPT-4 InstructGPT\n",
      "Majority MEC Majority MEC\n",
      "CSQA 86.1 ( +1.8) 81.2 ( −3.1) 74.7 ( +2.4) 67.3 ( −5.0)\n",
      "Logical Deduction 94.3 ( +2.0) 97.4 ( +5.1) 72.0 ( +8.0) 57.1 ( −6.9)\n",
      "Abstract Algebra 57.0 ( 0.0) 59.0 ( +2.0) 38.0 ( +5.0) 31.0 ( −2.0)\n",
      "High School Chemistry 71.9 ( 0.0) 77.2 ( +5.3) 45.8 ( +1.0) 39.4 ( −5.4)\n",
      "Professional Law 67.3 ( +1.2) 66.3 ( +0.2) 54.3 ( +5.7) 47.2 ( −1.4)\n",
      "Table 5: Impact of calibration methods on LLMs’ performance.\n",
      "amount of improvement differs considerably, thus\n",
      "cas\n",
      "---\n",
      "Found abstract in document 10:\n",
      "Tasks 1-shot 2-shot 5-shot\n",
      "Vanila Min Max Vanila Min Max Vanila Min Max\n",
      "CSQA 74.2 53.4 92.1 74.7 60.7 91.6 76.3 59.3 91.1\n",
      "Logical Deduction 61.0 16.0 97.0 72.0 17.7 97.3 64.7 17.3 96.0\n",
      "Abstract Algebra 36.0 3.0 72.0 38.0 9.0 73.0 34.0 7.0 73.0\n",
      "High School Chemistry 50.7 19.7 89.7 52.2 21.6 84.2 52.7 24.6 83.2\n",
      "Professional Law 52.1 22.3 74.7 53.3 29.3 75.3 53.7 25.5 75.2\n",
      "Table 6: Few-shot order sensitivity in InstructGPT.\n",
      "Tasks 1-shot 2-shot 5-shot\n",
      "Vanila Min Max Vanila Min Max Vanila Min Max\n",
      "CSQ\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# 看看documents裡有沒有Abstract\n",
    "for i, doc in enumerate(documents):\n",
    "    if 'abstract' in doc.page_content.lower():\n",
    "        print(f\"Found abstract in document {i}:\")\n",
    "        print(doc.page_content[:500])\n",
    "        print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First page content:\n",
      "Large Language Models Sensitivity to The Order of Options in\n",
      "Multiple-Choice Questions\n",
      "Pouya Pezeshkpour\n",
      "Megagon Labs\n",
      "pouya@megagon.ai\n",
      "Estevam Hruschka\n",
      "Megagon Labs\n",
      "estevam@megagon.ai\n",
      "Abstract\n",
      "Large Language Models (LLMs) have demon-\n",
      "strated remarkable capabilities in various NLP\n",
      "tasks. However, previous works have shown\n",
      "these models are sensitive towards prompt\n",
      "wording, and few-shot demonstrations and\n",
      "their order, posing challenges to fair assess-\n",
      "ment of these models. As these models be-\n",
      "come more powerful, it becomes imperative\n",
      "to understand and address these limitations.\n",
      "In this paper, we focus on LLMs robust-\n",
      "ness on the task of multiple-choice questions—\n",
      "commonly adopted task to study reasoning and\n",
      "fact-retrieving capability of LLMs. Investigat-\n",
      "ing the sensitivity of LLMs towards the order\n",
      "of options in multiple-choice questions, we\n",
      "demonstrate a considerable performance gap\n",
      "of approximately 13% to 75% in LLMs on dif-\n",
      "ferent benchmarks, when answer options are\n",
      "reordered, even wh\n"
     ]
    }
   ],
   "source": [
    "# 看看第一頁到底載入了什麼\n",
    "first_page = documents[0]\n",
    "print(\"First page content:\")\n",
    "print(first_page.page_content[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
