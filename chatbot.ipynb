{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mangtinglee/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 建立整體的ui介面，變成一個問答機器人\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA,  ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain-community pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "目的：建立langchain + openai 的基礎環境\n",
    "'''\n",
    "import os # 作業系統相關功能（讀取環境變數）\n",
    "from openai import OpenAI # openai api 客戶端\n",
    "from dotenv import load_dotenv, find_dotenv # dotenv 是專門用來讀取.env套件的套件，並接上環境\n",
    "_ = load_dotenv(find_dotenv()) # 讀取.env檔案\n",
    "client = OpenAI(\n",
    "    api_key=os.environ['OPENAI_API_KEY']\n",
    ")\n",
    "# openai.api_key  = os.environ['OPENAI_API_KEY'] 舊版\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一步測試：載入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 1\n",
      "python-dotenv could not parse statement starting at line 4\n",
      "python-dotenv could not parse statement starting at line 5\n",
      "python-dotenv could not parse statement starting at line 7\n",
      "python-dotenv could not parse statement starting at line 9\n",
      "python-dotenv could not parse statement starting at line 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success! API key loaded\n",
      "Key starts with: sk-proj-FWUUter...\n",
      "Key length: 164 characters\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 清除舊的環境變數\n",
    "if 'OPENAI_API_KEY' in os.environ:\n",
    "    del os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# 重新載入\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.environ.get('OPENAI_API_KEY')\n",
    "if api_key:\n",
    "    print(\"✅ Success! API key loaded\")\n",
    "    print(f\"Key starts with: {api_key[:15]}...\")\n",
    "    print(f\"Key length: {len(api_key)} characters\")\n",
    "else:\n",
    "    print(\"❌ Still not working\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder exists: True\n",
      "Found 23 PDF files:\n",
      "  - 2023_LLM limitation_Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions.pdf\n",
      "  - 2025_LLM limitation_The Order Effect- Investigating Prompt Sensitivity to Input Order in LLMs.pdf\n",
      "  - 2024_RAG_Evaluation of Retrieval-Augmented Generation- A Survey.pdf\n",
      "  - 2020_scaling laws_Scaling Laws for Neural Language Models.pdf\n",
      "  - 2022_LLM limitation_Robustness of Learning from Task Instructions.pdf\n"
     ]
    }
   ],
   "source": [
    "# 檢查資料夾是否存在\n",
    "import os\n",
    "folder_path = \"/Users/mangtinglee/Desktop/2025_gap_careerpath/RAG_LLM/pdfs\"\n",
    "\n",
    "print(f\"Folder exists: {os.path.exists(folder_path)}\")\n",
    "# 看看有哪些PDF檔案\n",
    "pdf_files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
    "print(f\"Found {len(pdf_files)} PDF files:\")\n",
    "for pdf in pdf_files[:5]:  # 顯示前5個檔名\n",
    "    print(f\"  - {pdf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test file loaded: 11 pages\n",
      "內容：Figure 2: Order sensitivity in few-shot setting:The error bars represent the range of minimum and maximum\n",
      "accuracy achievable in each task through oracle reordering. Our observations are as follows: (1) The sensitivity\n",
      "gap consistently remains substantial even with the addition of more demonstrations in the few-shot setting. (2) As\n",
      "performances improve, the sensitivity gap shrinks. (3) Adding more demonstrations does not necessarily result in a\n",
      "reduction of the sensitivity gap.\n",
      "4 Why Do LLMs Sho...\n",
      "metadata：{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-23T01:15:54+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-23T01:15:54+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/mangtinglee/Desktop/2025_gap_careerpath/RAG_LLM/pdfs/2023_LLM limitation_Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "# 測試1.載入一篇\n",
    "test_file = os.path.join(folder_path, pdf_files[0])\n",
    "loader = PyPDFLoader(test_file)\n",
    "documents = loader.load()\n",
    "print(f\"Test file loaded: {len(documents)} pages\")\n",
    "print(f\"內容：{documents[3].page_content[:500]}...\")\n",
    "print(f\"metadata：{documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二步測試：分割檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# 測試2. 分割檔案\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=150,\n",
    "    separators=[ \"\\n\\n\", \". \", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
    "    )    \n",
    "docs = text_splitter.split_documents(documents)\n",
    "print(len(docs)) # chunks\n",
    "print(len(documents)) # test 11 pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "separators=[\n",
    "    \"\\n\\n\",        # 段落分隔（最優先）\n",
    "    \". \",          # 句號\n",
    "    \"\\n\",          # 行分隔\n",
    "    \"(?<=\\. )\",    # 句號後分隔（用正則表達式）\n",
    "    \" \",           # 空格分隔\n",
    "    \"\"             # 字元分隔（最後手段）\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 問題：如何決定塊的大小和重疊度？\n",
    "- 兩種指標：\n",
    "    - 統計指標：\n",
    "        - 平均長度：接近設定的 `chunk _size`，代表接近目標 \n",
    "        - max：評估是否超過限制\n",
    "        - min：評估最小塊是否能接受\n",
    "        - std：chunk的大小範圍大概落在 mean ± std 之間，有點大的話大概長度不太一致，<200 (長度較一致)\n",
    "        - 一個最短的chunk：通常越少越好，<5%  \n",
    "\n",
    "    - 語意完整性（更重要）：\n",
    "        - 一個塊是否包含完整概念？\n",
    "        - 重要術語是否有被切斷？\n",
    "        - 評估方法：人工檢視（看完文字是否能理解）、問答測試法（先測試到最後，靠ai檢索是否能從準確回應答案）\n",
    "- 結論：現在看看不準，要等run過一遍再來看llm的檢索能力\n",
    "- trade-off解決：語意完整性 > 統計指標"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 54\n",
      "Average length: 784\n",
      "Min length: 125\n",
      "Max length: 998\n",
      "Length std dev: 215\n",
      "Warning: 2 chunks are very short\n"
     ]
    }
   ],
   "source": [
    "# 檢查chunk是否合理\n",
    "def analyze_chunks(docs):\n",
    "    lengths = [len(d.page_content) for d in docs]\n",
    "    \n",
    "    print(f\"Total chunks: {len(docs)}\")\n",
    "    print(f\"Average length: {sum(lengths)/len(lengths):.0f}\")\n",
    "    print(f\"Min length: {min(lengths)}\")  \n",
    "    print(f\"Max length: {max(lengths)}\")\n",
    "    print(f\"Length std dev: {(sum((x-sum(lengths)/len(lengths))**2 for x in lengths)/len(lengths))**0.5:.0f}\")\n",
    "    \n",
    "    # 檢查是否有太短的chunk（可能是切割錯誤）\n",
    "    short_chunks = [i for i, l in enumerate(lengths) if l < 200]\n",
    "    if short_chunks:\n",
    "        print(f\"Warning: {len(short_chunks)} chunks are very short\")\n",
    "        \n",
    "analyze_chunks(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第三步測試：embedding 並放入資料庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安裝新套件\n",
    "%pip install -U langchain-openai\n",
    "%pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish！\n",
      "資料夾存在嗎？True\n"
     ]
    }
   ],
   "source": [
    "# 建立資料庫路徑，已有路徑則可忽略\n",
    "import os\n",
    "\n",
    "# 建立資料夾\n",
    "os.makedirs('./chroma_db', exist_ok=True)\n",
    "print(\"finish！\")\n",
    "\n",
    "# 檢查是否成功\n",
    "print(f\"資料夾存在嗎？{os.path.exists('./chroma_db')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define embedding\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意，需要先在自己的環境中建立資料庫路徑\n",
    "persist_directory = './chroma_db' # 指定資料庫路徑\n",
    "!rm -rf ./chroma_db  # remove old database files if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立新的向量資料庫，並將文件放進去\n",
    "from langchain.vectorstores import Chroma\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n"
     ]
    }
   ],
   "source": [
    "# 檢查剛剛的向量行數是否與塊數相等\n",
    "print(vectordb._collection.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      ". Through these investigations, we\n",
      "contribute to a deeper understanding of how the\n",
      "order of options affects LLMs’ decision-making in\n",
      "multiple-choice questions (MCQ) and offer practi-\n",
      "cal solutions, which go beyond simple bootstrap-\n",
      "ping, to increase their robustness and accuracy in\n",
      "such scenarios.\n",
      "2 Background and Experimental Details\n",
      "This paper focuses on the task of multiple-choice\n",
      "question answering. In multiple-choice questions,\n",
      "the objective is to identify the correct answer to a\n",
      "given question from a set of possible options (an il-\n",
      "lustration is presented in Figure 1). To address this\n",
      "task using in-context learning models, we present\n",
      "a prompt in the following format: “Choose the\n",
      "answer to the question only from A, B, C, D,\n",
      "and E choices. Question: {question}. Choices:\n",
      "{options}. Answer:” to the models\n"
     ]
    }
   ],
   "source": [
    "question = \"can you explain the abstract in this article?\"\n",
    "ans_docs = vectordb.similarity_search(question,k=3)\n",
    "print(len(ans_docs))\n",
    "print(ans_docs[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_x/xcmm2w3d1hld6_nc6t0xyrch0000gn/T/ipykernel_75980/2355425439.py:2: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "# 手動儲存剛剛建立的資料庫\n",
    "vectordb.persist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rag邏輯\n",
    "def load_db(file, chain_type, k):\n",
    "    # load documents\n",
    "    loader = PyPDFLoader(file)\n",
    "    documents = loader.load()\n",
    "    # split documents\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    # define embedding\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    # create vector database from data\n",
    "    db = DocArrayInMemorySearch.from_documents(docs, embeddings)\n",
    "    # define retriever\n",
    "    retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
    "    # create a chatbot chain. Memory is managed externally.\n",
    "    qa = ConversationalRetrievalChain.from_llm(\n",
    "        llm=ChatOpenAI(model_name=llm_name, temperature=0), \n",
    "        chain_type=chain_type, \n",
    "        retriever=retriever, \n",
    "        return_source_documents=True,\n",
    "        return_generated_question=True,\n",
    "    )\n",
    "    return qa \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 作用：管理對話流程、連接rag和操作介面\n",
    "    # 例如：記住對話歷史、呼叫rag回答問題、整理答案給使用者看、提供輔助功能\n",
    "import panel as pn\n",
    "import param\n",
    "\n",
    "class cbfs(param.Parameterized):\n",
    "    chat_history = param.List([])\n",
    "    answer = param.String(\"\")\n",
    "    db_query  = param.String(\"\")\n",
    "    db_response = param.List([])\n",
    "    \n",
    "    def __init__(self,  **params):\n",
    "        super(cbfs, self).__init__( **params)\n",
    "        self.panels = []\n",
    "        self.loaded_file = \"docs/cs229_lectures/MachineLearning-Lecture01.pdf\"\n",
    "        self.qa = load_db(self.loaded_file,\"stuff\", 4)\n",
    "    \n",
    "    def call_load_db(self, count):\n",
    "        if count == 0 or file_input.value is None:  # init or no file specified :\n",
    "            return pn.pane.Markdown(f\"Loaded File: {self.loaded_file}\")\n",
    "        else:\n",
    "            file_input.save(\"temp.pdf\")  # local copy\n",
    "            self.loaded_file = file_input.filename\n",
    "            button_load.button_style=\"outline\"\n",
    "            self.qa = load_db(\"temp.pdf\", \"stuff\", 4)\n",
    "            button_load.button_style=\"solid\"\n",
    "        self.clr_history()\n",
    "        return pn.pane.Markdown(f\"Loaded File: {self.loaded_file}\")\n",
    "\n",
    "    def convchain(self, query):\n",
    "        if not query:\n",
    "            return pn.WidgetBox(pn.Row('User:', pn.pane.Markdown(\"\", width=600)), scroll=True)\n",
    "        result = self.qa({\"question\": query, \"chat_history\": self.chat_history})\n",
    "        self.chat_history.extend([(query, result[\"answer\"])])\n",
    "        self.db_query = result[\"generated_question\"]\n",
    "        self.db_response = result[\"source_documents\"]\n",
    "        self.answer = result['answer'] \n",
    "        self.panels.extend([\n",
    "            pn.Row('User:', pn.pane.Markdown(query, width=600)),\n",
    "            pn.Row('ChatBot:', pn.pane.Markdown(self.answer, width=600, style={'background-color': '#F6F6F6'}))\n",
    "        ])\n",
    "        inp.value = ''  #clears loading indicator when cleared\n",
    "        return pn.WidgetBox(*self.panels,scroll=True)\n",
    "\n",
    "    @param.depends('db_query ', )\n",
    "    def get_lquest(self):\n",
    "        if not self.db_query :\n",
    "            return pn.Column(\n",
    "                pn.Row(pn.pane.Markdown(f\"Last question to DB:\", styles={'background-color': '#F6F6F6'})),\n",
    "                pn.Row(pn.pane.Str(\"no DB accesses so far\"))\n",
    "            )\n",
    "        return pn.Column(\n",
    "            pn.Row(pn.pane.Markdown(f\"DB query:\", styles={'background-color': '#F6F6F6'})),\n",
    "            pn.pane.Str(self.db_query )\n",
    "        )\n",
    "\n",
    "    @param.depends('db_response', )\n",
    "    def get_sources(self):\n",
    "        if not self.db_response:\n",
    "            return \n",
    "        rlist=[pn.Row(pn.pane.Markdown(f\"Result of DB lookup:\", styles={'background-color': '#F6F6F6'}))]\n",
    "        for doc in self.db_response:\n",
    "            rlist.append(pn.Row(pn.pane.Str(doc)))\n",
    "        return pn.WidgetBox(*rlist, width=600, scroll=True)\n",
    "\n",
    "    @param.depends('convchain', 'clr_history') \n",
    "    def get_chats(self):\n",
    "        if not self.chat_history:\n",
    "            return pn.WidgetBox(pn.Row(pn.pane.Str(\"No History Yet\")), width=600, scroll=True)\n",
    "        rlist=[pn.Row(pn.pane.Markdown(f\"Current Chat History variable\", styles={'background-color': '#F6F6F6'}))]\n",
    "        for exchange in self.chat_history:\n",
    "            rlist.append(pn.Row(pn.pane.Str(exchange)))\n",
    "        return pn.WidgetBox(*rlist, width=600, scroll=True)\n",
    "\n",
    "    def clr_history(self,count=0):\n",
    "        self.chat_history = []\n",
    "        return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 網頁介面設計\n",
    "    # 單純提供視覺介面讓大家可以操作\n",
    "cb = cbfs()\n",
    "\n",
    "file_input = pn.widgets.FileInput(accept='.pdf')\n",
    "button_load = pn.widgets.Button(name=\"Load DB\", button_type='primary')\n",
    "button_clearhistory = pn.widgets.Button(name=\"Clear History\", button_type='warning')\n",
    "button_clearhistory.on_click(cb.clr_history)\n",
    "inp = pn.widgets.TextInput( placeholder='Enter text here…')\n",
    "\n",
    "bound_button_load = pn.bind(cb.call_load_db, button_load.param.clicks)\n",
    "conversation = pn.bind(cb.convchain, inp) \n",
    "\n",
    "jpg_pane = pn.pane.Image( './img/convchain.jpg')\n",
    "\n",
    "tab1 = pn.Column(\n",
    "    pn.Row(inp),\n",
    "    pn.layout.Divider(),\n",
    "    pn.panel(conversation,  loading_indicator=True, height=300),\n",
    "    pn.layout.Divider(),\n",
    ")\n",
    "tab2= pn.Column(\n",
    "    pn.panel(cb.get_lquest),\n",
    "    pn.layout.Divider(),\n",
    "    pn.panel(cb.get_sources ),\n",
    ")\n",
    "tab3= pn.Column(\n",
    "    pn.panel(cb.get_chats),\n",
    "    pn.layout.Divider(),\n",
    ")\n",
    "tab4=pn.Column(\n",
    "    pn.Row( file_input, button_load, bound_button_load),\n",
    "    pn.Row( button_clearhistory, pn.pane.Markdown(\"Clears chat history. Can use to start a new topic\" )),\n",
    "    pn.layout.Divider(),\n",
    "    pn.Row(jpg_pane.clone(width=400))\n",
    ")\n",
    "dashboard = pn.Column(\n",
    "    pn.Row(pn.pane.Markdown('# ChatWithYourData_Bot')),\n",
    "    pn.Tabs(('Conversation', tab1), ('Database', tab2), ('Chat History', tab3),('Configure', tab4))\n",
    ")\n",
    "dashboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
