# å¥—ä»¶
import os 
import pandas as pd
from openai import OpenAI # openai api å®¢æˆ¶ç«¯
import shutil  # æ›¿ä»£ !rm æŒ‡ä»¤
from dotenv import load_dotenv, find_dotenv # dotenv æ˜¯å°ˆé–€ç”¨ä¾†è®€å–.envå¥—ä»¶çš„å¥—ä»¶ï¼Œä¸¦æ¥ä¸Šç’°å¢ƒ
from langchain.document_loaders import PyPDFLoader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma


def setup_enviroment():

    # ä½œæ¥­ç³»çµ±ç›¸é—œåŠŸèƒ½ï¼ˆè®€å–ç’°å¢ƒè®Šæ•¸ï¼‰
    _ = load_dotenv(find_dotenv()) # è®€å–.envæª”æ¡ˆ
    client = OpenAI( api_key=os.environ['OPENAI_API_KEY'])
    print("done")

    # æ¸…é™¤èˆŠçš„ç’°å¢ƒè®Šæ•¸
    if 'OPENAI_API_KEY' in os.environ:
        del os.environ['OPENAI_API_KEY']
    # é‡æ–°è¼‰å…¥
    load_dotenv()
    api_key = os.environ.get('OPENAI_API_KEY')

    if api_key:
        print("âœ… Success! API key loaded")
        print(f"Key starts with: {api_key[:15]}...")
        print(f"Key length: {len(api_key)} characters")
    else:
        print("âŒ Still not working")



def load_metadata_mapping(csv_path):

    df = pd.read_csv(csv_path)
    df.columns = df.columns.str.strip() # df.columsæ˜¯ç‰©ä»¶ï¼ŒåŠ ä¸Š'.str()'è½‰æ›æˆé™£åˆ—æ‰å¯ä»¥ä½¿ç”¨.strip()
    # å°‡filenameä½œç‚ºkeyï¼Œå…¶ä»–è³‡è¨Šä½œç‚ºvalue
    metadata_map = {}
    for _, row in df.iterrows():
        metadata_map[row['filename']] = {
            'title': row['title'],
            'year': row['year'],
            'authors': row['authors'],
            'topic': row['topic']
        }
    return metadata_map

def filter_first_page_only(documents):
    """åªä¿ç•™ç¬¬ä¸€é ï¼ˆAbstractï¼‰"""
    first_page_docs = [doc for doc in documents if doc.metadata['page'] == 0]
    print(f"åŸå§‹é æ•¸ï¼š{len(documents)} â†’ åªä¿ç•™ç¬¬ä¸€é ï¼š{len(first_page_docs)} é ")
    return first_page_docs


def load_all_first_pages_with_csv_metadata(folder_path, csv_path):
    """è¼‰å…¥PDFç¬¬ä¸€é ä¸¦å¾CSVå°æ‡‰metadata
    æ¥æ”¶å…©å€‹åƒæ•¸ï¼šfolder_path, csv_path
    ä¸€ã€load_metadata_mappingï¼šè®€å– csv è£¡é¢çš„ metadata å°æ‡‰è¡¨
    äºŒã€è®€å– folder_pathï¼šè®€å–è³‡æ–™å¤¾å…§çš„pdf
    ä¸‰ã€å›åœˆè™•ç†æ¯ä¸€ä»½pdfï¼šä½¿ç”¨ PyPDFLoader ï¼Œä¸¦é€é def filter_first_page_only åªä¿ç•™ç¬¬ä¸€é 
    å››ã€å¾ csv å°æ‡‰ metadata
    äº”ã€è¿”å›ã€Œåªæœ‰ç¬¬ä¸€é ä¸”ä¸€å®šåŒ…å«æŒ‡å®šæ¬„ä½ã€ä»¥è½‰æˆdocumentçš„pdfã€
    """
    
    # 1. å…ˆè®€å–metadataå°æ‡‰è¡¨
    metadata_map = load_metadata_mapping(csv_path)
    print(f"è¼‰å…¥metadataå°æ‡‰è¡¨ï¼Œå…± {len(metadata_map)} ç­†è³‡æ–™")
    
    all_first_pages = []
    pdf_files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]
    
    for pdf_file in pdf_files:
        print(f"è™•ç†ï¼š{pdf_file}")
        
        file_path = os.path.join(folder_path, pdf_file)
        loader = PyPDFLoader(file_path)
        documents = loader.load()
        
        # åªå–ç¬¬ä¸€é 
        first_page_docs = filter_first_page_only(documents)
        
        # å¾CSVå°æ‡‰metadata
        for doc in first_page_docs:
            if pdf_file in metadata_map:
                # æ‰¾åˆ°å°æ‡‰çš„metadata
                doc.metadata.update(metadata_map[pdf_file])
                print(f"  âœ… å·²æ›´æ–°metadata: {metadata_map[pdf_file]['title']}")
            else:
                # æ‰¾ä¸åˆ°å°æ‡‰è³‡æ–™
                print(f"  âš ï¸  è­¦å‘Šï¼š{pdf_file} åœ¨CSVä¸­æ‰¾ä¸åˆ°å°æ‡‰è³‡æ–™")
                doc.metadata.update({
                    'title': pdf_file.replace('.pdf', ''),
                    'year': None,
                    'authors': 'Unknown',
                    'topic': 'Unknown'
                })
        all_first_pages.extend(first_page_docs)
    print(f"ç¸½å…±è¼‰å…¥ {len(all_first_pages)} å€‹ç¬¬ä¸€é ï¼Œmetadataå·²æ›´æ–°")
    return all_first_pages


 
def clean_metadata(metadata): # åªæ¥æ”¶å–®ä¸€doc.metadataï¼Œæ‰€ä»¥æ˜¯clean_all_documents_metadata çš„å­å‡½å¼
    """æ¸…ç†ä¸¦æ¨™æº–åŒ–metadata
    ä¸€ã€æ¥æ”¶ def load_all_first_pages_with_csv_metadata è™•ç†å®Œçš„ä¸€é æ–‡æª”
    äºŒã€æŒ‡å®šä¿ç•™çš„keys
    ä¸‰ã€æ¯ä»½
    """
    keep_keys = ['source', 'total_pages', 'title', 'page', 'year', 'authors', 'topic']
    
    clean_meta = {}
    for key in keep_keys:
        if key in metadata:
            clean_meta[key] = metadata[key]
    
    return clean_meta

def clean_all_documents_metadata(documents):
    '''æ¸…ç†æ‰€æœ‰æ–‡ä»¶çš„metadata'''
    for doc in documents:
        doc.metadata = clean_metadata(doc.metadata)
    print(f"finish  cleaning {len(documents)} metadata ")
    return documents



def split_document(documents):
    """åˆ†å‰²æª”æ¡ˆ
    ä¸€ã€è¼¸å…¥æ¸…ç†å¥½çš„clean_documents
    äºŒã€ä½¿ç”¨RecursiveCharacterTextSplitteråˆ†å‰²æ–‡ä»¶
    """
    # åˆ†å‰²æª”æ¡ˆ
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, 
        chunk_overlap=50,
        separators=[ "\n\n", ". ", "\n", "(?<=\. )", " ", ""]
        )    
    docs = text_splitter.split_documents(documents)
    print(f"split {len(docs)} documents") 
    print(f"end up {len(documents)} chunks") 



def create_vectordb(documents, persist_directory='./chroma_db'):
    '''å»ºç«‹å‘é‡è³‡æ–™åº«
    ä¸€ã€å»ºç«‹è³‡æ–™å¤¾
    äºŒã€æ¸…é™¤èˆŠçš„è³‡æ–™åº«ï¼Œç¢ºä¿è³‡æ–™ä¹¾æ·¨
    ä¸‰ã€å»ºç«‹embedding
    å››ã€å»ºç«‹å‘é‡è³‡æ–™åº«

    '''
    # å»ºç«‹è³‡æ–™å¤¾
    os.makedirs(persist_directory, exist_ok=True)
    print("folder exist")

    # æ¸…é™¤èˆŠçš„è³‡æ–™åº«ï¼Œç¢ºä¿è³‡æ–™ä¹¾æ·¨
    if os.path.exists(persist_directory):
        shutil.rmtree(persist_directory)
        os.makedirs(persist_directory, exist_ok=True)
        print("ğŸ—‘ï¸ å·²æ¸…é™¤èˆŠè³‡æ–™åº«")      

    # define embedding
    embeddings = OpenAIEmbeddings()

    # å»ºç«‹æ–°çš„å‘é‡è³‡æ–™åº«ï¼Œä¸¦å°‡æ–‡ä»¶æ”¾é€²å»

    vectordb = Chroma.from_documents(
        documents=documents, # å‰›å‰›å‚³å…¥çš„åƒæ•¸æª”æ¡ˆ
        embedding=embeddings,
        persist_directory=persist_directory
    )
    print(f"fininsh creating vectordb, contains {len(documents)} chunks")
    return vectordb


# ä½¿ç”¨ def mian() æ•´åˆæ•¸æ“šæµ
def main():
    """åŸ·è¡Œä¸»è¦æµç¨‹"""
    print("start buliding RAG database...")

    # step1 - setting enviorment
    if not setup_enviroment():
        return False

    # step2 - setting path
    folder_path = "/Users/mangtinglee/Desktop/2025_gap_careerpath/RAG_LLM/pdfs" #é€™å…©å¡Šå¯èƒ½é€£åˆ°githubä¹‹å¾Œè¦æ”¹
    csv_path = "/Users/mangtinglee/Desktop/2025_gap_careerpath/RAG_LLM/meta_data_correction.csv"

        # checking path
    if not os.path.exists(folder_path):
        print(f"not found pdf folder: {folder_path}")
        return False
        
    if not os.path.exists(csv_path):
        print(f"not found pdf folder: {csv_path}")
        return False

    try:
        # step3: loading pdf (data strem starting...)
        print("\n step 1: loading documents")
        documents = load_all_first_pages_with_csv_metadata(folder_path, csv_path)

        # step4: clean matadata
        print('\n step 2: clean metadata')
        clean_documents = clean_all_documents_metadata(documents)

        # step5: spliting document
        print("\n step 3: start spliting")
        split_docs = split_document(clean_documents)

        # step6: creat vector database
        print("\n step 4: creat vector database")
        vectordb = create_vectordb(split_docs)

    except Exception as e:
        print(f"error:{str(e)}")
        return False


if __name__ == "__main__":
    success = main()
    if success:
        print("finish data processing, now can execute streamlit_ui.py. ")
    else:
        print("error")
    

