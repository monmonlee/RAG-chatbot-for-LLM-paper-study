{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹æ•´é«”çš„uiä»‹é¢ï¼Œè®Šæˆä¸€å€‹å•ç­”æ©Ÿå™¨äºº\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA,  ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "ç›®çš„ï¼šå»ºç«‹langchain + openai çš„åŸºç¤ç’°å¢ƒ\n",
    "'''\n",
    "import os # ä½œæ¥­ç³»çµ±ç›¸é—œåŠŸèƒ½ï¼ˆè®€å–ç’°å¢ƒè®Šæ•¸ï¼‰\n",
    "from openai import OpenAI # openai api å®¢æˆ¶ç«¯\n",
    "from dotenv import load_dotenv, find_dotenv # dotenv æ˜¯å°ˆé–€ç”¨ä¾†è®€å–.envå¥—ä»¶çš„å¥—ä»¶ï¼Œä¸¦æ¥ä¸Šç’°å¢ƒ\n",
    "_ = load_dotenv(find_dotenv()) # è®€å–.envæª”æ¡ˆ\n",
    "client = OpenAI(\n",
    "    api_key=os.environ['OPENAI_API_KEY']\n",
    ")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Success! API key loaded\n",
      "Key starts with: sk-proj-FWUUter...\n",
      "Key length: 164 characters\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# æ¸…é™¤èˆŠçš„ç’°å¢ƒè®Šæ•¸\n",
    "if 'OPENAI_API_KEY' in os.environ:\n",
    "    del os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# é‡æ–°è¼‰å…¥\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.environ.get('OPENAI_API_KEY')\n",
    "if api_key:\n",
    "    print(\"âœ… Success! API key loaded\")\n",
    "    print(f\"Key starts with: {api_key[:15]}...\")\n",
    "    print(f\"Key length: {len(api_key)} characters\")\n",
    "else:\n",
    "    print(\"âŒ Still not working\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder exists: True\n"
     ]
    }
   ],
   "source": [
    "# æª¢æŸ¥è³‡æ–™å¤¾æ˜¯å¦å­˜åœ¨\n",
    "import os\n",
    "folder_path = \"/Users/mangtinglee/Desktop/2025_gap_careerpath/RAG_LLM/pdfs\"\n",
    "\n",
    "print(f\"Folder exists: {os.path.exists(folder_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¼‰å…¥metadataå°æ‡‰è¡¨ï¼Œå…± 24 ç­†è³‡æ–™\n",
      "è™•ç†ï¼š2023_LLM limitation_Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions.pdf\n",
      "åŸå§‹é æ•¸ï¼š11 â†’ åªä¿ç•™ç¬¬ä¸€é ï¼š1 é \n",
      "  âœ… å·²æ›´æ–°metadata: Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions\n",
      "è™•ç†ï¼š2025_LLM limitation_The Order Effect- Investigating Prompt Sensitivity to Input Order in LLMs.pdf\n",
      "åŸå§‹é æ•¸ï¼š13 â†’ åªä¿ç•™ç¬¬ä¸€é ï¼š1 é \n",
      "  âœ… å·²æ›´æ–°metadata: The Order Effect- Investigating Prompt Sensitivity to Input Order in LLMs\n",
      "è™•ç†ï¼š2024_RAG_Evaluation of Retrieval-Augmented Generation- A Survey.pdf\n",
      "åŸå§‹é æ•¸ï¼š21 â†’ åªä¿ç•™ç¬¬ä¸€é ï¼š1 é \n",
      "  âœ… å·²æ›´æ–°metadata: Evaluation of Retrieval-Augmented Generation- A Survey\n",
      "è™•ç†ï¼š2020_scaling laws_Scaling Laws for Neural Language Models.pdf\n",
      "åŸå§‹é æ•¸ï¼š30 â†’ åªä¿ç•™ç¬¬ä¸€é ï¼š1 é \n",
      "  âœ… å·²æ›´æ–°metadata: Scaling Laws for Neural Language Models\n",
      "è™•ç†ï¼š2022_LLM limitation_Robustness of Learning from Task Instructions.pdf\n",
      "åŸå§‹é æ•¸ï¼š12 â†’ åªä¿ç•™ç¬¬ä¸€é ï¼š1 é \n",
      "  âœ… å·²æ›´æ–°metadata: Robustness of Learning from Task Instructions\n",
      "è™•ç†ï¼š2024_RAG_Retrieval-Augmented Generation for Large Language Models- A Survey.pdf\n",
      "åŸå§‹é æ•¸ï¼š21 â†’ åªä¿ç•™ç¬¬ä¸€é ï¼š1 é \n",
      "  âœ… å·²æ›´æ–°metadata: Retrieval-Augmented Generation for Large Language Models- A Survey\n",
      "è™•ç†ï¼š2017_transformer_Attention Is All You Need.pdf\n",
      "åŸå§‹é æ•¸ï¼š15 â†’ åªä¿ç•™ç¬¬ä¸€é ï¼š1 é \n",
      "  âœ… å·²æ›´æ–°metadata: Attention Is All You Need\n",
      "è™•ç†ï¼š2024_LLM limitation_Evaluating the Zero-shot Robustness of Instruction-tuned Language Models.pdf\n",
      "åŸå§‹é æ•¸ï¼š127 â†’ åªä¿ç•™ç¬¬ä¸€é ï¼š1 é \n",
      "  âœ… å·²æ›´æ–°metadata: Evaluating the Zero-shot Robustness of Instruction-tuned Language Models\n",
      "è™•ç†ï¼š2024_RAG_Modular RAG- Transforming RAG Systems into LEGO-like Reconfigurable Frameworks.pdf\n",
      "åŸå§‹é æ•¸ï¼š17 â†’ åªä¿ç•™ç¬¬ä¸€é ï¼š1 é \n",
      "  âœ… å·²æ›´æ–°metadata: Modular RAG- Transforming RAG Systems into LEGO-like Reconfigurable Frameworks\n",
      "è™•ç†ï¼š2024_LLM limitation_LLaMA Beyond English- An Empirical Study on Language Capability Transfer.pdf\n",
      "åŸå§‹é æ•¸ï¼š10 â†’ åªä¿ç•™ç¬¬ä¸€é ï¼š1 é \n",
      "  âœ… å·²æ›´æ–°metadata: LLaMA Beyond English- An Empirical Study on Language Capability Transfer\n",
      "è™•ç†ï¼š2024_LLM limitation_Language Ranker- A Metric for Quantifying LLM Performance Across High and Low-Resource Languages.pdf\n",
      "åŸå§‹é æ•¸ï¼š11 â†’ åªä¿ç•™ç¬¬ä¸€é ï¼š1 é \n",
      "  âœ… å·²æ›´æ–°metadata: Language Ranker- A Metric for Quantifying LLM Performance Across High and Low-Resource Languages\n",
      "è™•ç†ï¼š2023_LLM limitation_Evaluating the Zero-shot Robustness of Instruction-tuned Language Models.pdf\n",
      "åŸå§‹é æ•¸ï¼š15 â†’ åªä¿ç•™ç¬¬ä¸€é ï¼š1 é \n",
      "  âœ… å·²æ›´æ–°metadata: Evaluating the Zero-shot Robustness of Instruction-tuned Language Models\n",
      "è™•ç†ï¼š2024_RAG_The Power of Noise- Redefining Retrieval for RAG Systems.pdf\n",
      "åŸå§‹é æ•¸ï¼š11 â†’ åªä¿ç•™ç¬¬ä¸€é ï¼š1 é \n",
      "  âœ… å·²æ›´æ–°metadata: The Power of Noise- Redefining Retrieval for RAG Systems\n",
      "è™•ç†ï¼š2022_LLM limitation_The Reversal Curse- LLMs trained on \"A is B\" fail to learn \"B is A\".pdf\n",
      "åŸå§‹é æ•¸ï¼š21 â†’ åªä¿ç•™ç¬¬ä¸€é ï¼š1 é \n",
      "  âœ… å·²æ›´æ–°metadata: The Reversal Curse- LLMs trained on \"A is B\" fail to learn \"B is A\"\n",
      "è™•ç†ï¼š2022_fine tuning_Scaling Instruction-Finetuned Language Models.pdf\n",
      "åŸå§‹é æ•¸ï¼š54 â†’ åªä¿ç•™ç¬¬ä¸€é ï¼š1 é \n",
      "  âœ… å·²æ›´æ–°metadata: Scaling Instruction-Finetuned Language Models\n",
      "è™•ç†ï¼š2023_LLM limitation_Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or- How I learned to start worrying about prompt formatting.pdf\n",
      "åŸå§‹é æ•¸ï¼š29 â†’ åªä¿ç•™ç¬¬ä¸€é ï¼š1 é \n",
      "  âœ… å·²æ›´æ–°metadata: Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or- How I learned to start worrying about prompt formatting\n",
      "è™•ç†ï¼š2024_LLM limitation_Does Prompt Formatting Have Any Impact on LLM Performance?.pdf\n",
      "åŸå§‹é æ•¸ï¼š16 â†’ åªä¿ç•™ç¬¬ä¸€é ï¼š1 é \n",
      "  âœ… å·²æ›´æ–°metadata: Does Prompt Formatting Have Any Impact on LLM Performance?\n",
      "è™•ç†ï¼š2023_LLM application_Challenges and Applications of Large Language Models.pdf\n",
      "åŸå§‹é æ•¸ï¼š72 â†’ åªä¿ç•™ç¬¬ä¸€é ï¼š1 é \n",
      "  âœ… å·²æ›´æ–°metadata: Challenges and Applications of Large Language Models\n",
      "è™•ç†ï¼š2023_scaling laws_BloombergGPT- A Large Language Model for Finance.pdf\n",
      "åŸå§‹é æ•¸ï¼š76 â†’ åªä¿ç•™ç¬¬ä¸€é ï¼š1 é \n",
      "  âœ… å·²æ›´æ–°metadata: BloombergGPT- A Large Language Model for Finance\n",
      "è™•ç†ï¼š2025_ai-agent_Large Language Model Agent- A Survey on Methodology, Applications and Challenges.pdf\n",
      "åŸå§‹é æ•¸ï¼š26 â†’ åªä¿ç•™ç¬¬ä¸€é ï¼š1 é \n",
      "  âœ… å·²æ›´æ–°metadata: Large Language Model Agent- A Survey on Methodology, Applications and Challenges\n",
      "è™•ç†ï¼š2024_fine tuning_Scaling Down to Scale Up- A Guide to Parameter-Efficient Fine-Tuning.pdf\n",
      "åŸå§‹é æ•¸ï¼š57 â†’ åªä¿ç•™ç¬¬ä¸€é ï¼š1 é \n",
      "  âœ… å·²æ›´æ–°metadata: Scaling Down to Scale Up- A Guide to Parameter-Efficient Fine-Tuning\n",
      "è™•ç†ï¼š2024_LLM limitation_Spanish and LLM Benchmarks- is MMLU Lost in Translation?.pdf\n",
      "åŸå§‹é æ•¸ï¼š8 â†’ åªä¿ç•™ç¬¬ä¸€é ï¼š1 é \n",
      "  âœ… å·²æ›´æ–°metadata: Spanish and LLM Benchmarks- is MMLU Lost in Translation?\n",
      "è™•ç†ï¼š2024_prompt engineering_The Prompt Report - A Systematic Survey of Prompt Engineering Techniques.pdf\n",
      "åŸå§‹é æ•¸ï¼š80 â†’ åªä¿ç•™ç¬¬ä¸€é ï¼š1 é \n",
      "  âœ… å·²æ›´æ–°metadata: The Prompt Report - A Systematic Survey of Prompt Engineering Techniques\n",
      "è™•ç†ï¼š2023_LLM limitation_Lost in the Middle- How Language Models Use Long Contexts.pdf\n",
      "åŸå§‹é æ•¸ï¼š18 â†’ åªä¿ç•™ç¬¬ä¸€é ï¼š1 é \n",
      "  âœ… å·²æ›´æ–°metadata: Lost in the Middle- How Language Models Use Long Contexts\n",
      "ç¸½å…±è¼‰å…¥ 24 å€‹ç¬¬ä¸€é ï¼Œmetadataå·²æ›´æ–°\n",
      "\n",
      "--- Document 1 ---\n",
      "Title: Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions\n",
      "Year: 2023\n",
      "Authors: Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions\n",
      "Topic: LLM limitation\n",
      "\n",
      "--- Document 2 ---\n",
      "Title: The Order Effect- Investigating Prompt Sensitivity to Input Order in LLMs\n",
      "Year: 2025\n",
      "Authors: Bryan Guan,Â Tanya Roosta,Â Peyman Passban,Â Mehdi Rezagholizadeh\n",
      "Topic: LLM limitation\n",
      "\n",
      "--- Document 3 ---\n",
      "Title: Evaluation of Retrieval-Augmented Generation- A Survey\n",
      "Year: 2024\n",
      "Authors: Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, Zhaofeng Liu\n",
      "Topic: RAG\n"
     ]
    }
   ],
   "source": [
    "# åŸ·è¡Œ\n",
    "import pandas as pd\n",
    "\n",
    "def load_metadata_mapping(csv_path):\n",
    "    \"\"\"è®€å–CSVæª”æ¡ˆå»ºç«‹metadataå°æ‡‰è¡¨\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df.columns = df.columns.str.strip() # df.columsæ˜¯ç‰©ä»¶ï¼ŒåŠ ä¸Š'.str()'è½‰æ›æˆé™£åˆ—æ‰å¯ä»¥ä½¿ç”¨.strip()\n",
    "    # å°‡filenameä½œç‚ºkeyï¼Œå…¶ä»–è³‡è¨Šä½œç‚ºvalue\n",
    "    metadata_map = {}\n",
    "    for _, row in df.iterrows():\n",
    "        metadata_map[row['filename']] = {\n",
    "            'title': row['title'],\n",
    "            'year': row['year'],\n",
    "            'authors': row['authors'],\n",
    "            'topic': row['topic']\n",
    "        }\n",
    "    return metadata_map\n",
    "\n",
    "def filter_first_page_only(documents):\n",
    "    \"\"\"åªä¿ç•™ç¬¬ä¸€é ï¼ˆAbstractï¼‰\"\"\"\n",
    "    first_page_docs = [doc for doc in documents if doc.metadata['page'] == 0]\n",
    "    print(f\"åŸå§‹é æ•¸ï¼š{len(documents)} â†’ åªä¿ç•™ç¬¬ä¸€é ï¼š{len(first_page_docs)} é \")\n",
    "    return first_page_docs\n",
    "\n",
    "def load_all_first_pages_with_csv_metadata(folder_path, csv_path):\n",
    "    \"\"\"è¼‰å…¥PDFç¬¬ä¸€é ä¸¦å¾CSVå°æ‡‰metadata\"\"\"\n",
    "    \n",
    "    # 1. å…ˆè®€å–metadataå°æ‡‰è¡¨\n",
    "    metadata_map = load_metadata_mapping(csv_path)\n",
    "    print(f\"è¼‰å…¥metadataå°æ‡‰è¡¨ï¼Œå…± {len(metadata_map)} ç­†è³‡æ–™\")\n",
    "    \n",
    "    all_first_pages = []\n",
    "    pdf_files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"è™•ç†ï¼š{pdf_file}\")\n",
    "        \n",
    "        file_path = os.path.join(folder_path, pdf_file)\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        \n",
    "        # åªå–ç¬¬ä¸€é \n",
    "        first_page_docs = filter_first_page_only(documents)\n",
    "        \n",
    "        # ğŸ¯ é—œéµï¼šå¾CSVå°æ‡‰metadata\n",
    "        for doc in first_page_docs:\n",
    "            if pdf_file in metadata_map:\n",
    "                # æ‰¾åˆ°å°æ‡‰çš„metadata\n",
    "                doc.metadata.update(metadata_map[pdf_file])\n",
    "                print(f\"  âœ… å·²æ›´æ–°metadata: {metadata_map[pdf_file]['title']}\")\n",
    "            else:\n",
    "                # æ‰¾ä¸åˆ°å°æ‡‰è³‡æ–™\n",
    "                print(f\"  âš ï¸  è­¦å‘Šï¼š{pdf_file} åœ¨CSVä¸­æ‰¾ä¸åˆ°å°æ‡‰è³‡æ–™\")\n",
    "                doc.metadata.update({\n",
    "                    'title': pdf_file.replace('.pdf', ''),\n",
    "                    'year': None,\n",
    "                    'authors': 'Unknown',\n",
    "                    'topic': 'Unknown'\n",
    "                })\n",
    "        \n",
    "        all_first_pages.extend(first_page_docs)\n",
    "    \n",
    "    print(f\"ç¸½å…±è¼‰å…¥ {len(all_first_pages)} å€‹ç¬¬ä¸€é ï¼Œmetadataå·²æ›´æ–°\")\n",
    "\n",
    "    # all_first_pages = clean_metadata(all_first_pages)\n",
    "\n",
    "    return all_first_pages\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ä½¿ç”¨\n",
    "\n",
    "csv_path = \"/Users/mangtinglee/Desktop/2025_gap_careerpath/RAG_LLM/meta_data_correction.csv\"\n",
    "all_abstracts_with_metadata = load_all_first_pages_with_csv_metadata(folder_path, csv_path)\n",
    "# all_abstracts_with_metadata = clean_metadata(all_abstracts_with_metadata)\n",
    "\n",
    "for i, doc in enumerate(all_abstracts_with_metadata[:3]):\n",
    "    print(f\"\\n--- Document {i+1} ---\")\n",
    "    print(f\"Title: {doc.metadata.get('title', 'N/A')}\")\n",
    "    print(f\"Year: {doc.metadata.get('year', 'N/A')}\")\n",
    "    print(f\"Authors: {doc.metadata.get('authors', 'N/A')}\")\n",
    "    print(f\"Topic: {doc.metadata.get('topic', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n",
      "{'source': '/Users/mangtinglee/Desktop/2025_gap_careerpath/RAG_LLM/pdfs/2020_scaling laws_Scaling Laws for Neural Language Models.pdf', 'total_pages': 30, 'title': 'Scaling Laws for Neural Language Models', 'page': 0, 'year': 2020, 'authors': 'Jared Kaplan,\\xa0Sam McCandlish,\\xa0Tom Henighan,\\xa0Tom B. Brown,\\xa0Benjamin Chess,\\xa0Rewon Child,\\xa0Scott Gray,\\xa0Alec Radford,\\xa0Jeffrey Wu,\\xa0Dario Amodei', 'topic': 'scaling laws'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nç‚ºä»€éº¼é€™æ¨£å¯«ï¼Ÿ\\nå› ç‚ºè¼‰å…¥å¾Œæœƒè®Šæˆlangchainçš„documentå½¢å¼ï¼Œä¾‹å¦‚ï¼š\\n    load_result = [\\n        Document(page_content = '...', metedata = {}),\\n        Document(page_content = '...', metedata = {}),\\n        Document(page_content = '...', metedata = {})                \\n    ]\\næ‰€ä»¥ï¼Œç•¶æˆ‘å€‘è¦ä¿®æ”¹metedataæ ¼å¼æ™‚ï¼Œéœ€è¦é€ééæ­·çš„æ–¹å¼è™•ç†ï¼ˆå› ç‚ºdef clean_metadataåªåƒã€Œå–®ä¸€Documentçš„metadataï¼ˆå­—å…¸å‹æ…‹ï¼‰ã€ï¼‰\\nå› æ­¤æœ€ç›´æ¥çš„åšæ³•å°±æ˜¯\\nfor doc in result:\\n    doc.metadata = clean_metadata(doc.metadata)\\n\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_metadata(metadata): # åªæ¥æ”¶å–®ä¸€doc.metadata\n",
    "    \"\"\"æ¸…ç†ä¸¦æ¨™æº–åŒ–metadata\"\"\"\n",
    "    keep_keys = ['source', 'total_pages', 'title', 'page', 'year', 'authors', 'topic']\n",
    "    \n",
    "    clean_meta = {}\n",
    "    for key in keep_keys:\n",
    "        if key in metadata:\n",
    "            clean_meta[key] = metadata[key]\n",
    "    \n",
    "    return clean_meta\n",
    "\n",
    "for doc in all_abstracts_with_metadata:\n",
    "    doc.metadata = clean_metadata(doc.metadata)\n",
    "print(\"done!\")\n",
    "\n",
    "print(all_abstracts_with_metadata[3].metadata)\n",
    "\n",
    "\"\"\"\n",
    "ç‚ºä»€éº¼é€™æ¨£å¯«ï¼Ÿ\n",
    "å› ç‚ºè¼‰å…¥å¾Œæœƒè®Šæˆlangchainçš„documentå½¢å¼ï¼Œä¾‹å¦‚ï¼š\n",
    "    load_result = [\n",
    "        Document(page_content = '...', metedata = {}),\n",
    "        Document(page_content = '...', metedata = {}),\n",
    "        Document(page_content = '...', metedata = {})                \n",
    "    ]\n",
    "æ‰€ä»¥ï¼Œç•¶æˆ‘å€‘è¦ä¿®æ”¹metedataæ ¼å¼æ™‚ï¼Œéœ€è¦é€ééæ­·çš„æ–¹å¼è™•ç†ï¼ˆå› ç‚ºdef clean_metadataåªåƒã€Œå–®ä¸€Documentçš„metadataï¼ˆå­—å…¸å‹æ…‹ï¼‰ã€ï¼‰\n",
    "å› æ­¤æœ€ç›´æ¥çš„åšæ³•å°±æ˜¯\n",
    "for doc in result:\n",
    "    doc.metadata = clean_metadata(doc.metadata)\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "# åˆ†å‰²æª”æ¡ˆ\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=50,\n",
    "    separators=[ \"\\n\\n\", \". \", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
    "    )    \n",
    "docs = text_splitter.split_documents(all_abstracts_with_metadata)\n",
    "print(len(docs)) # chunks\n",
    "print(len(all_abstracts_with_metadata)) # test 11 pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': '/Users/mangtinglee/Desktop/2025_gap_careerpath/RAG_LLM/pdfs/2023_LLM limitation_Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions.pdf', 'total_pages': 11, 'title': 'Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions', 'page': 0, 'year': 2023, 'authors': 'Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions', 'topic': 'LLM limitation'}, page_content='Large Language Models Sensitivity to The Order of Options in\\nMultiple-Choice Questions\\nPouya Pezeshkpour\\nMegagon Labs\\npouya@megagon.ai\\nEstevam Hruschka\\nMegagon Labs\\nestevam@megagon.ai\\nAbstract\\nLarge Language Models (LLMs) have demon-\\nstrated remarkable capabilities in various NLP\\ntasks. However, previous works have shown\\nthese models are sensitive towards prompt\\nwording, and few-shot demonstrations and\\ntheir order, posing challenges to fair assess-\\nment of these models. As these models be-\\ncome more powerful, it becomes imperative\\nto understand and address these limitations.\\nIn this paper, we focus on LLMs robust-\\nness on the task of multiple-choice questionsâ€”\\ncommonly adopted task to study reasoning and\\nfact-retrieving capability of LLMs')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finishï¼\n",
      "è³‡æ–™å¤¾å­˜åœ¨å—ï¼ŸTrue\n"
     ]
    }
   ],
   "source": [
    "# step.3 - embedding\n",
    "\n",
    "import os\n",
    "\n",
    "# å»ºç«‹è³‡æ–™å¤¾\n",
    "os.makedirs('./chroma_db', exist_ok=True)\n",
    "print(\"finishï¼\")\n",
    "\n",
    "# æª¢æŸ¥æ˜¯å¦æˆåŠŸ\n",
    "print(f\"è³‡æ–™å¤¾å­˜åœ¨å—ï¼Ÿ{os.path.exists('./chroma_db')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define embedding\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ³¨æ„ï¼Œéœ€è¦å…ˆåœ¨è‡ªå·±çš„ç’°å¢ƒä¸­å»ºç«‹è³‡æ–™åº«è·¯å¾‘\n",
    "persist_directory = './chroma_db' # æŒ‡å®šè³‡æ–™åº«è·¯å¾‘\n",
    "!rm -rf ./chroma_db  # remove old database files if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹æ–°çš„å‘é‡è³‡æ–™åº«ï¼Œä¸¦å°‡æ–‡ä»¶æ”¾é€²å»\n",
    "from langchain.vectorstores import Chroma\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_x/xcmm2w3d1hld6_nc6t0xyrch0000gn/T/ipykernel_58661/4092891401.py:1: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist() # æ‰‹å‹•å„²å­˜å‰›å‰›å»ºç«‹çš„è³‡æ–™åº«ï¼ˆè·Ÿç›¸ä¼¼åº¦æ²’æœ‰é—œä¿‚ï¼Œæ˜¯å„²å­˜è³‡æ–™åº«ï¼‰\n"
     ]
    }
   ],
   "source": [
    "vectordb.persist() # æ‰‹å‹•å„²å­˜å‰›å‰›å»ºç«‹çš„è³‡æ–™åº«ï¼ˆç¾åœ¨ä¸ç”¨æ‰‹å‹•äº†ï¼Œè‡ªå‹•å„²å­˜ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ¯æ¬¡é–‹å§‹æ–°sessionåªéœ€è¦ï¼ˆä¸éœ€è¦ç¶“éå‰é¢çš„éšæ®µï¼‰ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting langchain_chroma\n",
      "  Downloading langchain_chroma-0.2.6-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting langchain-core>=0.3.76 (from langchain_chroma)\n",
      "  Downloading langchain_core-0.3.76-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from langchain_chroma) (2.0.2)\n",
      "Requirement already satisfied: chromadb>=1.0.20 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from langchain_chroma) (1.0.20)\n",
      "Requirement already satisfied: build>=1.0.3 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (1.3.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (2.10.3)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (1.4.2)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.20->langchain_chroma) (0.35.0)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (5.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (4.12.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (1.19.2)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (1.36.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (0.22.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (1.71.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (0.17.3)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (33.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (8.2.3)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (6.0.1)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (3.11.2)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (14.0.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (4.20.0)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from langchain-core>=0.3.76->langchain_chroma) (0.4.14)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from langchain-core>=0.3.76->langchain_chroma) (1.33)\n",
      "Requirement already satisfied: packaging>=23.2 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from langchain-core>=0.3.76->langchain_chroma) (25.0)\n",
      "Requirement already satisfied: pyproject_hooks in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from build>=1.0.3->chromadb>=1.0.20->langchain_chroma) (1.2.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from build>=1.0.3->chromadb>=1.0.20->langchain_chroma) (6.8.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from build>=1.0.3->chromadb>=1.0.20->langchain_chroma) (2.0.1)\n",
      "Requirement already satisfied: anyio in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from httpx>=0.27.0->chromadb>=1.0.20->langchain_chroma) (4.0.0)\n",
      "Requirement already satisfied: certifi in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from httpx>=0.27.0->chromadb>=1.0.20->langchain_chroma) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from httpx>=0.27.0->chromadb>=1.0.20->langchain_chroma) (1.0.7)\n",
      "Requirement already satisfied: idna in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from httpx>=0.27.0->chromadb>=1.0.20->langchain_chroma) (3.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb>=1.0.20->langchain_chroma) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.3.76->langchain_chroma) (2.4)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from jsonschema>=4.19.0->chromadb>=1.0.20->langchain_chroma) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from jsonschema>=4.19.0->chromadb>=1.0.20->langchain_chroma) (2023.11.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from jsonschema>=4.19.0->chromadb>=1.0.20->langchain_chroma) (0.31.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from jsonschema>=4.19.0->chromadb>=1.0.20->langchain_chroma) (0.13.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (1.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (2.8.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (2.40.3)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (1.6.4)\n",
      "Requirement already satisfied: requests in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (2.32.5)\n",
      "Requirement already satisfied: requests-oauthlib in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (3.3.1)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (2.1.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (0.10)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from langsmith>=0.3.45->langchain-core>=0.3.76->langchain_chroma) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from langsmith>=0.3.45->langchain-core>=0.3.76->langchain_chroma) (0.24.0)\n",
      "Requirement already satisfied: coloredlogs in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from onnxruntime>=1.14.1->chromadb>=1.0.20->langchain_chroma) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from onnxruntime>=1.14.1->chromadb>=1.0.20->langchain_chroma) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from onnxruntime>=1.14.1->chromadb>=1.0.20->langchain_chroma) (5.29.4)\n",
      "Requirement already satisfied: sympy in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from onnxruntime>=1.14.1->chromadb>=1.0.20->langchain_chroma) (1.14.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.20->langchain_chroma) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.36.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.20->langchain_chroma) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.36.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.20->langchain_chroma) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from opentelemetry-sdk>=1.2.0->chromadb>=1.0.20->langchain_chroma) (0.57b0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from posthog<6.0.0,>=2.4.0->chromadb>=1.0.20->langchain_chroma) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from posthog<6.0.0,>=2.4.0->chromadb>=1.0.20->langchain_chroma) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from pydantic>=1.9->chromadb>=1.0.20->langchain_chroma) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from pydantic>=1.9->chromadb>=1.0.20->langchain_chroma) (2.27.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from rich>=10.11.0->chromadb>=1.0.20->langchain_chroma) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from rich>=10.11.0->chromadb>=1.0.20->langchain_chroma) (2.17.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from tokenizers>=0.13.2->chromadb>=1.0.20->langchain_chroma) (0.34.4)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from typer>=0.9.0->chromadb>=1.0.20->langchain_chroma) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from typer>=0.9.0->chromadb>=1.0.20->langchain_chroma) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.20->langchain_chroma) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.20->langchain_chroma) (1.1.1)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.20->langchain_chroma) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.20->langchain_chroma) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.20->langchain_chroma) (15.0.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from importlib-resources->chromadb>=1.0.20->langchain_chroma) (3.17.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (4.9.1)\n",
      "Requirement already satisfied: filelock in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.20->langchain_chroma) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.20->langchain_chroma) (2025.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.20->langchain_chroma) (1.1.9)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=1.0.20->langchain_chroma) (0.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from requests->kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (3.3.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from anyio->httpx>=0.27.0->chromadb>=1.0.20->langchain_chroma) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from anyio->httpx>=0.27.0->chromadb>=1.0.20->langchain_chroma) (1.2.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb>=1.0.20->langchain_chroma) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from sympy->onnxruntime>=1.14.1->chromadb>=1.0.20->langchain_chroma) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (0.6.1)\n",
      "Downloading langchain_chroma-0.2.6-py3-none-any.whl (12 kB)\n",
      "Downloading langchain_core-0.3.76-py3-none-any.whl (447 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: langchain-core, langchain_chroma\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.75\n",
      "    Uninstalling langchain-core-0.3.75:\n",
      "      Successfully uninstalled langchain-core-0.3.75\n",
      "Successfully installed langchain-core-0.3.76 langchain_chroma-0.2.6\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain_chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# å•Ÿå‹•è³‡æ–™åº«\n",
    "import os # ä½œæ¥­ç³»çµ±ç›¸é—œåŠŸèƒ½ï¼ˆè®€å–ç’°å¢ƒè®Šæ•¸ï¼‰\n",
    "from openai import OpenAI # openai api å®¢æˆ¶ç«¯\n",
    "from dotenv import load_dotenv, find_dotenv # dotenv æ˜¯å°ˆé–€ç”¨ä¾†è®€å–.envå¥—ä»¶çš„å¥—ä»¶ï¼Œä¸¦æ¥ä¸Šç’°å¢ƒ\n",
    "_ = load_dotenv(find_dotenv()) # è®€å–.envæª”æ¡ˆ\n",
    "client = OpenAI(\n",
    "    api_key=os.environ['OPENAI_API_KEY']\n",
    ")\n",
    "print(\"done\")\n",
    "# from langchain.vectorstores import Chroma\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectordb = Chroma(persist_directory=\"./chroma_db\", embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å°ç­†è¨˜ï¼šmetadataã€Œä¸æ˜¯é è¨­æœƒè¢«å‘é‡åŒ–çš„å…§å®¹ã€ï¼Œé€šå¸¸åªæœ‰ page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å•é¡Œé©—è­‰ç’°ç¯€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  strategic decision\n",
    "##### å•é¡Œï¼šã€Œæˆ‘å€‘æ‡‰è©²å…ˆåœ¨ç›¸ä¼¼åº¦ä¸Šå–å¾—ä¸€å®šçš„çµæœå†å˜—è©¦é€²éšæª¢ç´¢æ–¹å¼ï¼Œé‚„æ˜¯å…ˆè©¦è©¦çœ‹é€²éšæª¢ç´¢æ–¹å¼çš„æƒ…æ³ï¼Ÿã€\n",
    "##### å›ç­”ï¼šæ‡‰è©²å…ˆå»ºç«‹ baseline\n",
    "- æœ‰äº†baselineï¼Œæ‰çŸ¥é“åŸºç¤æ˜¯å¦å·²å¯ç”¨ï¼Œé€²éšçš„æ”¹å–„äº†å¤šå°‘\n",
    "##### å¾ŒçºŒæµç¨‹\n",
    "- ä¸€ã€é€éåŸºç¤ç›¸ä¼¼åº¦è©•ä¼°å›ç­”ç‹€æ³ï¼šç”¨ä¸åŒå•é¡Œæ¸¬è©¦ï¼Œä¸¦ä¸”é–±è®€å›ç­”æƒ…æ³ï¼ˆé–±è®€ç†è§£ï¼‰â†’ å‰›å‰›å·²å®Œæˆ\n",
    "- äºŒã€é€éåŸºç¤ç›¸ä¼¼åº¦å»ºç«‹ baseline ï¼šé‹ç”¨é‡åŒ–æŒ‡æ¨™ï¼Œäº†è§£ç¾åœ¨ã€Œç›¸ä¼¼åº¦ã€çš„å›ç­”ç‹€æ³ â†’ç¨å¾ŒåŸ·è¡Œ\n",
    "- ä¸‰ã€baselineå»ºç«‹å®Œæˆå¾Œï¼Œæ¯”è¼ƒmmrå’ŒSelf-queryæª¢ç´¢ç‹€æ³ï¼Œæ±ºå®šè¦ç”¨å“ªä¸€ç¨®(æœ‰æ•¸æ“šçš„æ”¯æŒ)\n",
    "- å››ã€ç¢ºèªæª¢ç´¢æ–¹æ³•å¾Œï¼Œé€²ä¸€æ­¥å»ºç«‹llmå›ç­”æ©Ÿåˆ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ç¬¬ä¸€æ­¥é©Ÿçµæœè©•ä¼°ï¼š\n",
    "- Question 2 & 5ï¼ˆé—œæ–¼Transformer/Attention) -> è¡¨ç¾è‰¯å¥½\n",
    "- Question 3 (é‡å­è¨ˆç®—) & Question 4 (ç…®ç¾©å¤§åˆ©éºµ)ï¼šå› ç‚ºæ˜¯é€éã€Œç›¸ä¼¼æ€§æª¢ç´¢ã€è™•ç† -> è¡¨ç¾æ­£å¸¸\n",
    "- Question 1 (2023å¹´è«–æ–‡) æ²’æœ‰å¾ˆå¥½åœ°åˆ©ç”¨å¹´ä»½è³‡è¨Š(éœ€è¦æª¢ç´¢2023)\n",
    "#### åˆ¤æ–·æª¢ç´¢å“è³ªï¼š\n",
    "- ç›¸é—œåº¦åˆ†æ•¸åˆ†ä½ˆï¼ˆæŸäº›å•é¡Œæ‡‰è©²é«˜åˆ†ã€æŸäº›å•é¡Œæ‡‰è©²ä½åˆ†ï¼‰ï¼šé‚Šç•Œå•é¡Œæ‡‰è©²è¦ä½åˆ†ã€ä¸­ç­‰ç›¸é—œä¸­é–“ã€é«˜åº¦ç›¸é—œåˆ†æ•¸æ‡‰è©²è¦é«˜\n",
    "- çµæœå¤šæ¨£æ€§ï¼šå¯ä»¥æª¢é©—çš„å¤šå…ƒæ€§æœ‰ã€Œè«–æ–‡ä¾†æºçš„å¤šæ¨£æ€§ã€ã€ã€Œä¸»é¡Œå¤šæ¨£æ€§ã€ã€ã€Œæ™‚é–“å¤šæ¨£æ€§ã€ç­‰ï¼Œç¸½ä¹‹å°±æ˜¯ä¸å¸Œæœ›å›ç­”éƒ½æ˜¯ä¸€æ¨£çš„\n",
    "- metadataæº–ç¢ºç‡ï¼š:æª¢é©—å•ã€Œ2023ã€å¹´ã€æŸæŸä½œè€…ã€æŸä¸»é¡Œçš„è«–æ–‡ï¼Œå›å‚³çµæœä¸­ï¼ŒçœŸæ­£ç¬¦åˆå¹´ä»½çš„æ¯”ä¾‹ï¼Œä¾‹å¦‚k=3ï¼Œæœ‰2é¡Œå°å°±æ˜¯67%\n",
    "- æª¢ç´¢é€Ÿåº¦ï¼šå°±æ˜¯è¨ˆæ™‚\n",
    "- å»ºç«‹å•é¡Œé›†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬ä¸€æ­¥é©Ÿï¼šåŸºç¤ç†è§£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Level 1ï¼šåŸºç¤åŠŸèƒ½é©—è­‰\n",
    "\n",
    "# 1. MetadataæŸ¥è©¢æ¸¬è©¦\n",
    "question1 = \"What papers were published in 2023?\"\n",
    "ans_docs1 = vectordb.similarity_search(question1,k=2) # å•é¡Œåƒæ•¸è¨˜å¾—è¦æ”¹\n",
    "\n",
    "question2 = \"Which authors studied transformer architecture?\"\n",
    "ans_docs2 = vectordb.similarity_search(question2,k=2)\n",
    "\n",
    "# 2. é‚Šç•Œæƒ…æ³æ¸¬è©¦\n",
    "question3 = \"What is quantum computing?\"\n",
    "ans_docs3 = vectordb.similarity_search(question3,k=2)\n",
    "\n",
    "question4 = \"How to cook pasta?\"  \n",
    "ans_docs4 = vectordb.similarity_search(question4,k=2)\n",
    "\n",
    "question5 = \"What is the main contribution of Attention is All You Need?\"\n",
    "ans_docs5 = vectordb.similarity_search(question5,k=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æª¢é©—é‡åŒ–æŒ‡æ¨™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_time_score(vectordb, test_queries, k=3, iterations=5): # ï¼ˆå‘é‡è³‡æ–™åº«ã€æ¸¬è©¦æŸ¥è©¢åˆ—è¡¨ã€æª¢ç´¢æ•¸é‡ã€æ¯éš”æŸ¥è©¢é‡è¤‡æ¸¬è©¦æ¬¡æ•¸ï¼‰\n",
    "\n",
    "    import time \n",
    "\n",
    "    all_times = [] # ä¸‰å€‹å•é¡Œçš„ç¸½æ™‚é–“\n",
    "\n",
    "    for query in test_queries: # å…±æœ‰ä¸‰å€‹å•é¡Œã€è¿­ä»£äº”æ¬¡ã€æ¯æ¬¡ç”¢å‡º3å¡Š\n",
    "        query_times = [] # total times\n",
    "        for _ in range(iterations):  # iterations å‰›å‰›å·²ç¶“é è¨­ 5\n",
    "            start_time = time.time() # start timing\n",
    "            vectordb.similarity_search(query, k=k) # k é è¨­ç‚º3, query = each questions\n",
    "            end_time = time.time() # end timing\n",
    "            query_times.append(end_time - start_time) # ç´€éŒ„æŸå•é¡Œè¿­ä»£äº”æ¬¡çš„æ™‚é–“\n",
    "        avg_time = sum(query_times) / len(query_times)\n",
    "        all_times.append(avg_time)\n",
    "\n",
    "    return  {\n",
    "        'avg_response_time': sum(all_times) / len(all_times),\n",
    "        'min_response_time': min(all_times),\n",
    "        'max_response_time': max(all_times)\n",
    "    }\n",
    "\n",
    "# æ¸¬è©¦ç›¸é—œæ€§ï¼ˆä¸åŒå•é¡Œçš„ç›¸é—œæ€§æ‡‰è©²ä¸åŒï¼‰\n",
    "# å°ç­†è¨˜ï¼šå•é¡Œä¸€å®šè¦è·Ÿå¡Šæœ‰æ‰€äº’å‹•ï¼Œæ‰æœƒæœ‰ã€Œåˆ†æ•¸ã€ï¼ˆsimilarity_search_with_scoreï¼‰ï¼Œå› æ­¤é€™è£¡æ˜¯é€éå•é¡Œè¿”å›é»ç©ã€‚\n",
    "# å‘é‡æ¨¡å‹åªè¦åŒä¸€é–“å…¬å¸ï¼Œå‘é‡çš„æ–¹å¼éƒ½ä¸€æ¨£ï¼Œå’Œè³‡æ–™åº«æˆ–å…§å®¹æ²’æœ‰é—œä¿‚\n",
    "# åˆ†æ•¸å…¶å¯¦å°±æ˜¯ã€Œè·é›¢ï¼ˆcosine distanceï¼‰ã€ï¼Œæ‰€ä»¥æ•¸å­—è¶Šå°è¶Šç›¸é—œ\n",
    "def evaluate_relevance_score(vectordb, test_queries_dict):\n",
    "    # å·²ç¶“å»ºç«‹ä¸€å€‹æœ‰é«˜ç›¸é—œå’Œä½ç›¸é—œçš„å•é¡Œé›†\n",
    "    results = {}\n",
    "\n",
    "    for catagory, queries in test_queries_dict.items(): # loop for find out each key(high/low):value(question)\n",
    "        scores =[]\n",
    "        for q in queries:\n",
    "            docs_with_scores = vectordb.similarity_search_with_score(q, k=3) \n",
    "            avg_score = sum([score for doc, score in docs_with_scores])/len([docs_with_scores])\n",
    "            scores.append(avg_score)\n",
    "\n",
    "        results[catagory]={\n",
    "            'avg_score': sum(scores) / len(scores),\n",
    "            'min_score': min(scores),\n",
    "            'max_score': max(scores),\n",
    "            'all_scores': scores            \n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "def evaluate_accuracy_score(vectordb, metadata_queries, k=3):\n",
    "    \"\"\"\n",
    "    è©•ä¼°metadataæŸ¥è©¢çš„æº–ç¢ºç‡\n",
    "    \n",
    "    Args:\n",
    "        vectordb: å‘é‡è³‡æ–™åº«\n",
    "        metadata_queries: {\n",
    "            'year_2023': 'What papers were published in 2023?',\n",
    "            'transformer_topic': 'transformer architecture papers',\n",
    "            'author_vaswani': 'papers by Vaswani'\n",
    "        }\n",
    "        k: æª¢ç´¢æ•¸é‡\n",
    "    \n",
    "    Returns:\n",
    "        dict: å„ç¨®metadataæŸ¥è©¢çš„æº–ç¢ºç‡\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # å¹´ä»½æŸ¥è©¢æº–ç¢ºç‡\n",
    "    if 'year_2023' in metadata_queries:\n",
    "        query = metadata_queries['year_2023']\n",
    "        docs = vectordb.similarity_search(query, k=k)\n",
    "        correct_count = sum(1 for doc in docs if doc.metadata.get('year') == 2023)\n",
    "        results['year_accuracy'] = correct_count / len(docs) if docs else 0\n",
    "    \n",
    "    # ä¸»é¡ŒæŸ¥è©¢æº–ç¢ºç‡\n",
    "    if 'transformer_topic' in metadata_queries:\n",
    "        query = metadata_queries['transformer_topic']\n",
    "        docs = vectordb.similarity_search(query, k=k)\n",
    "        correct_count = sum(1 for doc in docs if 'transformer' in doc.metadata.get('topic', '').lower())\n",
    "        results['topic_accuracy'] = correct_count / len(docs) if docs else 0\n",
    "    \n",
    "    # ä½œè€…æŸ¥è©¢æº–ç¢ºç‡\n",
    "    if 'author_vaswani' in metadata_queries:\n",
    "        query = metadata_queries['author_vaswani']\n",
    "        docs = vectordb.similarity_search(query, k=k)\n",
    "        correct_count = sum(1 for doc in docs if 'vaswani' in doc.metadata.get('authors', '').lower())\n",
    "        results['author_accuracy'] = correct_count / len(docs) if docs else 0\n",
    "    \n",
    "    return results\n",
    "\n",
    "def evaluate_diversity_score(vectordb, query, k=3):\n",
    "    \"\"\"\n",
    "    è©•ä¼°æª¢ç´¢çµæœçš„å¤šæ¨£æ€§\n",
    "    \n",
    "    Args:\n",
    "        vectordb: å‘é‡è³‡æ–™åº«\n",
    "        query: æŸ¥è©¢å­—ä¸²\n",
    "        k: æª¢ç´¢æ•¸é‡\n",
    "    \n",
    "    Returns:\n",
    "        dict: å„ç¨®å¤šæ¨£æ€§æŒ‡æ¨™\n",
    "    \"\"\"\n",
    "    docs = vectordb.similarity_search(query, k=k)\n",
    "    \n",
    "    # 1. è«–æ–‡ä¾†æºå¤šæ¨£æ€§\n",
    "    titles = [doc.metadata.get('title', 'Unknown') for doc in docs]\n",
    "    unique_titles = len(set(titles))\n",
    "    title_diversity = unique_titles / len(titles)\n",
    "    \n",
    "    # 2. å¹´ä»½å¤šæ¨£æ€§\n",
    "    years = [doc.metadata.get('year', 'Unknown') for doc in docs]\n",
    "    unique_years = len(set(years))\n",
    "    year_diversity = unique_years / len(years)\n",
    "    \n",
    "    # 3. ä¸»é¡Œå¤šæ¨£æ€§\n",
    "    topics = [doc.metadata.get('topic', 'Unknown') for doc in docs]\n",
    "    unique_topics = len(set(topics))\n",
    "    topic_diversity = unique_topics / len(topics)\n",
    "    \n",
    "    return {\n",
    "        'title_diversity': title_diversity,\n",
    "        'year_diversity': year_diversity,\n",
    "        'topic_diversity': topic_diversity,\n",
    "        'overall_diversity': (title_diversity + year_diversity + topic_diversity) / 3\n",
    "    }\n",
    "\n",
    "\n",
    "def running_baseline(vectordb):\n",
    "    print(\"starting baseline evalution\")\n",
    "\n",
    "    # 1. preparing testing data\n",
    "    relevance_queries = {\n",
    "        'high_relevance': [\n",
    "            'transformer architecture',\n",
    "            'attention mechanism',\n",
    "            'what is BERT'\n",
    "        ],\n",
    "        'low_relevence': [\n",
    "            'how to cook a pasta',\n",
    "            'quantum computing',\n",
    "            'weather forecast'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    metadata_queries = { # structure - key:actual question\n",
    "        'year_2023': 'what papers were published in 2023?',\n",
    "        'llm_limitation_topic': 'llm limitation papers',\n",
    "        'author_vaswani': 'papers by vaswani'\n",
    "    }\n",
    "\n",
    "\n",
    "    speed_queries = ['transformer', 'attention', 'LLM limitations']\n",
    "\n",
    "    # 2. running defs\n",
    "    relevance_results =  evaluate_relevance_score(vectordb, relevance_queries) # input: different questions\n",
    "    diversity_results =  evaluate_diversity_score(vectordb, 'LLM research') # input: target topic\n",
    "    accuracy_results = evaluate_accuracy_score(vectordb, metadata_queries) # input: question of metadata\n",
    "    time_results = evaluate_time_score(vectordb, speed_queries) # input:any questions\n",
    "\n",
    "    # 3. print results\n",
    "        \n",
    "    # about relavance \n",
    "\n",
    "    print(\"\\nğŸ“Š ç›¸é—œåº¦åˆ†æ•¸åˆ†ä½ˆ:\")\n",
    "    for category, stats in relevance_results.items():\n",
    "        print(f\"  {category}: å¹³å‡={stats['avg_score']:.3f}, ç¯„åœ=[{stats['min_score']:.3f}, {stats['max_score']:.3f}]\")    \n",
    "\n",
    "    # about diversity\n",
    "    print(f\"\\nğŸ¯ å¤šæ¨£æ€§åˆ†æ•¸: {diversity_results['overall_diversity']:.3f}\")\n",
    "    \n",
    "    # about accuracy\n",
    "    print(\"\\nğŸ“‹ Metadataæº–ç¢ºç‡:\")\n",
    "    for metric, accuracy in accuracy_results.items():\n",
    "        print(f\"  {metric}: {accuracy:.1%}\")    \n",
    "\n",
    "\n",
    "    # about response times\n",
    "    print(f\"\\n average response time: {time_results['avg_response_time']:.3f}seconds\"\n",
    "    f\"\\n min response time: {time_results['min_response_time']:.3f}seconds\"\n",
    "    f\"\\n max response time: {time_results['max_response_time']:.3f}seconds\")\n",
    "    \n",
    "\n",
    "\n",
    "    return { \n",
    "        'relevance': relevance_results,\n",
    "        'diversity': diversity_results,\n",
    "        'metadata': accuracy_results,\n",
    "        'speed': time_results\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting baseline evalution\n",
      "\n",
      "ğŸ“Š ç›¸é—œåº¦åˆ†æ•¸åˆ†ä½ˆ:\n",
      "  high_relevance: å¹³å‡=1.308, ç¯„åœ=[1.183, 1.433]\n",
      "  low_relevence: å¹³å‡=1.529, ç¯„åœ=[1.284, 1.671]\n",
      "\n",
      "ğŸ¯ å¤šæ¨£æ€§åˆ†æ•¸: 1.000\n",
      "\n",
      "ğŸ“‹ Metadataæº–ç¢ºç‡:\n",
      "  year_accuracy: 33.3%\n",
      "  author_accuracy: 0.0%\n",
      "\n",
      " average response time: 0.455seconds\n",
      " min response time: 0.420seconds\n",
      " max response time: 0.480seconds\n"
     ]
    }
   ],
   "source": [
    "results = running_baseline(vectordb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### baseline results\n",
    "- å»ºç«‹ç³»çµ±æ€§èƒ½åŸºæº–:å›æ‡‰æ™‚é–“ï¼š0.45ç§’ï¼Œé©—è­‰æª¢ç´¢åŠŸèƒ½æ­£å¸¸\n",
    "\n",
    "- é«˜ç›¸é—œ < ä½ç›¸é—œçš„è·é›¢è­‰æ˜ç³»çµ±work\n",
    "\n",
    "- å•é¡Œé»ï¼š\n",
    "    - å¤šæ¨£æ€§ 1.0 = å®Œå…¨æ²’å¤šæ¨£æ€§\n",
    "    - Metadata æº–ç¢ºç‡æ…˜çƒˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " average response time: 0.504seconds\n",
      " min response time: 0.468seconds\n",
      " max response time: 0.573seconds\n"
     ]
    }
   ],
   "source": [
    "def evaluate_time_score_mmr(vectordb, test_queries, k=3, iterations=5): # ï¼ˆå‘é‡è³‡æ–™åº«ã€æ¸¬è©¦æŸ¥è©¢åˆ—è¡¨ã€æª¢ç´¢æ•¸é‡ã€æ¯éš”æŸ¥è©¢é‡è¤‡æ¸¬è©¦æ¬¡æ•¸ï¼‰\n",
    "\n",
    "    import time \n",
    "\n",
    "    all_times = [] # ä¸‰å€‹å•é¡Œçš„ç¸½æ™‚é–“\n",
    "\n",
    "    for query in test_queries: # å…±æœ‰ä¸‰å€‹å•é¡Œã€è¿­ä»£äº”æ¬¡ã€æ¯æ¬¡ç”¢å‡º3å¡Š\n",
    "        query_times = [] # total times\n",
    "        for _ in range(iterations):  # iterations å‰›å‰›å·²ç¶“é è¨­ 5\n",
    "            start_time = time.time() # start timing\n",
    "            vectordb.max_marginal_relevance_search(query, k=k) # k é è¨­ç‚º3, query = each questions\n",
    "            end_time = time.time() # end timing\n",
    "            query_times.append(end_time - start_time) # ç´€éŒ„æŸå•é¡Œè¿­ä»£äº”æ¬¡çš„æ™‚é–“\n",
    "        avg_time = sum(query_times) / len(query_times)\n",
    "        all_times.append(avg_time)\n",
    "\n",
    "    return  {\n",
    "        'avg_response_time': sum(all_times) / len(all_times),\n",
    "        'min_response_time': min(all_times),\n",
    "        'max_response_time': max(all_times)\n",
    "    }\n",
    "# about response times\n",
    "\n",
    "\n",
    "speed_queries = ['transformer', 'attention', 'LLM limitations']\n",
    "\n",
    "time_results = evaluate_time_score_mmr(vectordb, speed_queries)\n",
    "\n",
    "print(f\"\\n average response time: {time_results['avg_response_time']:.3f}seconds\"\n",
    "    f\"\\n min response time: {time_results['min_response_time']:.3f}seconds\"\n",
    "    f\"\\n max response time: {time_results['max_response_time']:.3f}seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity çµæœè«–æ–‡:\n",
      "- Attention Is All You Need\n",
      "- Lost in the Middle- How Language Models Use Long Contexts\n",
      "- Attention Is All You Need\n",
      "- Scaling Laws for Neural Language Models\n",
      "- Attention Is All You Need\n",
      "\n",
      "MMR çµæœè«–æ–‡:\n",
      "- Attention Is All You Need\n",
      "- Lost in the Middle- How Language Models Use Long Contexts\n",
      "- Modular RAG- Transforming RAG Systems into LEGO-like Reconfigurable Frameworks\n",
      "- Large Language Model Agent- A Survey on Methodology, Applications and Challenges\n",
      "- Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or- How I learned to start worrying about prompt formatting\n"
     ]
    }
   ],
   "source": [
    "def diversity_showdown():\n",
    "    query = \"transformer architecture\"\n",
    "    \n",
    "    sim_docs = vectordb.similarity_search(query, k=5)\n",
    "    mmr_docs = vectordb.max_marginal_relevance_search(query, k=5)\n",
    "    \n",
    "    print(\"Similarity çµæœè«–æ–‡:\")\n",
    "    for doc in sim_docs:\n",
    "        print(f\"- {doc.metadata['title']}\")\n",
    "    \n",
    "    print(\"\\nMMR çµæœè«–æ–‡:\")  \n",
    "    for doc in mmr_docs:\n",
    "        print(f\"- {doc.metadata['title']}\")\n",
    "    \n",
    "diversity_result = diversity_showdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¯”è¼ƒç¸½è¡¨ï¼š\n",
    "    æ™‚é–“æ•ˆç‡ï¼š\n",
    "        mmr:\n",
    "            average response time: 0.504seconds\n",
    "            min response time: 0.468seconds\n",
    "            max response time: 0.573seconds\n",
    "        Similarityï¼š\n",
    "            average response time: 0.455seconds\n",
    "            min response time: 0.420seconds\n",
    "            max response time: 0.480seconds\n",
    "    \n",
    "    å¤šæ¨£æ€§ï¼š\n",
    "        Similarity :\n",
    "            - Attention Is All You Need\n",
    "            - Lost in the Middle- How Language Models Use Long Contexts\n",
    "            - Attention Is All You Need\n",
    "            - Scaling Laws for Neural Language Models\n",
    "            - Attention Is All You Need\n",
    "        MMR çµæœè«–æ–‡:\n",
    "            - Attention Is All You Need\n",
    "            - Lost in the Middle- How Language Models Use Long Contexts\n",
    "            - Modular RAG- Transforming RAG Systems into LEGO-like Reconfigurable Frameworks\n",
    "            - Large Language Model Agent- A Survey on Methodology, Applications and Challenges\n",
    "            - Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or- How I learned to start worrying about prompt formatting    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mmr_metadata_precision():\n",
    "    \"\"\"\n",
    "    æ¸¬è©¦MMRåœ¨ç‰¹å®šmetadataæŸ¥è©¢ä¸Šæ˜¯å¦æ¯”similarityæ›´ç²¾æº–\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. å¹´ä»½æŸ¥è©¢\n",
    "    year_query = \"What papers were published in 2023?\"\n",
    "    sim_2023 = vectordb.similarity_search(year_query, k=5)\n",
    "    mmr_2023 = vectordb.max_marginal_relevance_search(year_query, k=5)\n",
    "    \n",
    "    print(\"2023å¹´è«–æ–‡æŸ¥è©¢:\")\n",
    "    print(\"Similarityçµæœ:\")\n",
    "    for doc in sim_2023:\n",
    "        year = doc.metadata.get('year', 'Unknown')\n",
    "        print(f\"  {year}: {doc.metadata.get('title', 'Unknown')}\")\n",
    "    \n",
    "    print(\"MMRçµæœ:\")  \n",
    "    for doc in mmr_2023:\n",
    "        year = doc.metadata.get('year', 'Unknown')\n",
    "        print(f\"  {year}: {doc.metadata.get('title', 'Unknown')}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023å¹´è«–æ–‡æŸ¥è©¢:\n",
      "Similarityçµæœ:\n",
      "  2023: BloombergGPT- A Large Language Model for Finance\n",
      "  2024: Modular RAG- Transforming RAG Systems into LEGO-like Reconfigurable Frameworks\n",
      "  2024: The Power of Noise- Redefining Retrieval for RAG Systems\n",
      "  2023: Challenges and Applications of Large Language Models\n",
      "  2025: The Order Effect- Investigating Prompt Sensitivity to Input Order in LLMs\n",
      "MMRçµæœ:\n",
      "  2023: BloombergGPT- A Large Language Model for Finance\n",
      "  2023: Challenges and Applications of Large Language Models\n",
      "  2024: Language Ranker- A Metric for Quantifying LLM Performance Across High and Low-Resource Languages\n",
      "  2025: Large Language Model Agent- A Survey on Methodology, Applications and Challenges\n",
      "  2024: Retrieval-Augmented Generation for Large Language Models- A Survey\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = test_mmr_metadata_precision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search_with_year(query, target_year, k=5):\n",
    "    # å…ˆç”¨å‘é‡æœå°‹æ‰¾åˆ°å€™é¸çµæœï¼ˆæ›´å¤§çš„kï¼‰\n",
    "\n",
    "    candidates = vectordb.max_marginal_relevance_search(query, k=k*3)  # æ‰¾15å€‹å€™é¸\n",
    "    \n",
    "    # å†ç”¨metadataç¯©é¸å¹´ä»½\n",
    "    year_filtered = [doc for doc in candidates \n",
    "                    if doc.metadata.get('year') == target_year]\n",
    "    \n",
    "    # å›å‚³å‰kå€‹\n",
    "    return year_filtered[:k]\n",
    "\n",
    "query = \"What papers were published in 2023?\" \n",
    "year = 2023\n",
    "test_accurac_hybird = hybrid_search_with_year(query, year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2023: BloombergGPT- A Large Language Model for Finance\n",
      "  2023: Challenges and Applications of Large Language Models\n",
      "  2023: Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions\n",
      "  2023: Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or- How I learned to start worrying about prompt formatting\n",
      "  2023: BloombergGPT- A Large Language Model for Finance\n"
     ]
    }
   ],
   "source": [
    "for doc in test_accurac_hybird:\n",
    "    year = doc.metadata.get('year', 'Unknown')\n",
    "    print(f\"  {year}: {doc.metadata.get('title', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "çœ‹èµ·ä¾†ç”¨æ··åˆæª¢ç´¢çš„æ–¹å¼ï¼Œå¯ä»¥çœŸçš„å¥½å¥½æŒ‡å®šæˆ‘è¦çš„å¹´ä»½ã€‚\n",
    "ä½†æˆ‘æœ‰ä»¥ä¸‹å•é¡Œæƒ³å’Œä½ è¨è«–ï¼Œä½ å…ˆä¸è¦çµ¦æˆ‘ç¨‹å¼ç¢¼ï¼Œè·Ÿæˆ‘ç”¨æ–‡å­—è¨è«–ï¼š\n",
    "ä¸€ã€æˆ‘çš„metadataä¸­ï¼Œå­˜åœ¨ä»¥ä¸‹æ¬„ä½ï¼Œé€™äº›éƒ½å¯ä»¥è®Šæˆmetadataç¯©é¸çš„æ¢ä»¶å—ï¼Ÿä¾‹å¦‚title, year, author, topicç­‰ï¼Ÿ\n",
    "äºŒã€å¦‚æœå¯ä»¥éƒ½è®Šæˆç¯©é¸æ¢ä»¶ï¼Œå¦‚ä½•å°‡é€™äº›æª¢ç´¢æ¢ä»¶åŠ åˆ°ã€Œæª¢ç´¢ã€çš„ç¨‹å¼ä¸­ï¼Ÿå› ç‚ºæˆ‘å€‘å†æª¢ç´¢éšæ®µå¯ä»¥è‡ªå·±æ‰“year=2023ä¹‹é¡çš„åƒæ•¸ï¼Œå¯æ˜¯æ•´é«”çš„ragæˆ‘å€‘æ‡‰è©²åªæœƒæ‰“ä¸€å€‹å•é¡Œï¼Œé€™æ¨£æœ‰è¾¦æ³•å¹«æˆ‘å€‘åˆ†è¾¨å—ï¼Ÿ\n",
    "ä¸‰ã€æˆ‘é‚„æ˜¯å¾ˆæƒ³è§£æ±ºä¸èƒ½ç²¾æº–æª¢ç´¢çš„å•é¡Œï¼Œæ‰€ä»¥æ­£åœ¨è©¦hybridçš„æ–¹å¼ï¼Œä½†æˆ‘ä»”ç´°ä¸€æƒ³ä½ å‰›å‰›è·Ÿæˆ‘èªªé€™å€‹ragçš„å®šä½æ˜¯ä»€éº¼ï¼Œæˆ‘å°±åœ¨æƒ³æˆ‘ç”¨äº†mmræå‡äº†ã€Œå¤šæ¨£æ€§ã€ï¼Œç”¨hybirdå¯ä»¥æå‡ã€Œæº–ç¢ºç‡ã€ï¼Œå“ªä¸€å€‹å°æˆ‘ä¾†èªªå¯ä»¥æª¢ç´¢è«–æ–‡æ¯”è¼ƒé‡è¦ï¼Ÿæˆ‘æ˜¯ç«™åœ¨ã€Œå­¸ç¿’llmçŸ¥è­˜ã€çš„è§’åº¦å»é–‹ç™¼é€™å€‹ragï¼Œæ‰€ä»¥æ‰æœƒæ¸…ç†ä¹¾æ·¨metadataå‰é¢æåˆ°æœ€é‡è¦çš„å››å¤§åƒæ•¸ï¼Œæ‰€ä»¥éƒ½å¾ˆé‡è¦ï¼Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å»ºç«‹hybrid rag(æŸ¥è©¢è§£æå™¨) ç›®æ¨™ï¼š æŠŠè‡ªç„¶èªè¨€æŸ¥è©¢ â†’ è½‰æ›ç‚º structured parameters\n",
    "- å»ºç«‹llmè§£æå™¨\n",
    "- æŸ¥è©¢ä½¿ç”¨mmr+è§£æå™¨ï¼šè¦å‰‡åˆ¤æ–·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'year': 2024, 'topic': 'RAG', 'source': '/Users/mangtinglee/Desktop/2025_gap_careerpath/RAG_LLM/pdfs/2024_RAG_Evaluation of Retrieval-Augmented Generation- A Survey.pdf', 'total_pages': 21, 'title': 'Evaluation of Retrieval-Augmented Generation- A Survey', 'page': 0, 'authors': 'Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, Zhaofeng Liu'}\n",
      "{'authors': 'Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang', 'title': 'Retrieval-Augmented Generation for Large Language Models- A Survey', 'total_pages': 21, 'page': 0, 'year': 2024, 'topic': 'RAG', 'source': '/Users/mangtinglee/Desktop/2025_gap_careerpath/RAG_LLM/pdfs/2024_RAG_Retrieval-Augmented Generation for Large Language Models- A Survey.pdf'}\n",
      "{'title': 'Evaluation of Retrieval-Augmented Generation- A Survey', 'total_pages': 21, 'source': '/Users/mangtinglee/Desktop/2025_gap_careerpath/RAG_LLM/pdfs/2024_RAG_Evaluation of Retrieval-Augmented Generation- A Survey.pdf', 'page': 0, 'authors': 'Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, Zhaofeng Liu', 'topic': 'RAG', 'year': 2024}\n",
      "{'authors': 'Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang', 'year': 2024, 'source': '/Users/mangtinglee/Desktop/2025_gap_careerpath/RAG_LLM/pdfs/2024_RAG_Retrieval-Augmented Generation for Large Language Models- A Survey.pdf', 'page': 0, 'topic': 'RAG', 'title': 'Retrieval-Augmented Generation for Large Language Models- A Survey', 'total_pages': 21}\n",
      "{'total_pages': 21, 'page': 0, 'source': '/Users/mangtinglee/Desktop/2025_gap_careerpath/RAG_LLM/pdfs/2024_RAG_Retrieval-Augmented Generation for Large Language Models- A Survey.pdf', 'title': 'Retrieval-Augmented Generation for Large Language Models- A Survey', 'year': 2024, 'topic': 'RAG', 'authors': 'Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang'}\n"
     ]
    }
   ],
   "source": [
    "# llmå¥—ä»¶èˆ‡å•Ÿå‹•\n",
    "\n",
    "from langchain_openai import ChatOpenAI   \n",
    "\n",
    "# å»ºç«‹ LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")   # æŒ‡å®šæ¨¡å‹\n",
    "\n",
    "\n",
    "# llmè§£æå™¨ï¼šè§£æå•é¡Œå…§éƒ¨çš„åƒæ•¸\n",
    "def llm_extract_metadata(query):\n",
    "    prompt = f\"\"\"\n",
    "you are a metedata extraction export, extract year and topic information from user queries.\n",
    "\n",
    "Available topics:\n",
    "    - \"transformer\"\n",
    "    - \"scaling laws\"\n",
    "    - \"fine tuning\"\n",
    "    - \"LLM limitation\"\n",
    "    - \"LLM application\"\n",
    "    - \"prompt engineering\"\n",
    "    - \"RAG\"\n",
    "    - \"ai-agent\"\n",
    "    - null (if no specific topic)\n",
    "\n",
    "    Available years: 2017-2025 or null\n",
    "\n",
    "    Query = \"{query}\"\n",
    "\n",
    "    Extract and return ONLY a JSON object:\n",
    "    {{\"year\":<number or null>, \"topic\":\"<topic or null>\"}}\n",
    "\n",
    "    Examples:\n",
    "    Query:\"2023å¹´é—œæ–¼transformerçš„ç ”ç©¶\"\n",
    "    {{\"year\":2023, \"topic\":\"transformer\"}}\n",
    "\n",
    "    Query:\"LLMçš„é™åˆ¶æœ‰å“ªäº›\"\n",
    "    {{\"year\":null, \"topic\":\"llm limitation\"}}\n",
    "\n",
    "    Query: \"æœ€æ–°çš„RAGè«–æ–‡\"\n",
    "    {{\"year\": 2025, \"topic\": \"RAG\"}}\n",
    "\n",
    "    Now extract from the query above:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = llm.invoke(prompt) # response çš„è³‡æ–™å‹æ…‹æ˜¯AIMessageï¼Œä¸æ˜¯å­—ä¸²\n",
    "        response_text = response.content  # response.content æ‰èƒ½å–å¾—æ–‡å­—å…§å®¹\n",
    "        # è§£æjsonæ ¼å¼\n",
    "        import json\n",
    "        metadata = json.loads(response_text.strip())\n",
    "        return metadata\n",
    "    except:\n",
    "        return {\"year\":None, \"topic\":None }\n",
    "\n",
    "def smart_hybrid_search(query, k=5):\n",
    "    # llmè§£æè¿”å›çš„æŸ¥è©¢\n",
    "    metadata = llm_extract_metadata(query)\n",
    "    year = metadata.get('year')\n",
    "    topic = metadata.get('topic')\n",
    "\n",
    "    # æ ¹æ“šè§£æç­–ç•¥é¸æ“‡æª¢ç´¢æ–¹å¼\n",
    "    if year and topic:\n",
    "        candidates = vectordb.max_marginal_relevance_search(query, k=k*4)\n",
    "        filtered = [doc for doc in candidates\n",
    "                    if doc.metadata.get('year') == year\n",
    "                    and doc.metadata.get('topic') == topic]\n",
    "    elif year:\n",
    "        candidates = vectordb.max_marginal_relevance_search(query, k=k*3)\n",
    "        filtered = [doc for doc in candidates\n",
    "                    if doc.metadata.get('year') == year]\n",
    "\n",
    "    elif topic:\n",
    "        candidates = vectordb.max_marginal_relevance_search(query, k=k*3)\n",
    "        filtered = [doc for doc in candidates\n",
    "                    if doc.metadata.get('topic') == topic]\n",
    "\n",
    "    else:\n",
    "        filtered = vectordb.max_marginal_relevance_search(query, k=k)\n",
    "\n",
    "    # ç¢ºä¿æœ‰è¶³å¤ çµæœ\n",
    "    if len(filtered) < k:\n",
    "        additional = vectordb.max_marginal_relevance_search(query, k=k)\n",
    "        filtered.extend = ([doc for doc in additional if doc not in filtered])\n",
    "    \n",
    "    return filtered[:k]\n",
    "\n",
    "\n",
    "test_queries =\"2024å¹´é—œæ–¼RAGçš„ç ”ç©¶\"\n",
    "test_hybid = smart_hybrid_search(test_queries)\n",
    "for doc in test_hybid:\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# éœ€è¦å°‡å‰›å‰›çš„hybrid retrivalåŒ…è£æˆè‡ªå®šç¾©retriverç‰©ä»¶ï¼Œæ‰å¯ä»¥æ¥è»Œ\n",
    "from langchain.schema import BaseRetriever, Document\n",
    "from typing import List, Any\n",
    "from langchain_openai import ChatOpenAI\n",
    "import json\n",
    "from pydantic import Field\n",
    "\n",
    "class SmartHybridRetriever(BaseRetriever):  # ä¿®æ­£æ‹¼å­—éŒ¯èª¤\n",
    "\n",
    "    # æ˜ç¢ºå®šç¾©fields\n",
    "    vectordb: Any = Field(default=None)\n",
    "    llm: Any = Field(default=None)\n",
    "    \n",
    "    # def __init__(self, vectordb, llm, **kwargs):\n",
    "    #     # å…ˆå‚³çµ¦çˆ¶é¡\n",
    "    #     super().__init__(**kwargs)\n",
    "    #     # ç„¶å¾Œè¨­å®šå±¬æ€§\n",
    "    #     object.__setattr__(self, 'vectordb', vectordb)\n",
    "    #     object.__setattr__(self, 'llm', llm)\n",
    "    \n",
    "    \n",
    "    def __init__(self, vectordb, llm):\n",
    "        super().__init__()  # é‡è¦ï¼šèª¿ç”¨çˆ¶é¡åˆå§‹åŒ–\n",
    "        self.vectordb = vectordb\n",
    "        self.llm = llm\n",
    "    \n",
    "    def llm_extract_metadata(self, query):  # åŠ å…¥selfåƒæ•¸\n",
    "        prompt = f\"\"\"\n",
    "you are a metadata extraction expert, extract year and topic information from user queries.\n",
    "\n",
    "Available topics:\n",
    "    - \"transformer\"\n",
    "    - \"scaling laws\"\n",
    "    - \"fine tuning\"\n",
    "    - \"LLM limitation\"\n",
    "    - \"LLM application\"\n",
    "    - \"prompt engineering\"\n",
    "    - \"RAG\"\n",
    "    - \"LLM agent\"  # ä¿®æ­£ç‚º\"LLM agent\"\n",
    "    - null (if no specific topic)\n",
    "\n",
    "Available years: 2017-2025 or null\n",
    "\n",
    "Query = \"{query}\"\n",
    "\n",
    "Extract and return ONLY a JSON object:\n",
    "{{\"year\":<number or null>, \"topic\":\"<topic or null>\"}}\n",
    "\n",
    "Examples:\n",
    "Query:\"2023å¹´é—œæ–¼transformerçš„ç ”ç©¶\"\n",
    "{{\"year\":2023, \"topic\":\"transformer\"}}\n",
    "\n",
    "Query:\"LLMçš„é™åˆ¶æœ‰å“ªäº›\"\n",
    "{{\"year\":null, \"topic\":\"LLM limitation\"}}\n",
    "\n",
    "Query: \"æœ€æ–°çš„RAGè«–æ–‡\"\n",
    "{{\"year\": 2025, \"topic\": \"RAG\"}}\n",
    "\n",
    "Now extract from the query above:\n",
    "\"\"\"\n",
    "        try:\n",
    "            response = self.llm.invoke(prompt)  # ä½¿ç”¨self.llm\n",
    "            response_text = response.content\n",
    "            metadata = json.loads(response_text.strip())\n",
    "            return metadata\n",
    "        except:\n",
    "            return {\"year\": None, \"topic\": None}\n",
    "\n",
    "    def smart_hybrid_search(self, query, k=5):  # åŠ å…¥selfåƒæ•¸\n",
    "        metadata = self.llm_extract_metadata(query)  # ä½¿ç”¨self\n",
    "        year = metadata.get('year')\n",
    "        topic = metadata.get('topic')\n",
    "\n",
    "        if year and topic:\n",
    "            candidates = self.vectordb.max_marginal_relevance_search(query, k=k*4)\n",
    "            filtered = [doc for doc in candidates\n",
    "                       if doc.metadata.get('year') == year\n",
    "                       and doc.metadata.get('topic') == topic]\n",
    "        elif year:\n",
    "            candidates = self.vectordb.max_marginal_relevance_search(query, k=k*3)\n",
    "            filtered = [doc for doc in candidates\n",
    "                       if doc.metadata.get('year') == year]\n",
    "        elif topic:\n",
    "            candidates = self.vectordb.max_marginal_relevance_search(query, k=k*3)\n",
    "            filtered = [doc for doc in candidates\n",
    "                       if doc.metadata.get('topic') == topic]\n",
    "        else:\n",
    "            filtered = self.vectordb.max_marginal_relevance_search(query, k=k)\n",
    "\n",
    "        # ä¿®æ­£extendçš„ä½¿ç”¨\n",
    "        if len(filtered) < k:\n",
    "            additional = self.vectordb.max_marginal_relevance_search(query, k=k)\n",
    "            filtered.extend([doc for doc in additional if doc not in filtered])  # ä¿®æ­£\n",
    "        \n",
    "        return filtered[:k]\n",
    "    \n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        \"\"\"BaseRetrieverè¦æ±‚çš„æŠ½è±¡æ–¹æ³•\"\"\"\n",
    "        return self.smart_hybrid_search(query, k=5)\n",
    "    \n",
    "    async def _aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        \"\"\"ç•°æ­¥ç‰ˆæœ¬\"\"\"\n",
    "        return self._get_relevant_documents(query)\n",
    "\n",
    "# ä½¿ç”¨æ–¹å¼\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")  # åœ¨å¤–éƒ¨å®šç¾©\n",
    "custom_retriever = SmartHybridRetriever(vectordb=vectordb, llm=llm)  # ä¿®æ­£è®Šæ•¸å\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA,  ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "#åŠ å…¥llmå›ç­”\n",
    "# æ–°é …ç›®ï¼šè¼‰å…¥ConversationBufferMemory\n",
    "# æ­¤å¥—ä»¶èƒ½è®“å•ç­”æ©Ÿå™¨äººè¨˜ä½éå¾€æ­·å²å•ç­”\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\", # å‘Šè¨´ chain å»å“ªè£¡æ‰¾æ­·å²å°è©±\n",
    "    return_messages=True, # è¿”å›çš„æ˜¯ç‰©ä»¶ï¼ˆé•·å¾—åƒjson or meta dataï¼‰\n",
    "    output_key=\"answer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹è‡ªè¨‚ç¾©çš„prompt(ç‚ºäº†å¥½å¥½åˆ©ç”¨æª¢ç´¢)\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# å»ºç«‹è‡ªå®šç¾©prompt\n",
    "qa_prompt = PromptTemplate(\n",
    "    template=\"\"\"è«‹åŸºæ–¼ä»¥ä¸‹æª¢ç´¢åˆ°çš„æ–‡ç»å…§å®¹å›ç­”å•é¡Œã€‚\n",
    "\n",
    "æª¢ç´¢åˆ°çš„æ–‡ç»ï¼š\n",
    "{context}\n",
    "\n",
    "å•é¡Œï¼š{question}\n",
    "\n",
    "è«‹ç›´æ¥åŸºæ–¼ä¸Šè¿°æ–‡ç»å…§å®¹å›ç­”å•é¡Œï¼Œåˆ—å‡ºç›¸é—œçš„è«–æ–‡æ¨™é¡Œå’Œä¸»è¦å…§å®¹ã€‚å¦‚æœæ–‡ç»ä¸­æ²’æœ‰ç›¸é—œè³‡è¨Šï¼Œè«‹æ˜ç¢ºèªªæ˜ã€‚\n",
    "\n",
    "ç­”æ¡ˆï¼š\"\"\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# é‡æ–°å»ºç«‹qa chainï¼Œä½¿ç”¨è‡ªå®šç¾©prompt\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0),\n",
    "    chain_type=\"stuff\",  # æ”¹ç‚ºstuffæ›´ç›´æ¥\n",
    "    retriever=custom_retriever,\n",
    "    return_source_documents=True,\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={\"prompt\": qa_prompt}  # åŠ å…¥è‡ªå®šç¾©prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformeræ¶æ§‹çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯åŸºæ–¼æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼Œä¸¦ä¸”å®Œå…¨ä¸ä½¿ç”¨å¾ªç’°ç¥ç¶“ç¶²çµ¡æˆ–å·ç©ç¥ç¶“ç¶²çµ¡ã€‚é€™ç¨®ç°¡å–®çš„ç¶²çµ¡æ¶æ§‹é€šéæ³¨æ„åŠ›æ©Ÿåˆ¶å°‡ç·¨ç¢¼å™¨å’Œè§£ç¢¼å™¨ç›¸é€£æ¥ï¼Œä¸¦ä¸”åœ¨æ©Ÿå™¨ç¿»è­¯ä»»å‹™ä¸­è¡¨ç¾å„ªç•°ï¼ŒåŒæ™‚å…·æœ‰æ›´å¥½çš„ä¸¦è¡Œæ€§å’Œæ›´çŸ­çš„è¨“ç·´æ™‚é–“ã€‚Transformeræ¨¡å‹åœ¨WMT 2014è‹±å¾·ç¿»è­¯ä»»å‹™ä¸Šå–å¾—äº†28.4 BLEUçš„åˆ†æ•¸ï¼Œåœ¨è‹±æ³•ç¿»è­¯ä»»å‹™ä¸Šå–å¾—äº†41.8 BLEUçš„åˆ†æ•¸ï¼Œä¸¦ä¸”åœ¨å…¶ä»–ä»»å‹™ä¸Šä¹Ÿè¡¨ç¾è‰¯å¥½ã€‚Transformerçš„æå‡ºæ˜¯åŸºæ–¼å°‡å¾ªç’°ç¥ç¶“ç¶²çµ¡æ›¿æ›ç‚ºè‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶ï¼Œä¸¦ä¸”æ¶ˆé™¤äº†å¾ªç’°å’Œå·ç©çš„ä½¿ç”¨ã€‚\n"
     ]
    }
   ],
   "source": [
    "question = \"transformeræ¶æ§‹çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä»€éº¼ï¼Ÿ\"\n",
    "result = qa({\"question\": question})\n",
    "print(result['answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ ¹æ“šæª¢ç´¢åˆ°çš„æ–‡ç»å…§å®¹ï¼Œé€™å€‹æ¶æ§‹çš„é™åˆ¶åŒ…æ‹¬ï¼š\n",
      "1. LLMså°æ–¼æç¤ºä¸­å…ƒç´ çš„é †åºæ•æ„Ÿï¼ŒåŒ…æ‹¬few-shot demonstrationsçš„æ’åˆ—å’Œå€™é¸æ¨¡å‹ç”Ÿæˆçš„å›æ‡‰é †åºï¼Œå¯èƒ½å½±éŸ¿å…¶ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚\n",
      "   - ç›¸é—œè«–æ–‡ï¼šZhao et al., 2021; Wang et al., 2023b\n",
      "2. LLMså°æ–¼æç¤ºä¸­çš„å¾®å°è®ŠåŒ–æ•æ„Ÿï¼Œä¾‹å¦‚åœ¨å¤šé¸é¡Œå›ç­”ä»»å‹™ä¸­ï¼Œé¸é …çš„é †åºå¯èƒ½å½±éŸ¿å…¶è¡¨ç¾ã€‚\n",
      "   - ç›¸é—œè«–æ–‡ï¼šæœªæåŠ\n",
      "3. LLMsåœ¨ä½è³‡æºèªè¨€ä¸­è¡¨ç¾ä¸ä½³ï¼Œå¯èƒ½ç”±æ–¼ç¼ºä¹è¨“ç·´æ•¸æ“šå°è‡´ç„¡æ³•ç†è§£æ–‡åŒ–ç‰¹å®šè¡¨é”æˆ–æˆèªã€‚\n",
      "   - ç›¸é—œè«–æ–‡ï¼šZhang et al. 2023; Lankford, Afli, and Way 2024\n",
      "4. Transformerèªè¨€æ¨¡å‹éœ€è¦å¤§é‡è¨˜æ†¶é«”å’Œè¨ˆç®—è³‡æºï¼Œå°åºåˆ—é•·åº¦çš„è¦æ±‚å¢é•·è¿…é€Ÿï¼Œå¯èƒ½é™åˆ¶äº†æ¨¡å‹çš„è¨“ç·´ä¸Šä¸‹æ–‡çª—å£å¤§å°ã€‚\n",
      "   - ç›¸é—œè«–æ–‡ï¼šVaswani et al., 2017; Dai et al., 2019; Dao et al., 2022; Poli et al., 2023\n",
      "\n",
      "ç¶œåˆä»¥ä¸Šè³‡è¨Šï¼Œé€™å€‹æ¶æ§‹çš„é™åˆ¶åŒ…æ‹¬å°æç¤ºä¸­å…ƒç´ é †åºå’Œå¾®å°è®ŠåŒ–çš„æ•æ„Ÿæ€§ï¼Œä»¥åŠåœ¨ä½è³‡æºèªè¨€å’Œå¤§åºåˆ—é•·åº¦ä¸‹çš„è¡¨ç¾ä¸ä½³ã€‚\n"
     ]
    }
   ],
   "source": [
    "question = \"é€™å€‹æ¶æ§‹æœ‰ä»€éº¼é™åˆ¶ï¼Ÿ\"\n",
    "result = qa({\"question\": question})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "question = \"2023å¹´æœ‰å“ªäº›é‡è¦çš„è«–æ–‡ï¼Ÿ\"\n",
    "å›ç­”ï¼šæ ¹æ“šæª¢ç´¢åˆ°çš„æ–‡ç»å…§å®¹ï¼Œ2023å¹´çš„é‡è¦è«–æ–‡åŒ…æ‹¬ï¼š\n",
    "\n",
    "1. \"Lost in the Middle: How Language Models Use Long Contexts\"ï¼Œä½œè€…åŒ…æ‹¬Nelson F. Liuã€Kevin Linã€John Hewittç­‰äººï¼Œä¸»è¦æ¢è¨èªè¨€æ¨¡å‹åœ¨è™•ç†é•·æ–‡æœ¬ä¸Šçš„è¡¨ç¾ï¼Œä¸¦åˆ†æäº†åœ¨å¤šæ–‡æª”å•ç­”å’Œéµ-å€¼æª¢ç´¢ä»»å‹™ä¸­çš„è¡¨ç¾ã€‚\n",
    "\n",
    "2. \"Challenges: What problems remain unresolved?\"ï¼Œä½œè€…æœªæåŠï¼Œä¸»è¦æ¢è¨èªè¨€æ¨¡å‹ç ”ç©¶ä¸­å°šæœªè§£æ±ºçš„å•é¡Œã€‚\n",
    "\n",
    "3. \"Applications: Where are LLMs currently being applied, and how are the challenges constraining them?\"ï¼Œä½œè€…æœªæåŠï¼Œä¸»è¦æ¢è¨ç›®å‰èªè¨€æ¨¡å‹çš„æ‡‰ç”¨é ˜åŸŸä»¥åŠé¢è‡¨çš„æŒ‘æˆ°ã€‚\n",
    "\n",
    "ä»¥ä¸Šæ˜¯æ ¹æ“šæ–‡ç»å…§å®¹èƒ½å¤ æ‰¾åˆ°çš„2023å¹´é‡è¦è«–æ–‡ï¼Œå…¶ä»–æ–‡ç»ä¸­æœªæåŠç›¸é—œè³‡è¨Šã€‚ï¼ˆä½†å¦‚æœç”¨æ•¸2023çš„è«–æ–‡ï¼Œå…¶å¯¦æœ‰å…­ç¯‡ï¼Œè€Œä¸”æ¯ä¸€ç¯‡éƒ½æœ‰æåŠä½œè€…ï¼‰\n",
    "\n",
    "question = \"å‘Šè¨´æˆ‘é—œæ–¼LLM limitationçš„ç ”ç©¶ï¼Œä¸¦å±•ç¤ºå‡ºæ–‡ç« çš„åç¨±ï¼Ÿ\"\n",
    "å›ç­”ï¼š\n",
    "åœ¨LLM limitationçš„ç ”ç©¶ä¸­ï¼Œé‡è¦çš„ç™¼ç¾åŒ…æ‹¬ï¼š\n",
    "1. æ–‡ç»ä¸­æåˆ°LLMså°æ–¼promptä¸­å„å€‹å…ƒç´ çš„æ’åˆ—æ•æ„Ÿï¼Œé€™ç›´æ¥å½±éŸ¿äº†å®ƒå€‘åœ¨ç‰¹å®šä»»å‹™ä¸­ç†è§£å’Œæ¨ç†èƒ½åŠ›çš„è©•ä¼°ã€‚\n",
    "2. å…ˆå‰çš„ç ”ç©¶è¡¨æ˜ï¼ŒLLMså°æ–¼few-shot demonstrationsçš„æ’åˆ—å’Œå€™é¸æ¨¡å‹ç”Ÿæˆçš„å›æ‡‰é †åºæ•æ„Ÿï¼Œé€™å½±éŸ¿äº†LLMsä½œç‚ºè©•ä¼°è³ªé‡çš„è£åˆ¤æ™‚çš„è¡¨ç¾ã€‚\n",
    "3. LLMså°æ–¼promptä¸­å…ƒç´ çš„é †åºåœ¨ä¸åŒä»»å‹™ä¸­æ˜¯å¦æ•æ„Ÿï¼Œé€™ä¹Ÿæ˜¯ä¸€å€‹éœ€è¦æ¢è¨çš„å•é¡Œã€‚\n",
    "è©•æ¯”ï¼šçœ‹èµ·ä¾†å› ç‚ºæˆ‘å€‘çš„è§£æå™¨åªé‡å°å¹´ä»½å’Œä¸»é¡Œï¼Œå°ä½œè€…çš„metadataå¥½åƒæ²’é€™éº¼æ•æ„Ÿï¼ˆä½†å…¶å¯¦éƒ½æœ‰ï¼‰\n",
    "\n",
    "\n",
    "question = \"2024å¹´é—œæ–¼RAGçš„ç ”ç©¶æœ‰ä»€éº¼é€²å±•ï¼Ÿ\"\n",
    "æ ¹æ“šæª¢ç´¢åˆ°çš„æ–‡ç»ï¼Œ2024å¹´é—œæ–¼RAGçš„ç ”ç©¶æœ‰ä»¥ä¸‹é€²å±•ï¼š\n",
    "1. æ–‡ç»æ¨™é¡Œï¼šEvaluation of Retrieval-Augmented Generation: A Survey\n",
    "   ä¸»è¦å…§å®¹ï¼šè©²ç ”ç©¶å°RAGæ–¹æ³•é€²è¡Œäº†å…¨é¢å’Œç³»çµ±æ€§çš„å›é¡§ï¼Œæè¿°äº†å…¶é€éå¤–éƒ¨ä¿¡æ¯æª¢ç´¢å¢å¼·ç”Ÿæˆæ¨¡å‹çš„èƒ½åŠ›ã€‚ç ”ç©¶è©•ä¼°äº†RAGç³»çµ±çš„ç¨ç‰¹æŒ‘æˆ°ï¼Œä¸¦æå‡ºäº†çµ±ä¸€çš„è©•ä¼°éç¨‹ï¼Œæ—¨åœ¨æä¾›å°RAGç³»çµ±çš„è©•ä¼°å’ŒåŸºæº–çš„å…¨é¢æ¦‚è¿°ã€‚\n",
    "\n",
    "2. æ–‡ç»æ¨™é¡Œï¼šæœªæåŠRAGç ”ç©¶é€²å±•\n",
    "   ä¸»è¦å…§å®¹ï¼šæœªæåŠ2024å¹´é—œæ–¼RAGçš„å…·é«”ç ”ç©¶é€²å±•ã€‚\n",
    "\n",
    "è©•æ¯”ï¼šä½†å…¶å¯¦ragï¼’ï¼ï¼’ï¼”çš„pdfæœ‰äº”ç¯‡ã€‚\n",
    "question = \"ä»€éº¼æ˜¯attentionæ©Ÿåˆ¶ï¼Ÿ\"\n",
    "å›ç­”ï¼šæ ¹æ“šæª¢ç´¢åˆ°çš„æ–‡ç»å…§å®¹ï¼Œæåˆ°äº†ä¸€å€‹æ–°çš„ç°¡å–®ç¶²çµ¡æ¶æ§‹ï¼ŒTransformerï¼Œå®ƒå®Œå…¨åŸºæ–¼attentionæ©Ÿåˆ¶ï¼Œä¸ä½¿ç”¨å¾ªç’°å’Œå·ç©ã€‚é€™å€‹ç¶²çµ¡æ¶æ§‹å°‡ç·¨ç¢¼å™¨å’Œè§£ç¢¼å™¨é€šéattentionæ©Ÿåˆ¶é€£æ¥èµ·ä¾†ã€‚å› æ­¤ï¼Œæ ¹æ“šæ–‡ç»å…§å®¹ï¼Œattentionæ©Ÿåˆ¶æ˜¯ä¸€ç¨®é€£æ¥ç·¨ç¢¼å™¨å’Œè§£ç¢¼å™¨çš„æ©Ÿåˆ¶ï¼Œç”¨æ–¼åºåˆ—è½‰æ›æ¨¡å‹ä¸­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æª¢ç´¢åˆ°çš„æ–‡ä»¶æ•¸é‡: 5\n",
      "æ–‡ä»¶1: BloombergGPT- A Large Language Model for Finance\n",
      "å¹´ä»½: 2023\n",
      "å…§å®¹é è¦½: . . . . . . . . . . . . . . 9\n",
      "âˆ—. Co-first authors. Corresponding email: airesearch@bloomberg.net\n",
      "1\n",
      "a\n",
      "--------------------------------------------------\n",
      "æ–‡ä»¶2: Challenges and Applications of Large Language Models\n",
      "å¹´ä»½: 2023\n",
      "å…§å®¹é è¦½: . . . . 43\n",
      "3.8 Reasoning . . . . . . . . . . . . . 44\n",
      "3.9 Robotics and Embodied Agents . . 45\n",
      "3.10 S\n",
      "--------------------------------------------------\n",
      "æ–‡ä»¶3: Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions\n",
      "å¹´ä»½: 2023\n",
      "å…§å®¹é è¦½: . For example, how much does\n",
      "the order of options in multiple-choice question\n",
      "(MCQ) answering tasks \n",
      "--------------------------------------------------\n",
      "æ–‡ä»¶4: Lost in the Middle- How Language Models Use Long Contexts\n",
      "å¹´ä»½: 2023\n",
      "å…§å®¹é è¦½: Lost in the Middle: How Language Models Use Long Contexts\n",
      "Nelson F. Liu1âˆ— Kevin Lin2 John Hewitt1 As\n",
      "--------------------------------------------------\n",
      "æ–‡ä»¶5: Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions\n",
      "å¹´ä»½: 2023\n",
      "å…§å®¹é è¦½: windowâ€ (the example is from CSQA dataset).\n",
      "et al., 2022; Touvron et al., 2023; OpenAI, 2023).\n",
      "Howev\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# å…ˆå–®ç¨æ¸¬è©¦ä½ çš„retriever\n",
    "question = \"2023å¹´æœ‰å“ªäº›é‡è¦çš„è«–æ–‡ï¼Ÿ\"\n",
    "docs = custom_retriever._get_relevant_documents(question)\n",
    "print(f\"æª¢ç´¢åˆ°çš„æ–‡ä»¶æ•¸é‡: {len(docs)}\")\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"æ–‡ä»¶{i+1}: {doc.metadata.get('title', 'Unknown')}\")\n",
    "    print(f\"å¹´ä»½: {doc.metadata.get('year')}\")\n",
    "    print(f\"å…§å®¹é è¦½: {doc.page_content[:100]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ç›®å‰çš„ç†è§£**\n",
    "\n",
    "é¦–å…ˆåœ¨ä¹‹å‰æª¢ç´¢éšæ®µé‚„æ²’æœ‰åŠ å…¥å›ç­”å‰ï¼Œå°±å·²ç¶“æŠŠ`def llm_extract_metadata()`å’Œ`def smart_hybrid_search()`å»ºç«‹å¥½äº†ã€‚\n",
    "\n",
    "`smart_hybrid_search`æœƒå…ˆé€é`llm_extract_metadata`è§£æå™¨å›å‚³å¹´ä»½å’Œä¸»é¡Œã€‚\n",
    "\n",
    "æ‰€ä»¥ç•¶æ™‚çš„åŸ·è¡Œæ–¹æ³•æœƒæ˜¯ï¼š\n",
    "\n",
    "```python\n",
    "    test_queries =\"2024å¹´é—œæ–¼RAGçš„ç ”ç©¶\"\n",
    "    test_hybid = smart_hybrid_search(test_queries)\n",
    "```\n",
    "\n",
    "ä½†å› ç‚ºç¾åœ¨åŠ ä¸Šå›ç­”ï¼Œ`ConversationalRetrievalChain`éœ€è¦`retriever`ç‰©ä»¶ï¼ˆ`retriever=retriever`ï¼‰ï¼Œä½†å› ç‚ºå‰›å‰›åšçš„æ˜¯å‡½å¼ä¸¦éé€™ç¨®ç‰©ä»¶ï¼Œå› æ­¤éœ€è¦åšä¸€äº›èª¿æ•´ã€‚\n",
    "\n",
    "è€Œèª¿æ•´æ–¹æ³•æ˜¯éœ€è¦å°‡å‰›å‰›çš„æ··åˆæª¢ç´¢ã€ŒåŒ…è£æˆè‡ªå®šç¾©retriverç‰©ä»¶ã€ï¼Œæ‰å¯ä»¥å¯«å‡º`retriever=custom_retriever`ã€‚\n",
    "ä»¥ä¸Šæ˜¯æˆ‘èƒ½ç†è§£çš„åœ°æ–¹ï¼Œä½†å¯¦éš›ä¸Šçš„åšæ³•ä¸å¤ªæ‡‚ã€‚\n",
    "\n",
    "\n",
    "**ä¸å¤ªæ‡‚çš„åœ°æ–¹**\n",
    "\n",
    "- ä¸€ã€çœ‹èµ·ä¾†langchain.schemaå¯ä»¥è®“æˆ‘å€‘è‡ªå·±å»ºç«‹ä¸€å€‹retrieverç‰©ä»¶ï¼Œä½†æˆ‘ä¸å¤ªæ‡‚é€™å€‹å¥—ä»¶æ˜¯ä»€éº¼æ±è¥¿ï¼Œä¹Ÿä¸æ‡‚ç‚ºä»€éº¼æˆ‘å€‘éœ€è¦ä¸€å€‹classï¼Œå› ç‚ºæˆ‘ç›®å‰åªç†Ÿæ‚‰def\n",
    "    - å›ç­”\n",
    "        - defåƒæ˜¯ä¸€å€‹ã€Œå‹•ä½œã€ï¼ŒæŒ‰ä¸€ä¸‹åŸ·è¡Œä¸€æ¬¡ï¼›classåƒæ˜¯ä¸€å€‹è—åœ–ï¼Œè£¡é¢æœƒæœ‰ã€Œè³‡æ–™ã€ä¹Ÿæœƒæœ‰ã€Œå‹•ä½œã€\n",
    "        -   ä¾‹å¦‚åƒclass SmartHybridRetriever()å°±åŒ…å«äº†ï¼š\n",
    "            - è³‡æ–™ï¼švectordbã€llm\n",
    "            - å‹•ä½œï¼šllm_extract_metadata()ã€smart_hybrid_search()\n",
    "        - ç‚ºä»€éº¼è¦åŒ…æˆclass?å› ç‚ºConversationalRetrievalChainåªèªå¾—ç‰©ä»¶ï¼Œä¸èƒ½ç›´æ¥åƒä¸€å€‹å‡½å¼ï¼Œæ‰€ä»¥æˆ‘å€‘éœ€è¦ä¸€å€‹classç‰©ä»¶ï¼ŒæŠŠå…©å€‹å‡½å¼åŒ…èµ·ä¾†ã€‚\n",
    "\n",
    "\n",
    "äºŒã€æ›´é€²ä¸€æ­¥ï¼Œfrom typing import List, Anyå’Œfrom pydantic import Fieldæ›´æ˜¯å®Œå…¨æ²’ç¢°éäº†ï¼Œæ‰€ä»¥ä¸æ‡‚vectordb: Any = Field(default=None)\n",
    "    llm: Any = Field(default=None)æ˜¯ä»€éº¼ã€ï¼¿ï¼¿init__è£¡é¢çš„super(), objectä¹Ÿå…¨éƒ¨ä¸æ‡‚ã€‚\n",
    "\n",
    "ä¸‰ã€ç„¶å¾Œçœ‹èµ·ä¾†def llm_extract_metadata()å’Œdef smart_hybrid_search()éƒ½å¢åŠ äº†selfçš„åƒæ•¸ï¼Œæ˜¯ä»€éº¼æ„æ€ï¼Ÿ\n",
    "\n",
    "å››ã€def _get_relevant_documents(self, query: str) -> List[Document]:é€™åˆæ˜¯ä»€éº¼ï¼Ÿä»€éº¼å«åšè¦æ±‚æŠ½è±¡çš„æ–¹æ³•ï¼Ÿ\n",
    "        \"\"\"BaseRetrieverè¦æ±‚çš„æŠ½è±¡æ–¹æ³•\"\"\"\n",
    "\n",
    "äº”ã€ç„¶å¾Œæˆ‘å¾ä¾†ä¹Ÿæ²’çœ‹é async def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸€ã€å› ç‚ºclasså°±åƒæ˜¯ä¸€å¼µè—åœ–ä¸€æ¨£ï¼Œä»–åŒæ™‚åŒ…å«äº†æ‰€éœ€çš„ã€Œè³‡æ–™ï¼ˆå±¬æ€§ï¼‰ã€å’Œã€Œè¡Œç‚ºï¼ˆå‡½å¼ï¼‰ã€ï¼Œå¦‚æœæ˜¯defï¼Œæˆ‘å€‘èƒ½æ¯ä¸€å€‹åŸ·è¡Œdeféƒ½æ¯”å™“æ‹–è‘—ä¸€å¤§å †çš„å‡½æ•¸ä¸”ä»–éœ€è¦æŒ‰ç…§é †åºä¾†åŸ·è¡Œï¼Œå› æ­¤ç¨‹å¼æœƒé¡¯å¾—å¾ˆæ··äº‚ï¼Œè€Œä¸”å‡è¨­ä»Šå¤©æœ‰å¤§å­¸ç”Ÿå’Œå­¸ç”Ÿå…©ç¨®å°è±¡ï¼Œå…¶å¯¦ä»–å€‘å¯ä»¥æœ‰å€‹çˆ¶é¡å«åšã€Œå­¸ç”Ÿã€ï¼Œæ¥ä¸‹ä¾†åœ¨ä»–å€‘å„è‡ªçš„classè£¡ç¹¼æ‰¿å­¸ç”Ÿçš„åŸºæœ¬è¼ªå»“ï¼Œå†åŠ å…¥ç‰¹å®šè¡Œç‚ºçŸ¥è­˜å°±å¯ä»¥äº†ï¼Œå¦‚æœæ˜¯defå¯ä»¥èƒ½å°±æ²’æ³•é€™æ¨£ç¹¼æ‰¿å’Œå…±ç”¨ï¼Œè³‡æ–™æœƒä¸æ®µçš„å‚³ä¾†å‚³å»ã€‚\n",
    "äºŒã€å› ç‚ºåœ¨æˆ‘å€‘çš„classè£¡ï¼Œæœ‰äº›éœ€è¦é€éçˆ¶é¡ä¾†ç¹¼æ‰¿ï¼Œç¹¼æ‰¿çš„è©±å°±æœƒå¯«æˆsuper().__init__()ï¼Œä»£è¡¨æ¥æ”¶çˆ¶é¡çš„å±¬æ€§åˆ°è‡ªå·±çš„å±¬æ€§ä¸­\n",
    "\n",
    "ä¸‰ã€å› ç‚ºBaseRetrieverä»–ä¸æ˜¯ä¸€ç­çš„classï¼Œè€Œæ˜¯pydantic classï¼Œé€™ç¨®è¦å®šè¦å…ˆæª¢æŸ¥å±¬æ€§ï¼Œæ‰€ä»¥æˆ‘å€‘æ‰æœƒæœ‰fieldé‚£å…©è¡Œ\n",
    "\n",
    "å››ã€selfæŒ‡çš„æ˜¯åœ¨classä¹‹å¤–å‘¼å«é€™å€‹é¡åˆ¥çš„ã€Œå°è±¡ã€ï¼Œä»Šå¤©å°è±¡å°±æœƒæ˜¯retriever\n",
    "\n",
    "äº”ã€å› ç‚ºå‡è¨­r1æ˜¯çµ¦llmè«–æ–‡ã€R2æ˜¯çµ¦ç¤¾å·¥è«–æ–‡ï¼Œå¦‚æœæˆ‘å€‘ä»Šå¤©è¦ä½¿ç”¨r1ï¼Œåªè¦ä½¿ç”¨llm = r1.search(\"å•é¡Œa\")\n",
    "sw = r2.search(\"å•é¡Œb\")\n",
    "å°±ä¸æœƒææ··äº†ï¼ˆè«‹ä½ å¤šèªªæ˜ï¼‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
