{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立整體的ui介面，變成一個問答機器人\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA,  ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "目的：建立langchain + openai 的基礎環境\n",
    "'''\n",
    "import os # 作業系統相關功能（讀取環境變數）\n",
    "from openai import OpenAI # openai api 客戶端\n",
    "from dotenv import load_dotenv, find_dotenv # dotenv 是專門用來讀取.env套件的套件，並接上環境\n",
    "_ = load_dotenv(find_dotenv()) # 讀取.env檔案\n",
    "client = OpenAI(\n",
    "    api_key=os.environ['OPENAI_API_KEY']\n",
    ")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success! API key loaded\n",
      "Key starts with: sk-proj-FWUUter...\n",
      "Key length: 164 characters\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 清除舊的環境變數\n",
    "if 'OPENAI_API_KEY' in os.environ:\n",
    "    del os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# 重新載入\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.environ.get('OPENAI_API_KEY')\n",
    "if api_key:\n",
    "    print(\"✅ Success! API key loaded\")\n",
    "    print(f\"Key starts with: {api_key[:15]}...\")\n",
    "    print(f\"Key length: {len(api_key)} characters\")\n",
    "else:\n",
    "    print(\"❌ Still not working\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder exists: True\n"
     ]
    }
   ],
   "source": [
    "# 檢查資料夾是否存在\n",
    "import os\n",
    "folder_path = \"/Users/mangtinglee/Desktop/2025_gap_careerpath/RAG_LLM/pdfs\"\n",
    "\n",
    "print(f\"Folder exists: {os.path.exists(folder_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "載入metadata對應表，共 24 筆資料\n",
      "處理：2023_LLM limitation_Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions.pdf\n",
      "原始頁數：11 → 只保留第一頁：1 頁\n",
      "  ✅ 已更新metadata: Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions\n",
      "處理：2025_LLM limitation_The Order Effect- Investigating Prompt Sensitivity to Input Order in LLMs.pdf\n",
      "原始頁數：13 → 只保留第一頁：1 頁\n",
      "  ✅ 已更新metadata: The Order Effect- Investigating Prompt Sensitivity to Input Order in LLMs\n",
      "處理：2024_RAG_Evaluation of Retrieval-Augmented Generation- A Survey.pdf\n",
      "原始頁數：21 → 只保留第一頁：1 頁\n",
      "  ✅ 已更新metadata: Evaluation of Retrieval-Augmented Generation- A Survey\n",
      "處理：2020_scaling laws_Scaling Laws for Neural Language Models.pdf\n",
      "原始頁數：30 → 只保留第一頁：1 頁\n",
      "  ✅ 已更新metadata: Scaling Laws for Neural Language Models\n",
      "處理：2022_LLM limitation_Robustness of Learning from Task Instructions.pdf\n",
      "原始頁數：12 → 只保留第一頁：1 頁\n",
      "  ✅ 已更新metadata: Robustness of Learning from Task Instructions\n",
      "處理：2024_RAG_Retrieval-Augmented Generation for Large Language Models- A Survey.pdf\n",
      "原始頁數：21 → 只保留第一頁：1 頁\n",
      "  ✅ 已更新metadata: Retrieval-Augmented Generation for Large Language Models- A Survey\n",
      "處理：2017_transformer_Attention Is All You Need.pdf\n",
      "原始頁數：15 → 只保留第一頁：1 頁\n",
      "  ✅ 已更新metadata: Attention Is All You Need\n",
      "處理：2024_LLM limitation_Evaluating the Zero-shot Robustness of Instruction-tuned Language Models.pdf\n",
      "原始頁數：127 → 只保留第一頁：1 頁\n",
      "  ✅ 已更新metadata: Evaluating the Zero-shot Robustness of Instruction-tuned Language Models\n",
      "處理：2024_RAG_Modular RAG- Transforming RAG Systems into LEGO-like Reconfigurable Frameworks.pdf\n",
      "原始頁數：17 → 只保留第一頁：1 頁\n",
      "  ✅ 已更新metadata: Modular RAG- Transforming RAG Systems into LEGO-like Reconfigurable Frameworks\n",
      "處理：2024_LLM limitation_LLaMA Beyond English- An Empirical Study on Language Capability Transfer.pdf\n",
      "原始頁數：10 → 只保留第一頁：1 頁\n",
      "  ✅ 已更新metadata: LLaMA Beyond English- An Empirical Study on Language Capability Transfer\n",
      "處理：2024_LLM limitation_Language Ranker- A Metric for Quantifying LLM Performance Across High and Low-Resource Languages.pdf\n",
      "原始頁數：11 → 只保留第一頁：1 頁\n",
      "  ✅ 已更新metadata: Language Ranker- A Metric for Quantifying LLM Performance Across High and Low-Resource Languages\n",
      "處理：2023_LLM limitation_Evaluating the Zero-shot Robustness of Instruction-tuned Language Models.pdf\n",
      "原始頁數：15 → 只保留第一頁：1 頁\n",
      "  ✅ 已更新metadata: Evaluating the Zero-shot Robustness of Instruction-tuned Language Models\n",
      "處理：2024_RAG_The Power of Noise- Redefining Retrieval for RAG Systems.pdf\n",
      "原始頁數：11 → 只保留第一頁：1 頁\n",
      "  ✅ 已更新metadata: The Power of Noise- Redefining Retrieval for RAG Systems\n",
      "處理：2022_LLM limitation_The Reversal Curse- LLMs trained on \"A is B\" fail to learn \"B is A\".pdf\n",
      "原始頁數：21 → 只保留第一頁：1 頁\n",
      "  ✅ 已更新metadata: The Reversal Curse- LLMs trained on \"A is B\" fail to learn \"B is A\"\n",
      "處理：2022_fine tuning_Scaling Instruction-Finetuned Language Models.pdf\n",
      "原始頁數：54 → 只保留第一頁：1 頁\n",
      "  ✅ 已更新metadata: Scaling Instruction-Finetuned Language Models\n",
      "處理：2023_LLM limitation_Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or- How I learned to start worrying about prompt formatting.pdf\n",
      "原始頁數：29 → 只保留第一頁：1 頁\n",
      "  ✅ 已更新metadata: Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or- How I learned to start worrying about prompt formatting\n",
      "處理：2024_LLM limitation_Does Prompt Formatting Have Any Impact on LLM Performance?.pdf\n",
      "原始頁數：16 → 只保留第一頁：1 頁\n",
      "  ✅ 已更新metadata: Does Prompt Formatting Have Any Impact on LLM Performance?\n",
      "處理：2023_LLM application_Challenges and Applications of Large Language Models.pdf\n",
      "原始頁數：72 → 只保留第一頁：1 頁\n",
      "  ✅ 已更新metadata: Challenges and Applications of Large Language Models\n",
      "處理：2023_scaling laws_BloombergGPT- A Large Language Model for Finance.pdf\n",
      "原始頁數：76 → 只保留第一頁：1 頁\n",
      "  ✅ 已更新metadata: BloombergGPT- A Large Language Model for Finance\n",
      "處理：2025_ai-agent_Large Language Model Agent- A Survey on Methodology, Applications and Challenges.pdf\n",
      "原始頁數：26 → 只保留第一頁：1 頁\n",
      "  ✅ 已更新metadata: Large Language Model Agent- A Survey on Methodology, Applications and Challenges\n",
      "處理：2024_fine tuning_Scaling Down to Scale Up- A Guide to Parameter-Efficient Fine-Tuning.pdf\n",
      "原始頁數：57 → 只保留第一頁：1 頁\n",
      "  ✅ 已更新metadata: Scaling Down to Scale Up- A Guide to Parameter-Efficient Fine-Tuning\n",
      "處理：2024_LLM limitation_Spanish and LLM Benchmarks- is MMLU Lost in Translation?.pdf\n",
      "原始頁數：8 → 只保留第一頁：1 頁\n",
      "  ✅ 已更新metadata: Spanish and LLM Benchmarks- is MMLU Lost in Translation?\n",
      "處理：2024_prompt engineering_The Prompt Report - A Systematic Survey of Prompt Engineering Techniques.pdf\n",
      "原始頁數：80 → 只保留第一頁：1 頁\n",
      "  ✅ 已更新metadata: The Prompt Report - A Systematic Survey of Prompt Engineering Techniques\n",
      "處理：2023_LLM limitation_Lost in the Middle- How Language Models Use Long Contexts.pdf\n",
      "原始頁數：18 → 只保留第一頁：1 頁\n",
      "  ✅ 已更新metadata: Lost in the Middle- How Language Models Use Long Contexts\n",
      "總共載入 24 個第一頁，metadata已更新\n",
      "\n",
      "--- Document 1 ---\n",
      "Title: Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions\n",
      "Year: 2023\n",
      "Authors: Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions\n",
      "Topic: LLM limitation\n",
      "\n",
      "--- Document 2 ---\n",
      "Title: The Order Effect- Investigating Prompt Sensitivity to Input Order in LLMs\n",
      "Year: 2025\n",
      "Authors: Bryan Guan, Tanya Roosta, Peyman Passban, Mehdi Rezagholizadeh\n",
      "Topic: LLM limitation\n",
      "\n",
      "--- Document 3 ---\n",
      "Title: Evaluation of Retrieval-Augmented Generation- A Survey\n",
      "Year: 2024\n",
      "Authors: Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, Zhaofeng Liu\n",
      "Topic: RAG\n"
     ]
    }
   ],
   "source": [
    "# 執行\n",
    "import pandas as pd\n",
    "\n",
    "def load_metadata_mapping(csv_path):\n",
    "    \"\"\"讀取CSV檔案建立metadata對應表\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df.columns = df.columns.str.strip() # df.colums是物件，加上'.str()'轉換成陣列才可以使用.strip()\n",
    "    # 將filename作為key，其他資訊作為value\n",
    "    metadata_map = {}\n",
    "    for _, row in df.iterrows():\n",
    "        metadata_map[row['filename']] = {\n",
    "            'title': row['title'],\n",
    "            'year': row['year'],\n",
    "            'authors': row['authors'],\n",
    "            'topic': row['topic']\n",
    "        }\n",
    "    return metadata_map\n",
    "\n",
    "def filter_first_page_only(documents):\n",
    "    \"\"\"只保留第一頁（Abstract）\"\"\"\n",
    "    first_page_docs = [doc for doc in documents if doc.metadata['page'] == 0]\n",
    "    print(f\"原始頁數：{len(documents)} → 只保留第一頁：{len(first_page_docs)} 頁\")\n",
    "    return first_page_docs\n",
    "\n",
    "def load_all_first_pages_with_csv_metadata(folder_path, csv_path):\n",
    "    \"\"\"載入PDF第一頁並從CSV對應metadata\"\"\"\n",
    "    \n",
    "    # 1. 先讀取metadata對應表\n",
    "    metadata_map = load_metadata_mapping(csv_path)\n",
    "    print(f\"載入metadata對應表，共 {len(metadata_map)} 筆資料\")\n",
    "    \n",
    "    all_first_pages = []\n",
    "    pdf_files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"處理：{pdf_file}\")\n",
    "        \n",
    "        file_path = os.path.join(folder_path, pdf_file)\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        \n",
    "        # 只取第一頁\n",
    "        first_page_docs = filter_first_page_only(documents)\n",
    "        \n",
    "        # 🎯 關鍵：從CSV對應metadata\n",
    "        for doc in first_page_docs:\n",
    "            if pdf_file in metadata_map:\n",
    "                # 找到對應的metadata\n",
    "                doc.metadata.update(metadata_map[pdf_file])\n",
    "                print(f\"  ✅ 已更新metadata: {metadata_map[pdf_file]['title']}\")\n",
    "            else:\n",
    "                # 找不到對應資料\n",
    "                print(f\"  ⚠️  警告：{pdf_file} 在CSV中找不到對應資料\")\n",
    "                doc.metadata.update({\n",
    "                    'title': pdf_file.replace('.pdf', ''),\n",
    "                    'year': None,\n",
    "                    'authors': 'Unknown',\n",
    "                    'topic': 'Unknown'\n",
    "                })\n",
    "        \n",
    "        all_first_pages.extend(first_page_docs)\n",
    "    \n",
    "    print(f\"總共載入 {len(all_first_pages)} 個第一頁，metadata已更新\")\n",
    "\n",
    "    # all_first_pages = clean_metadata(all_first_pages)\n",
    "\n",
    "    return all_first_pages\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 使用\n",
    "\n",
    "csv_path = \"/Users/mangtinglee/Desktop/2025_gap_careerpath/RAG_LLM/meta_data_correction.csv\"\n",
    "all_abstracts_with_metadata = load_all_first_pages_with_csv_metadata(folder_path, csv_path)\n",
    "# all_abstracts_with_metadata = clean_metadata(all_abstracts_with_metadata)\n",
    "\n",
    "for i, doc in enumerate(all_abstracts_with_metadata[:3]):\n",
    "    print(f\"\\n--- Document {i+1} ---\")\n",
    "    print(f\"Title: {doc.metadata.get('title', 'N/A')}\")\n",
    "    print(f\"Year: {doc.metadata.get('year', 'N/A')}\")\n",
    "    print(f\"Authors: {doc.metadata.get('authors', 'N/A')}\")\n",
    "    print(f\"Topic: {doc.metadata.get('topic', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n",
      "{'source': '/Users/mangtinglee/Desktop/2025_gap_careerpath/RAG_LLM/pdfs/2020_scaling laws_Scaling Laws for Neural Language Models.pdf', 'total_pages': 30, 'title': 'Scaling Laws for Neural Language Models', 'page': 0, 'year': 2020, 'authors': 'Jared Kaplan,\\xa0Sam McCandlish,\\xa0Tom Henighan,\\xa0Tom B. Brown,\\xa0Benjamin Chess,\\xa0Rewon Child,\\xa0Scott Gray,\\xa0Alec Radford,\\xa0Jeffrey Wu,\\xa0Dario Amodei', 'topic': 'scaling laws'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n為什麼這樣寫？\\n因為載入後會變成langchain的document形式，例如：\\n    load_result = [\\n        Document(page_content = '...', metedata = {}),\\n        Document(page_content = '...', metedata = {}),\\n        Document(page_content = '...', metedata = {})                \\n    ]\\n所以，當我們要修改metedata格式時，需要透過遍歷的方式處理（因為def clean_metadata只吃「單一Document的metadata（字典型態）」）\\n因此最直接的做法就是\\nfor doc in result:\\n    doc.metadata = clean_metadata(doc.metadata)\\n\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_metadata(metadata): # 只接收單一doc.metadata\n",
    "    \"\"\"清理並標準化metadata\"\"\"\n",
    "    keep_keys = ['source', 'total_pages', 'title', 'page', 'year', 'authors', 'topic']\n",
    "    \n",
    "    clean_meta = {}\n",
    "    for key in keep_keys:\n",
    "        if key in metadata:\n",
    "            clean_meta[key] = metadata[key]\n",
    "    \n",
    "    return clean_meta\n",
    "\n",
    "for doc in all_abstracts_with_metadata:\n",
    "    doc.metadata = clean_metadata(doc.metadata)\n",
    "print(\"done!\")\n",
    "\n",
    "print(all_abstracts_with_metadata[3].metadata)\n",
    "\n",
    "\"\"\"\n",
    "為什麼這樣寫？\n",
    "因為載入後會變成langchain的document形式，例如：\n",
    "    load_result = [\n",
    "        Document(page_content = '...', metedata = {}),\n",
    "        Document(page_content = '...', metedata = {}),\n",
    "        Document(page_content = '...', metedata = {})                \n",
    "    ]\n",
    "所以，當我們要修改metedata格式時，需要透過遍歷的方式處理（因為def clean_metadata只吃「單一Document的metadata（字典型態）」）\n",
    "因此最直接的做法就是\n",
    "for doc in result:\n",
    "    doc.metadata = clean_metadata(doc.metadata)\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "# 分割檔案\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=50,\n",
    "    separators=[ \"\\n\\n\", \". \", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
    "    )    \n",
    "docs = text_splitter.split_documents(all_abstracts_with_metadata)\n",
    "print(len(docs)) # chunks\n",
    "print(len(all_abstracts_with_metadata)) # test 11 pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': '/Users/mangtinglee/Desktop/2025_gap_careerpath/RAG_LLM/pdfs/2023_LLM limitation_Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions.pdf', 'total_pages': 11, 'title': 'Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions', 'page': 0, 'year': 2023, 'authors': 'Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions', 'topic': 'LLM limitation'}, page_content='Large Language Models Sensitivity to The Order of Options in\\nMultiple-Choice Questions\\nPouya Pezeshkpour\\nMegagon Labs\\npouya@megagon.ai\\nEstevam Hruschka\\nMegagon Labs\\nestevam@megagon.ai\\nAbstract\\nLarge Language Models (LLMs) have demon-\\nstrated remarkable capabilities in various NLP\\ntasks. However, previous works have shown\\nthese models are sensitive towards prompt\\nwording, and few-shot demonstrations and\\ntheir order, posing challenges to fair assess-\\nment of these models. As these models be-\\ncome more powerful, it becomes imperative\\nto understand and address these limitations.\\nIn this paper, we focus on LLMs robust-\\nness on the task of multiple-choice questions—\\ncommonly adopted task to study reasoning and\\nfact-retrieving capability of LLMs')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish！\n",
      "資料夾存在嗎？True\n"
     ]
    }
   ],
   "source": [
    "# step.3 - embedding\n",
    "\n",
    "import os\n",
    "\n",
    "# 建立資料夾\n",
    "os.makedirs('./chroma_db', exist_ok=True)\n",
    "print(\"finish！\")\n",
    "\n",
    "# 檢查是否成功\n",
    "print(f\"資料夾存在嗎？{os.path.exists('./chroma_db')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define embedding\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意，需要先在自己的環境中建立資料庫路徑\n",
    "persist_directory = './chroma_db' # 指定資料庫路徑\n",
    "!rm -rf ./chroma_db  # remove old database files if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立新的向量資料庫，並將文件放進去\n",
    "from langchain.vectorstores import Chroma\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_x/xcmm2w3d1hld6_nc6t0xyrch0000gn/T/ipykernel_58661/4092891401.py:1: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist() # 手動儲存剛剛建立的資料庫（跟相似度沒有關係，是儲存資料庫）\n"
     ]
    }
   ],
   "source": [
    "vectordb.persist() # 手動儲存剛剛建立的資料庫（現在不用手動了，自動儲存）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 每次開始新session只需要（不需要經過前面的階段）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting langchain_chroma\n",
      "  Downloading langchain_chroma-0.2.6-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting langchain-core>=0.3.76 (from langchain_chroma)\n",
      "  Downloading langchain_core-0.3.76-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from langchain_chroma) (2.0.2)\n",
      "Requirement already satisfied: chromadb>=1.0.20 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from langchain_chroma) (1.0.20)\n",
      "Requirement already satisfied: build>=1.0.3 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (1.3.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (2.10.3)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (1.4.2)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.20->langchain_chroma) (0.35.0)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (5.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (4.12.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (1.19.2)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (1.36.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (0.22.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (1.71.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (0.17.3)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (33.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (8.2.3)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (6.0.1)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (3.11.2)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (14.0.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from chromadb>=1.0.20->langchain_chroma) (4.20.0)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from langchain-core>=0.3.76->langchain_chroma) (0.4.14)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from langchain-core>=0.3.76->langchain_chroma) (1.33)\n",
      "Requirement already satisfied: packaging>=23.2 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from langchain-core>=0.3.76->langchain_chroma) (25.0)\n",
      "Requirement already satisfied: pyproject_hooks in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from build>=1.0.3->chromadb>=1.0.20->langchain_chroma) (1.2.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from build>=1.0.3->chromadb>=1.0.20->langchain_chroma) (6.8.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from build>=1.0.3->chromadb>=1.0.20->langchain_chroma) (2.0.1)\n",
      "Requirement already satisfied: anyio in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from httpx>=0.27.0->chromadb>=1.0.20->langchain_chroma) (4.0.0)\n",
      "Requirement already satisfied: certifi in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from httpx>=0.27.0->chromadb>=1.0.20->langchain_chroma) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from httpx>=0.27.0->chromadb>=1.0.20->langchain_chroma) (1.0.7)\n",
      "Requirement already satisfied: idna in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from httpx>=0.27.0->chromadb>=1.0.20->langchain_chroma) (3.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb>=1.0.20->langchain_chroma) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.3.76->langchain_chroma) (2.4)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from jsonschema>=4.19.0->chromadb>=1.0.20->langchain_chroma) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from jsonschema>=4.19.0->chromadb>=1.0.20->langchain_chroma) (2023.11.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from jsonschema>=4.19.0->chromadb>=1.0.20->langchain_chroma) (0.31.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from jsonschema>=4.19.0->chromadb>=1.0.20->langchain_chroma) (0.13.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (1.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (2.8.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (2.40.3)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (1.6.4)\n",
      "Requirement already satisfied: requests in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (2.32.5)\n",
      "Requirement already satisfied: requests-oauthlib in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (3.3.1)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (2.1.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (0.10)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from langsmith>=0.3.45->langchain-core>=0.3.76->langchain_chroma) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from langsmith>=0.3.45->langchain-core>=0.3.76->langchain_chroma) (0.24.0)\n",
      "Requirement already satisfied: coloredlogs in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from onnxruntime>=1.14.1->chromadb>=1.0.20->langchain_chroma) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from onnxruntime>=1.14.1->chromadb>=1.0.20->langchain_chroma) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from onnxruntime>=1.14.1->chromadb>=1.0.20->langchain_chroma) (5.29.4)\n",
      "Requirement already satisfied: sympy in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from onnxruntime>=1.14.1->chromadb>=1.0.20->langchain_chroma) (1.14.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.20->langchain_chroma) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.36.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.20->langchain_chroma) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.36.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.20->langchain_chroma) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from opentelemetry-sdk>=1.2.0->chromadb>=1.0.20->langchain_chroma) (0.57b0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from posthog<6.0.0,>=2.4.0->chromadb>=1.0.20->langchain_chroma) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from posthog<6.0.0,>=2.4.0->chromadb>=1.0.20->langchain_chroma) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from pydantic>=1.9->chromadb>=1.0.20->langchain_chroma) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from pydantic>=1.9->chromadb>=1.0.20->langchain_chroma) (2.27.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from rich>=10.11.0->chromadb>=1.0.20->langchain_chroma) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from rich>=10.11.0->chromadb>=1.0.20->langchain_chroma) (2.17.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from tokenizers>=0.13.2->chromadb>=1.0.20->langchain_chroma) (0.34.4)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from typer>=0.9.0->chromadb>=1.0.20->langchain_chroma) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from typer>=0.9.0->chromadb>=1.0.20->langchain_chroma) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.20->langchain_chroma) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.20->langchain_chroma) (1.1.1)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.20->langchain_chroma) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.20->langchain_chroma) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.20->langchain_chroma) (15.0.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from importlib-resources->chromadb>=1.0.20->langchain_chroma) (3.17.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (4.9.1)\n",
      "Requirement already satisfied: filelock in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.20->langchain_chroma) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.20->langchain_chroma) (2025.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.20->langchain_chroma) (1.1.9)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=1.0.20->langchain_chroma) (0.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from requests->kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (3.3.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from anyio->httpx>=0.27.0->chromadb>=1.0.20->langchain_chroma) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from anyio->httpx>=0.27.0->chromadb>=1.0.20->langchain_chroma) (1.2.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb>=1.0.20->langchain_chroma) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from sympy->onnxruntime>=1.14.1->chromadb>=1.0.20->langchain_chroma) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /Users/mangtinglee/Library/Python/3.9/lib/python/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.20->langchain_chroma) (0.6.1)\n",
      "Downloading langchain_chroma-0.2.6-py3-none-any.whl (12 kB)\n",
      "Downloading langchain_core-0.3.76-py3-none-any.whl (447 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: langchain-core, langchain_chroma\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.75\n",
      "    Uninstalling langchain-core-0.3.75:\n",
      "      Successfully uninstalled langchain-core-0.3.75\n",
      "Successfully installed langchain-core-0.3.76 langchain_chroma-0.2.6\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain_chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# 啟動資料庫\n",
    "import os # 作業系統相關功能（讀取環境變數）\n",
    "from openai import OpenAI # openai api 客戶端\n",
    "from dotenv import load_dotenv, find_dotenv # dotenv 是專門用來讀取.env套件的套件，並接上環境\n",
    "_ = load_dotenv(find_dotenv()) # 讀取.env檔案\n",
    "client = OpenAI(\n",
    "    api_key=os.environ['OPENAI_API_KEY']\n",
    ")\n",
    "print(\"done\")\n",
    "# from langchain.vectorstores import Chroma\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectordb = Chroma(persist_directory=\"./chroma_db\", embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 小筆記：metadata「不是預設會被向量化的內容」，通常只有 page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 問題驗證環節"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  strategic decision\n",
    "##### 問題：「我們應該先在相似度上取得一定的結果再嘗試進階檢索方式，還是先試試看進階檢索方式的情況？」\n",
    "##### 回答：應該先建立 baseline\n",
    "- 有了baseline，才知道基礎是否已可用，進階的改善了多少\n",
    "##### 後續流程\n",
    "- 一、透過基礎相似度評估回答狀況：用不同問題測試，並且閱讀回答情況（閱讀理解）→ 剛剛已完成\n",
    "- 二、透過基礎相似度建立 baseline ：運用量化指標，了解現在「相似度」的回答狀況 →稍後執行\n",
    "- 三、baseline建立完成後，比較mmr和Self-query檢索狀況，決定要用哪一種(有數據的支持)\n",
    "- 四、確認檢索方法後，進一步建立llm回答機制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第一步驟結果評估：\n",
    "- Question 2 & 5（關於Transformer/Attention) -> 表現良好\n",
    "- Question 3 (量子計算) & Question 4 (煮義大利麵)：因為是透過「相似性檢索」處理 -> 表現正常\n",
    "- Question 1 (2023年論文) 沒有很好地利用年份資訊(需要檢索2023)\n",
    "#### 判斷檢索品質：\n",
    "- 相關度分數分佈（某些問題應該高分、某些問題應該低分）：邊界問題應該要低分、中等相關中間、高度相關分數應該要高\n",
    "- 結果多樣性：可以檢驗的多元性有「論文來源的多樣性」、「主題多樣性」、「時間多樣性」等，總之就是不希望回答都是一樣的\n",
    "- metadata準確率：:檢驗問「2023」年、某某作者、某主題的論文，回傳結果中，真正符合年份的比例，例如k=3，有2題對就是67%\n",
    "- 檢索速度：就是計時\n",
    "- 建立問題集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一步驟：基礎理解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Level 1：基礎功能驗證\n",
    "\n",
    "# 1. Metadata查詢測試\n",
    "question1 = \"What papers were published in 2023?\"\n",
    "ans_docs1 = vectordb.similarity_search(question1,k=2) # 問題參數記得要改\n",
    "\n",
    "question2 = \"Which authors studied transformer architecture?\"\n",
    "ans_docs2 = vectordb.similarity_search(question2,k=2)\n",
    "\n",
    "# 2. 邊界情況測試\n",
    "question3 = \"What is quantum computing?\"\n",
    "ans_docs3 = vectordb.similarity_search(question3,k=2)\n",
    "\n",
    "question4 = \"How to cook pasta?\"  \n",
    "ans_docs4 = vectordb.similarity_search(question4,k=2)\n",
    "\n",
    "question5 = \"What is the main contribution of Attention is All You Need?\"\n",
    "ans_docs5 = vectordb.similarity_search(question5,k=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 檢驗量化指標"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_time_score(vectordb, test_queries, k=3, iterations=5): # （向量資料庫、測試查詢列表、檢索數量、每隔查詢重複測試次數）\n",
    "\n",
    "    import time \n",
    "\n",
    "    all_times = [] # 三個問題的總時間\n",
    "\n",
    "    for query in test_queries: # 共有三個問題、迭代五次、每次產出3塊\n",
    "        query_times = [] # total times\n",
    "        for _ in range(iterations):  # iterations 剛剛已經預設 5\n",
    "            start_time = time.time() # start timing\n",
    "            vectordb.similarity_search(query, k=k) # k 預設為3, query = each questions\n",
    "            end_time = time.time() # end timing\n",
    "            query_times.append(end_time - start_time) # 紀錄某問題迭代五次的時間\n",
    "        avg_time = sum(query_times) / len(query_times)\n",
    "        all_times.append(avg_time)\n",
    "\n",
    "    return  {\n",
    "        'avg_response_time': sum(all_times) / len(all_times),\n",
    "        'min_response_time': min(all_times),\n",
    "        'max_response_time': max(all_times)\n",
    "    }\n",
    "\n",
    "# 測試相關性（不同問題的相關性應該不同）\n",
    "# 小筆記：問題一定要跟塊有所互動，才會有「分數」（similarity_search_with_score），因此這裡是透過問題返回點積。\n",
    "# 向量模型只要同一間公司，向量的方式都一樣，和資料庫或內容沒有關係\n",
    "# 分數其實就是「距離（cosine distance）」，所以數字越小越相關\n",
    "def evaluate_relevance_score(vectordb, test_queries_dict):\n",
    "    # 已經建立一個有高相關和低相關的問題集\n",
    "    results = {}\n",
    "\n",
    "    for catagory, queries in test_queries_dict.items(): # loop for find out each key(high/low):value(question)\n",
    "        scores =[]\n",
    "        for q in queries:\n",
    "            docs_with_scores = vectordb.similarity_search_with_score(q, k=3) \n",
    "            avg_score = sum([score for doc, score in docs_with_scores])/len([docs_with_scores])\n",
    "            scores.append(avg_score)\n",
    "\n",
    "        results[catagory]={\n",
    "            'avg_score': sum(scores) / len(scores),\n",
    "            'min_score': min(scores),\n",
    "            'max_score': max(scores),\n",
    "            'all_scores': scores            \n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "def evaluate_accuracy_score(vectordb, metadata_queries, k=3):\n",
    "    \"\"\"\n",
    "    評估metadata查詢的準確率\n",
    "    \n",
    "    Args:\n",
    "        vectordb: 向量資料庫\n",
    "        metadata_queries: {\n",
    "            'year_2023': 'What papers were published in 2023?',\n",
    "            'transformer_topic': 'transformer architecture papers',\n",
    "            'author_vaswani': 'papers by Vaswani'\n",
    "        }\n",
    "        k: 檢索數量\n",
    "    \n",
    "    Returns:\n",
    "        dict: 各種metadata查詢的準確率\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # 年份查詢準確率\n",
    "    if 'year_2023' in metadata_queries:\n",
    "        query = metadata_queries['year_2023']\n",
    "        docs = vectordb.similarity_search(query, k=k)\n",
    "        correct_count = sum(1 for doc in docs if doc.metadata.get('year') == 2023)\n",
    "        results['year_accuracy'] = correct_count / len(docs) if docs else 0\n",
    "    \n",
    "    # 主題查詢準確率\n",
    "    if 'transformer_topic' in metadata_queries:\n",
    "        query = metadata_queries['transformer_topic']\n",
    "        docs = vectordb.similarity_search(query, k=k)\n",
    "        correct_count = sum(1 for doc in docs if 'transformer' in doc.metadata.get('topic', '').lower())\n",
    "        results['topic_accuracy'] = correct_count / len(docs) if docs else 0\n",
    "    \n",
    "    # 作者查詢準確率\n",
    "    if 'author_vaswani' in metadata_queries:\n",
    "        query = metadata_queries['author_vaswani']\n",
    "        docs = vectordb.similarity_search(query, k=k)\n",
    "        correct_count = sum(1 for doc in docs if 'vaswani' in doc.metadata.get('authors', '').lower())\n",
    "        results['author_accuracy'] = correct_count / len(docs) if docs else 0\n",
    "    \n",
    "    return results\n",
    "\n",
    "def evaluate_diversity_score(vectordb, query, k=3):\n",
    "    \"\"\"\n",
    "    評估檢索結果的多樣性\n",
    "    \n",
    "    Args:\n",
    "        vectordb: 向量資料庫\n",
    "        query: 查詢字串\n",
    "        k: 檢索數量\n",
    "    \n",
    "    Returns:\n",
    "        dict: 各種多樣性指標\n",
    "    \"\"\"\n",
    "    docs = vectordb.similarity_search(query, k=k)\n",
    "    \n",
    "    # 1. 論文來源多樣性\n",
    "    titles = [doc.metadata.get('title', 'Unknown') for doc in docs]\n",
    "    unique_titles = len(set(titles))\n",
    "    title_diversity = unique_titles / len(titles)\n",
    "    \n",
    "    # 2. 年份多樣性\n",
    "    years = [doc.metadata.get('year', 'Unknown') for doc in docs]\n",
    "    unique_years = len(set(years))\n",
    "    year_diversity = unique_years / len(years)\n",
    "    \n",
    "    # 3. 主題多樣性\n",
    "    topics = [doc.metadata.get('topic', 'Unknown') for doc in docs]\n",
    "    unique_topics = len(set(topics))\n",
    "    topic_diversity = unique_topics / len(topics)\n",
    "    \n",
    "    return {\n",
    "        'title_diversity': title_diversity,\n",
    "        'year_diversity': year_diversity,\n",
    "        'topic_diversity': topic_diversity,\n",
    "        'overall_diversity': (title_diversity + year_diversity + topic_diversity) / 3\n",
    "    }\n",
    "\n",
    "\n",
    "def running_baseline(vectordb):\n",
    "    print(\"starting baseline evalution\")\n",
    "\n",
    "    # 1. preparing testing data\n",
    "    relevance_queries = {\n",
    "        'high_relevance': [\n",
    "            'transformer architecture',\n",
    "            'attention mechanism',\n",
    "            'what is BERT'\n",
    "        ],\n",
    "        'low_relevence': [\n",
    "            'how to cook a pasta',\n",
    "            'quantum computing',\n",
    "            'weather forecast'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    metadata_queries = { # structure - key:actual question\n",
    "        'year_2023': 'what papers were published in 2023?',\n",
    "        'llm_limitation_topic': 'llm limitation papers',\n",
    "        'author_vaswani': 'papers by vaswani'\n",
    "    }\n",
    "\n",
    "\n",
    "    speed_queries = ['transformer', 'attention', 'LLM limitations']\n",
    "\n",
    "    # 2. running defs\n",
    "    relevance_results =  evaluate_relevance_score(vectordb, relevance_queries) # input: different questions\n",
    "    diversity_results =  evaluate_diversity_score(vectordb, 'LLM research') # input: target topic\n",
    "    accuracy_results = evaluate_accuracy_score(vectordb, metadata_queries) # input: question of metadata\n",
    "    time_results = evaluate_time_score(vectordb, speed_queries) # input:any questions\n",
    "\n",
    "    # 3. print results\n",
    "        \n",
    "    # about relavance \n",
    "\n",
    "    print(\"\\n📊 相關度分數分佈:\")\n",
    "    for category, stats in relevance_results.items():\n",
    "        print(f\"  {category}: 平均={stats['avg_score']:.3f}, 範圍=[{stats['min_score']:.3f}, {stats['max_score']:.3f}]\")    \n",
    "\n",
    "    # about diversity\n",
    "    print(f\"\\n🎯 多樣性分數: {diversity_results['overall_diversity']:.3f}\")\n",
    "    \n",
    "    # about accuracy\n",
    "    print(\"\\n📋 Metadata準確率:\")\n",
    "    for metric, accuracy in accuracy_results.items():\n",
    "        print(f\"  {metric}: {accuracy:.1%}\")    \n",
    "\n",
    "\n",
    "    # about response times\n",
    "    print(f\"\\n average response time: {time_results['avg_response_time']:.3f}seconds\"\n",
    "    f\"\\n min response time: {time_results['min_response_time']:.3f}seconds\"\n",
    "    f\"\\n max response time: {time_results['max_response_time']:.3f}seconds\")\n",
    "    \n",
    "\n",
    "\n",
    "    return { \n",
    "        'relevance': relevance_results,\n",
    "        'diversity': diversity_results,\n",
    "        'metadata': accuracy_results,\n",
    "        'speed': time_results\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting baseline evalution\n",
      "\n",
      "📊 相關度分數分佈:\n",
      "  high_relevance: 平均=1.308, 範圍=[1.183, 1.433]\n",
      "  low_relevence: 平均=1.529, 範圍=[1.284, 1.671]\n",
      "\n",
      "🎯 多樣性分數: 1.000\n",
      "\n",
      "📋 Metadata準確率:\n",
      "  year_accuracy: 33.3%\n",
      "  author_accuracy: 0.0%\n",
      "\n",
      " average response time: 0.455seconds\n",
      " min response time: 0.420seconds\n",
      " max response time: 0.480seconds\n"
     ]
    }
   ],
   "source": [
    "results = running_baseline(vectordb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### baseline results\n",
    "- 建立系統性能基準:回應時間：0.45秒，驗證檢索功能正常\n",
    "\n",
    "- 高相關 < 低相關的距離證明系統work\n",
    "\n",
    "- 問題點：\n",
    "    - 多樣性 1.0 = 完全沒多樣性\n",
    "    - Metadata 準確率慘烈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " average response time: 0.504seconds\n",
      " min response time: 0.468seconds\n",
      " max response time: 0.573seconds\n"
     ]
    }
   ],
   "source": [
    "def evaluate_time_score_mmr(vectordb, test_queries, k=3, iterations=5): # （向量資料庫、測試查詢列表、檢索數量、每隔查詢重複測試次數）\n",
    "\n",
    "    import time \n",
    "\n",
    "    all_times = [] # 三個問題的總時間\n",
    "\n",
    "    for query in test_queries: # 共有三個問題、迭代五次、每次產出3塊\n",
    "        query_times = [] # total times\n",
    "        for _ in range(iterations):  # iterations 剛剛已經預設 5\n",
    "            start_time = time.time() # start timing\n",
    "            vectordb.max_marginal_relevance_search(query, k=k) # k 預設為3, query = each questions\n",
    "            end_time = time.time() # end timing\n",
    "            query_times.append(end_time - start_time) # 紀錄某問題迭代五次的時間\n",
    "        avg_time = sum(query_times) / len(query_times)\n",
    "        all_times.append(avg_time)\n",
    "\n",
    "    return  {\n",
    "        'avg_response_time': sum(all_times) / len(all_times),\n",
    "        'min_response_time': min(all_times),\n",
    "        'max_response_time': max(all_times)\n",
    "    }\n",
    "# about response times\n",
    "\n",
    "\n",
    "speed_queries = ['transformer', 'attention', 'LLM limitations']\n",
    "\n",
    "time_results = evaluate_time_score_mmr(vectordb, speed_queries)\n",
    "\n",
    "print(f\"\\n average response time: {time_results['avg_response_time']:.3f}seconds\"\n",
    "    f\"\\n min response time: {time_results['min_response_time']:.3f}seconds\"\n",
    "    f\"\\n max response time: {time_results['max_response_time']:.3f}seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity 結果論文:\n",
      "- Attention Is All You Need\n",
      "- Lost in the Middle- How Language Models Use Long Contexts\n",
      "- Attention Is All You Need\n",
      "- Scaling Laws for Neural Language Models\n",
      "- Attention Is All You Need\n",
      "\n",
      "MMR 結果論文:\n",
      "- Attention Is All You Need\n",
      "- Lost in the Middle- How Language Models Use Long Contexts\n",
      "- Modular RAG- Transforming RAG Systems into LEGO-like Reconfigurable Frameworks\n",
      "- Large Language Model Agent- A Survey on Methodology, Applications and Challenges\n",
      "- Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or- How I learned to start worrying about prompt formatting\n"
     ]
    }
   ],
   "source": [
    "def diversity_showdown():\n",
    "    query = \"transformer architecture\"\n",
    "    \n",
    "    sim_docs = vectordb.similarity_search(query, k=5)\n",
    "    mmr_docs = vectordb.max_marginal_relevance_search(query, k=5)\n",
    "    \n",
    "    print(\"Similarity 結果論文:\")\n",
    "    for doc in sim_docs:\n",
    "        print(f\"- {doc.metadata['title']}\")\n",
    "    \n",
    "    print(\"\\nMMR 結果論文:\")  \n",
    "    for doc in mmr_docs:\n",
    "        print(f\"- {doc.metadata['title']}\")\n",
    "    \n",
    "diversity_result = diversity_showdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比較總表：\n",
    "    時間效率：\n",
    "        mmr:\n",
    "            average response time: 0.504seconds\n",
    "            min response time: 0.468seconds\n",
    "            max response time: 0.573seconds\n",
    "        Similarity：\n",
    "            average response time: 0.455seconds\n",
    "            min response time: 0.420seconds\n",
    "            max response time: 0.480seconds\n",
    "    \n",
    "    多樣性：\n",
    "        Similarity :\n",
    "            - Attention Is All You Need\n",
    "            - Lost in the Middle- How Language Models Use Long Contexts\n",
    "            - Attention Is All You Need\n",
    "            - Scaling Laws for Neural Language Models\n",
    "            - Attention Is All You Need\n",
    "        MMR 結果論文:\n",
    "            - Attention Is All You Need\n",
    "            - Lost in the Middle- How Language Models Use Long Contexts\n",
    "            - Modular RAG- Transforming RAG Systems into LEGO-like Reconfigurable Frameworks\n",
    "            - Large Language Model Agent- A Survey on Methodology, Applications and Challenges\n",
    "            - Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or- How I learned to start worrying about prompt formatting    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mmr_metadata_precision():\n",
    "    \"\"\"\n",
    "    測試MMR在特定metadata查詢上是否比similarity更精準\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 年份查詢\n",
    "    year_query = \"What papers were published in 2023?\"\n",
    "    sim_2023 = vectordb.similarity_search(year_query, k=5)\n",
    "    mmr_2023 = vectordb.max_marginal_relevance_search(year_query, k=5)\n",
    "    \n",
    "    print(\"2023年論文查詢:\")\n",
    "    print(\"Similarity結果:\")\n",
    "    for doc in sim_2023:\n",
    "        year = doc.metadata.get('year', 'Unknown')\n",
    "        print(f\"  {year}: {doc.metadata.get('title', 'Unknown')}\")\n",
    "    \n",
    "    print(\"MMR結果:\")  \n",
    "    for doc in mmr_2023:\n",
    "        year = doc.metadata.get('year', 'Unknown')\n",
    "        print(f\"  {year}: {doc.metadata.get('title', 'Unknown')}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023年論文查詢:\n",
      "Similarity結果:\n",
      "  2023: BloombergGPT- A Large Language Model for Finance\n",
      "  2024: Modular RAG- Transforming RAG Systems into LEGO-like Reconfigurable Frameworks\n",
      "  2024: The Power of Noise- Redefining Retrieval for RAG Systems\n",
      "  2023: Challenges and Applications of Large Language Models\n",
      "  2025: The Order Effect- Investigating Prompt Sensitivity to Input Order in LLMs\n",
      "MMR結果:\n",
      "  2023: BloombergGPT- A Large Language Model for Finance\n",
      "  2023: Challenges and Applications of Large Language Models\n",
      "  2024: Language Ranker- A Metric for Quantifying LLM Performance Across High and Low-Resource Languages\n",
      "  2025: Large Language Model Agent- A Survey on Methodology, Applications and Challenges\n",
      "  2024: Retrieval-Augmented Generation for Large Language Models- A Survey\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = test_mmr_metadata_precision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search_with_year(query, target_year, k=5):\n",
    "    # 先用向量搜尋找到候選結果（更大的k）\n",
    "\n",
    "    candidates = vectordb.max_marginal_relevance_search(query, k=k*3)  # 找15個候選\n",
    "    \n",
    "    # 再用metadata篩選年份\n",
    "    year_filtered = [doc for doc in candidates \n",
    "                    if doc.metadata.get('year') == target_year]\n",
    "    \n",
    "    # 回傳前k個\n",
    "    return year_filtered[:k]\n",
    "\n",
    "query = \"What papers were published in 2023?\" \n",
    "year = 2023\n",
    "test_accurac_hybird = hybrid_search_with_year(query, year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2023: BloombergGPT- A Large Language Model for Finance\n",
      "  2023: Challenges and Applications of Large Language Models\n",
      "  2023: Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions\n",
      "  2023: Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or- How I learned to start worrying about prompt formatting\n",
      "  2023: BloombergGPT- A Large Language Model for Finance\n"
     ]
    }
   ],
   "source": [
    "for doc in test_accurac_hybird:\n",
    "    year = doc.metadata.get('year', 'Unknown')\n",
    "    print(f\"  {year}: {doc.metadata.get('title', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看起來用混合檢索的方式，可以真的好好指定我要的年份。\n",
    "但我有以下問題想和你討論，你先不要給我程式碼，跟我用文字討論：\n",
    "一、我的metadata中，存在以下欄位，這些都可以變成metadata篩選的條件嗎？例如title, year, author, topic等？\n",
    "二、如果可以都變成篩選條件，如何將這些檢索條件加到「檢索」的程式中？因為我們再檢索階段可以自己打year=2023之類的參數，可是整體的rag我們應該只會打一個問題，這樣有辦法幫我們分辨嗎？\n",
    "三、我還是很想解決不能精準檢索的問題，所以正在試hybrid的方式，但我仔細一想你剛剛跟我說這個rag的定位是什麼，我就在想我用了mmr提升了「多樣性」，用hybird可以提升「準確率」，哪一個對我來說可以檢索論文比較重要？我是站在「學習llm知識」的角度去開發這個rag，所以才會清理乾淨metadata前面提到最重要的四大參數，所以都很重要？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 建立hybrid rag(查詢解析器) 目標： 把自然語言查詢 → 轉換為 structured parameters\n",
    "- 建立llm解析器\n",
    "- 查詢使用mmr+解析器：規則判斷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'year': 2024, 'topic': 'RAG', 'source': '/Users/mangtinglee/Desktop/2025_gap_careerpath/RAG_LLM/pdfs/2024_RAG_Evaluation of Retrieval-Augmented Generation- A Survey.pdf', 'total_pages': 21, 'title': 'Evaluation of Retrieval-Augmented Generation- A Survey', 'page': 0, 'authors': 'Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, Zhaofeng Liu'}\n",
      "{'authors': 'Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang', 'title': 'Retrieval-Augmented Generation for Large Language Models- A Survey', 'total_pages': 21, 'page': 0, 'year': 2024, 'topic': 'RAG', 'source': '/Users/mangtinglee/Desktop/2025_gap_careerpath/RAG_LLM/pdfs/2024_RAG_Retrieval-Augmented Generation for Large Language Models- A Survey.pdf'}\n",
      "{'title': 'Evaluation of Retrieval-Augmented Generation- A Survey', 'total_pages': 21, 'source': '/Users/mangtinglee/Desktop/2025_gap_careerpath/RAG_LLM/pdfs/2024_RAG_Evaluation of Retrieval-Augmented Generation- A Survey.pdf', 'page': 0, 'authors': 'Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, Zhaofeng Liu', 'topic': 'RAG', 'year': 2024}\n",
      "{'authors': 'Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang', 'year': 2024, 'source': '/Users/mangtinglee/Desktop/2025_gap_careerpath/RAG_LLM/pdfs/2024_RAG_Retrieval-Augmented Generation for Large Language Models- A Survey.pdf', 'page': 0, 'topic': 'RAG', 'title': 'Retrieval-Augmented Generation for Large Language Models- A Survey', 'total_pages': 21}\n",
      "{'total_pages': 21, 'page': 0, 'source': '/Users/mangtinglee/Desktop/2025_gap_careerpath/RAG_LLM/pdfs/2024_RAG_Retrieval-Augmented Generation for Large Language Models- A Survey.pdf', 'title': 'Retrieval-Augmented Generation for Large Language Models- A Survey', 'year': 2024, 'topic': 'RAG', 'authors': 'Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang'}\n"
     ]
    }
   ],
   "source": [
    "# llm套件與啟動\n",
    "\n",
    "from langchain_openai import ChatOpenAI   \n",
    "\n",
    "# 建立 LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")   # 指定模型\n",
    "\n",
    "\n",
    "# llm解析器：解析問題內部的參數\n",
    "def llm_extract_metadata(query):\n",
    "    prompt = f\"\"\"\n",
    "you are a metedata extraction export, extract year and topic information from user queries.\n",
    "\n",
    "Available topics:\n",
    "    - \"transformer\"\n",
    "    - \"scaling laws\"\n",
    "    - \"fine tuning\"\n",
    "    - \"LLM limitation\"\n",
    "    - \"LLM application\"\n",
    "    - \"prompt engineering\"\n",
    "    - \"RAG\"\n",
    "    - \"ai-agent\"\n",
    "    - null (if no specific topic)\n",
    "\n",
    "    Available years: 2017-2025 or null\n",
    "\n",
    "    Query = \"{query}\"\n",
    "\n",
    "    Extract and return ONLY a JSON object:\n",
    "    {{\"year\":<number or null>, \"topic\":\"<topic or null>\"}}\n",
    "\n",
    "    Examples:\n",
    "    Query:\"2023年關於transformer的研究\"\n",
    "    {{\"year\":2023, \"topic\":\"transformer\"}}\n",
    "\n",
    "    Query:\"LLM的限制有哪些\"\n",
    "    {{\"year\":null, \"topic\":\"llm limitation\"}}\n",
    "\n",
    "    Query: \"最新的RAG論文\"\n",
    "    {{\"year\": 2025, \"topic\": \"RAG\"}}\n",
    "\n",
    "    Now extract from the query above:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = llm.invoke(prompt) # response 的資料型態是AIMessage，不是字串\n",
    "        response_text = response.content  # response.content 才能取得文字內容\n",
    "        # 解析json格式\n",
    "        import json\n",
    "        metadata = json.loads(response_text.strip())\n",
    "        return metadata\n",
    "    except:\n",
    "        return {\"year\":None, \"topic\":None }\n",
    "\n",
    "def smart_hybrid_search(query, k=5):\n",
    "    # llm解析返回的查詢\n",
    "    metadata = llm_extract_metadata(query)\n",
    "    year = metadata.get('year')\n",
    "    topic = metadata.get('topic')\n",
    "\n",
    "    # 根據解析策略選擇檢索方式\n",
    "    if year and topic:\n",
    "        candidates = vectordb.max_marginal_relevance_search(query, k=k*4)\n",
    "        filtered = [doc for doc in candidates\n",
    "                    if doc.metadata.get('year') == year\n",
    "                    and doc.metadata.get('topic') == topic]\n",
    "    elif year:\n",
    "        candidates = vectordb.max_marginal_relevance_search(query, k=k*3)\n",
    "        filtered = [doc for doc in candidates\n",
    "                    if doc.metadata.get('year') == year]\n",
    "\n",
    "    elif topic:\n",
    "        candidates = vectordb.max_marginal_relevance_search(query, k=k*3)\n",
    "        filtered = [doc for doc in candidates\n",
    "                    if doc.metadata.get('topic') == topic]\n",
    "\n",
    "    else:\n",
    "        filtered = vectordb.max_marginal_relevance_search(query, k=k)\n",
    "\n",
    "    # 確保有足夠結果\n",
    "    if len(filtered) < k:\n",
    "        additional = vectordb.max_marginal_relevance_search(query, k=k)\n",
    "        filtered.extend = ([doc for doc in additional if doc not in filtered])\n",
    "    \n",
    "    return filtered[:k]\n",
    "\n",
    "\n",
    "test_queries =\"2024年關於RAG的研究\"\n",
    "test_hybid = smart_hybrid_search(test_queries)\n",
    "for doc in test_hybid:\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要將剛剛的hybrid retrival包裝成自定義retriver物件，才可以接軌\n",
    "from langchain.schema import BaseRetriever, Document\n",
    "from typing import List, Any\n",
    "from langchain_openai import ChatOpenAI\n",
    "import json\n",
    "from pydantic import Field\n",
    "\n",
    "class SmartHybridRetriever(BaseRetriever):  # 修正拼字錯誤\n",
    "\n",
    "    # 明確定義fields\n",
    "    vectordb: Any = Field(default=None)\n",
    "    llm: Any = Field(default=None)\n",
    "    \n",
    "    # def __init__(self, vectordb, llm, **kwargs):\n",
    "    #     # 先傳給父類\n",
    "    #     super().__init__(**kwargs)\n",
    "    #     # 然後設定屬性\n",
    "    #     object.__setattr__(self, 'vectordb', vectordb)\n",
    "    #     object.__setattr__(self, 'llm', llm)\n",
    "    \n",
    "    \n",
    "    def __init__(self, vectordb, llm):\n",
    "        super().__init__()  # 重要：調用父類初始化\n",
    "        self.vectordb = vectordb\n",
    "        self.llm = llm\n",
    "    \n",
    "    def llm_extract_metadata(self, query):  # 加入self參數\n",
    "        prompt = f\"\"\"\n",
    "you are a metadata extraction expert, extract year and topic information from user queries.\n",
    "\n",
    "Available topics:\n",
    "    - \"transformer\"\n",
    "    - \"scaling laws\"\n",
    "    - \"fine tuning\"\n",
    "    - \"LLM limitation\"\n",
    "    - \"LLM application\"\n",
    "    - \"prompt engineering\"\n",
    "    - \"RAG\"\n",
    "    - \"LLM agent\"  # 修正為\"LLM agent\"\n",
    "    - null (if no specific topic)\n",
    "\n",
    "Available years: 2017-2025 or null\n",
    "\n",
    "Query = \"{query}\"\n",
    "\n",
    "Extract and return ONLY a JSON object:\n",
    "{{\"year\":<number or null>, \"topic\":\"<topic or null>\"}}\n",
    "\n",
    "Examples:\n",
    "Query:\"2023年關於transformer的研究\"\n",
    "{{\"year\":2023, \"topic\":\"transformer\"}}\n",
    "\n",
    "Query:\"LLM的限制有哪些\"\n",
    "{{\"year\":null, \"topic\":\"LLM limitation\"}}\n",
    "\n",
    "Query: \"最新的RAG論文\"\n",
    "{{\"year\": 2025, \"topic\": \"RAG\"}}\n",
    "\n",
    "Now extract from the query above:\n",
    "\"\"\"\n",
    "        try:\n",
    "            response = self.llm.invoke(prompt)  # 使用self.llm\n",
    "            response_text = response.content\n",
    "            metadata = json.loads(response_text.strip())\n",
    "            return metadata\n",
    "        except:\n",
    "            return {\"year\": None, \"topic\": None}\n",
    "\n",
    "    def smart_hybrid_search(self, query, k=5):  # 加入self參數\n",
    "        metadata = self.llm_extract_metadata(query)  # 使用self\n",
    "        year = metadata.get('year')\n",
    "        topic = metadata.get('topic')\n",
    "\n",
    "        if year and topic:\n",
    "            candidates = self.vectordb.max_marginal_relevance_search(query, k=k*4)\n",
    "            filtered = [doc for doc in candidates\n",
    "                       if doc.metadata.get('year') == year\n",
    "                       and doc.metadata.get('topic') == topic]\n",
    "        elif year:\n",
    "            candidates = self.vectordb.max_marginal_relevance_search(query, k=k*3)\n",
    "            filtered = [doc for doc in candidates\n",
    "                       if doc.metadata.get('year') == year]\n",
    "        elif topic:\n",
    "            candidates = self.vectordb.max_marginal_relevance_search(query, k=k*3)\n",
    "            filtered = [doc for doc in candidates\n",
    "                       if doc.metadata.get('topic') == topic]\n",
    "        else:\n",
    "            filtered = self.vectordb.max_marginal_relevance_search(query, k=k)\n",
    "\n",
    "        # 修正extend的使用\n",
    "        if len(filtered) < k:\n",
    "            additional = self.vectordb.max_marginal_relevance_search(query, k=k)\n",
    "            filtered.extend([doc for doc in additional if doc not in filtered])  # 修正\n",
    "        \n",
    "        return filtered[:k]\n",
    "    \n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        \"\"\"BaseRetriever要求的抽象方法\"\"\"\n",
    "        return self.smart_hybrid_search(query, k=5)\n",
    "    \n",
    "    async def _aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        \"\"\"異步版本\"\"\"\n",
    "        return self._get_relevant_documents(query)\n",
    "\n",
    "# 使用方式\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")  # 在外部定義\n",
    "custom_retriever = SmartHybridRetriever(vectordb=vectordb, llm=llm)  # 修正變數名\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA,  ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "#加入llm回答\n",
    "# 新項目：載入ConversationBufferMemory\n",
    "# 此套件能讓問答機器人記住過往歷史問答\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\", # 告訴 chain 去哪裡找歷史對話\n",
    "    return_messages=True, # 返回的是物件（長得像json or meta data）\n",
    "    output_key=\"answer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立自訂義的prompt(為了好好利用檢索)\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# 建立自定義prompt\n",
    "qa_prompt = PromptTemplate(\n",
    "    template=\"\"\"請基於以下檢索到的文獻內容回答問題。\n",
    "\n",
    "檢索到的文獻：\n",
    "{context}\n",
    "\n",
    "問題：{question}\n",
    "\n",
    "請直接基於上述文獻內容回答問題，列出相關的論文標題和主要內容。如果文獻中沒有相關資訊，請明確說明。\n",
    "\n",
    "答案：\"\"\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# 重新建立qa chain，使用自定義prompt\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0),\n",
    "    chain_type=\"stuff\",  # 改為stuff更直接\n",
    "    retriever=custom_retriever,\n",
    "    return_source_documents=True,\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={\"prompt\": qa_prompt}  # 加入自定義prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer架構的核心概念是基於注意力機制，並且完全不使用循環神經網絡或卷積神經網絡。這種簡單的網絡架構通過注意力機制將編碼器和解碼器相連接，並且在機器翻譯任務中表現優異，同時具有更好的並行性和更短的訓練時間。Transformer模型在WMT 2014英德翻譯任務上取得了28.4 BLEU的分數，在英法翻譯任務上取得了41.8 BLEU的分數，並且在其他任務上也表現良好。Transformer的提出是基於將循環神經網絡替換為自注意力機制，並且消除了循環和卷積的使用。\n"
     ]
    }
   ],
   "source": [
    "question = \"transformer架構的核心概念是什麼？\"\n",
    "result = qa({\"question\": question})\n",
    "print(result['answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根據檢索到的文獻內容，這個架構的限制包括：\n",
      "1. LLMs對於提示中元素的順序敏感，包括few-shot demonstrations的排列和候選模型生成的回應順序，可能影響其理解和推理能力。\n",
      "   - 相關論文：Zhao et al., 2021; Wang et al., 2023b\n",
      "2. LLMs對於提示中的微小變化敏感，例如在多選題回答任務中，選項的順序可能影響其表現。\n",
      "   - 相關論文：未提及\n",
      "3. LLMs在低資源語言中表現不佳，可能由於缺乏訓練數據導致無法理解文化特定表達或成語。\n",
      "   - 相關論文：Zhang et al. 2023; Lankford, Afli, and Way 2024\n",
      "4. Transformer語言模型需要大量記憶體和計算資源，對序列長度的要求增長迅速，可能限制了模型的訓練上下文窗口大小。\n",
      "   - 相關論文：Vaswani et al., 2017; Dai et al., 2019; Dao et al., 2022; Poli et al., 2023\n",
      "\n",
      "綜合以上資訊，這個架構的限制包括對提示中元素順序和微小變化的敏感性，以及在低資源語言和大序列長度下的表現不佳。\n"
     ]
    }
   ],
   "source": [
    "question = \"這個架構有什麼限制？\"\n",
    "result = qa({\"question\": question})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "question = \"2023年有哪些重要的論文？\"\n",
    "回答：根據檢索到的文獻內容，2023年的重要論文包括：\n",
    "\n",
    "1. \"Lost in the Middle: How Language Models Use Long Contexts\"，作者包括Nelson F. Liu、Kevin Lin、John Hewitt等人，主要探討語言模型在處理長文本上的表現，並分析了在多文檔問答和鍵-值檢索任務中的表現。\n",
    "\n",
    "2. \"Challenges: What problems remain unresolved?\"，作者未提及，主要探討語言模型研究中尚未解決的問題。\n",
    "\n",
    "3. \"Applications: Where are LLMs currently being applied, and how are the challenges constraining them?\"，作者未提及，主要探討目前語言模型的應用領域以及面臨的挑戰。\n",
    "\n",
    "以上是根據文獻內容能夠找到的2023年重要論文，其他文獻中未提及相關資訊。（但如果用數2023的論文，其實有六篇，而且每一篇都有提及作者）\n",
    "\n",
    "question = \"告訴我關於LLM limitation的研究，並展示出文章的名稱？\"\n",
    "回答：\n",
    "在LLM limitation的研究中，重要的發現包括：\n",
    "1. 文獻中提到LLMs對於prompt中各個元素的排列敏感，這直接影響了它們在特定任務中理解和推理能力的評估。\n",
    "2. 先前的研究表明，LLMs對於few-shot demonstrations的排列和候選模型生成的回應順序敏感，這影響了LLMs作為評估質量的裁判時的表現。\n",
    "3. LLMs對於prompt中元素的順序在不同任務中是否敏感，這也是一個需要探討的問題。\n",
    "評比：看起來因為我們的解析器只針對年份和主題，對作者的metadata好像沒這麼敏感（但其實都有）\n",
    "\n",
    "\n",
    "question = \"2024年關於RAG的研究有什麼進展？\"\n",
    "根據檢索到的文獻，2024年關於RAG的研究有以下進展：\n",
    "1. 文獻標題：Evaluation of Retrieval-Augmented Generation: A Survey\n",
    "   主要內容：該研究對RAG方法進行了全面和系統性的回顧，描述了其透過外部信息檢索增強生成模型的能力。研究評估了RAG系統的獨特挑戰，並提出了統一的評估過程，旨在提供對RAG系統的評估和基準的全面概述。\n",
    "\n",
    "2. 文獻標題：未提及RAG研究進展\n",
    "   主要內容：未提及2024年關於RAG的具體研究進展。\n",
    "\n",
    "評比：但其實rag２０２４的pdf有五篇。\n",
    "question = \"什麼是attention機制？\"\n",
    "回答：根據檢索到的文獻內容，提到了一個新的簡單網絡架構，Transformer，它完全基於attention機制，不使用循環和卷積。這個網絡架構將編碼器和解碼器通過attention機制連接起來。因此，根據文獻內容，attention機制是一種連接編碼器和解碼器的機制，用於序列轉換模型中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "檢索到的文件數量: 5\n",
      "文件1: BloombergGPT- A Large Language Model for Finance\n",
      "年份: 2023\n",
      "內容預覽: . . . . . . . . . . . . . . 9\n",
      "∗. Co-first authors. Corresponding email: airesearch@bloomberg.net\n",
      "1\n",
      "a\n",
      "--------------------------------------------------\n",
      "文件2: Challenges and Applications of Large Language Models\n",
      "年份: 2023\n",
      "內容預覽: . . . . 43\n",
      "3.8 Reasoning . . . . . . . . . . . . . 44\n",
      "3.9 Robotics and Embodied Agents . . 45\n",
      "3.10 S\n",
      "--------------------------------------------------\n",
      "文件3: Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions\n",
      "年份: 2023\n",
      "內容預覽: . For example, how much does\n",
      "the order of options in multiple-choice question\n",
      "(MCQ) answering tasks \n",
      "--------------------------------------------------\n",
      "文件4: Lost in the Middle- How Language Models Use Long Contexts\n",
      "年份: 2023\n",
      "內容預覽: Lost in the Middle: How Language Models Use Long Contexts\n",
      "Nelson F. Liu1∗ Kevin Lin2 John Hewitt1 As\n",
      "--------------------------------------------------\n",
      "文件5: Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions\n",
      "年份: 2023\n",
      "內容預覽: window” (the example is from CSQA dataset).\n",
      "et al., 2022; Touvron et al., 2023; OpenAI, 2023).\n",
      "Howev\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 先單獨測試你的retriever\n",
    "question = \"2023年有哪些重要的論文？\"\n",
    "docs = custom_retriever._get_relevant_documents(question)\n",
    "print(f\"檢索到的文件數量: {len(docs)}\")\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"文件{i+1}: {doc.metadata.get('title', 'Unknown')}\")\n",
    "    print(f\"年份: {doc.metadata.get('year')}\")\n",
    "    print(f\"內容預覽: {doc.page_content[:100]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**目前的理解**\n",
    "\n",
    "首先在之前檢索階段還沒有加入回答前，就已經把`def llm_extract_metadata()`和`def smart_hybrid_search()`建立好了。\n",
    "\n",
    "`smart_hybrid_search`會先透過`llm_extract_metadata`解析器回傳年份和主題。\n",
    "\n",
    "所以當時的執行方法會是：\n",
    "\n",
    "```python\n",
    "    test_queries =\"2024年關於RAG的研究\"\n",
    "    test_hybid = smart_hybrid_search(test_queries)\n",
    "```\n",
    "\n",
    "但因為現在加上回答，`ConversationalRetrievalChain`需要`retriever`物件（`retriever=retriever`），但因為剛剛做的是函式並非這種物件，因此需要做一些調整。\n",
    "\n",
    "而調整方法是需要將剛剛的混合檢索「包裝成自定義retriver物件」，才可以寫出`retriever=custom_retriever`。\n",
    "以上是我能理解的地方，但實際上的做法不太懂。\n",
    "\n",
    "\n",
    "**不太懂的地方**\n",
    "\n",
    "- 一、看起來langchain.schema可以讓我們自己建立一個retriever物件，但我不太懂這個套件是什麼東西，也不懂為什麼我們需要一個class，因為我目前只熟悉def\n",
    "    - 回答\n",
    "        - def像是一個「動作」，按一下執行一次；class像是一個藍圖，裡面會有「資料」也會有「動作」\n",
    "        -   例如像class SmartHybridRetriever()就包含了：\n",
    "            - 資料：vectordb、llm\n",
    "            - 動作：llm_extract_metadata()、smart_hybrid_search()\n",
    "        - 為什麼要包成class?因為ConversationalRetrievalChain只認得物件，不能直接吃一個函式，所以我們需要一個class物件，把兩個函式包起來。\n",
    "\n",
    "\n",
    "二、更進一步，from typing import List, Any和from pydantic import Field更是完全沒碰過了，所以不懂vectordb: Any = Field(default=None)\n",
    "    llm: Any = Field(default=None)是什麼、＿＿init__裡面的super(), object也全部不懂。\n",
    "\n",
    "三、然後看起來def llm_extract_metadata()和def smart_hybrid_search()都增加了self的參數，是什麼意思？\n",
    "\n",
    "四、def _get_relevant_documents(self, query: str) -> List[Document]:這又是什麼？什麼叫做要求抽象的方法？\n",
    "        \"\"\"BaseRetriever要求的抽象方法\"\"\"\n",
    "\n",
    "五、然後我從來也沒看過 async def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一、因為class就像是一張藍圖一樣，他同時包含了所需的「資料（屬性）」和「行為（函式）」，如果是def，我們能每一個執行def都比噓拖著一大堆的函數且他需要按照順序來執行，因此程式會顯得很混亂，而且假設今天有大學生和學生兩種對象，其實他們可以有個父類叫做「學生」，接下來在他們各自的class裡繼承學生的基本輪廓，再加入特定行為知識就可以了，如果是def可以能就沒法這樣繼承和共用，資料會不段的傳來傳去。\n",
    "二、因為在我們的class裡，有些需要透過父類來繼承，繼承的話就會寫成super().__init__()，代表接收父類的屬性到自己的屬性中\n",
    "\n",
    "三、因為BaseRetriever他不是一班的class，而是pydantic class，這種規定要先檢查屬性，所以我們才會有field那兩行\n",
    "\n",
    "四、self指的是在class之外呼叫這個類別的「對象」，今天對象就會是retriever\n",
    "\n",
    "五、因為假設r1是給llm論文、R2是給社工論文，如果我們今天要使用r1，只要使用llm = r1.search(\"問題a\")\n",
    "sw = r2.search(\"問題b\")\n",
    "就不會搞混了（請你多說明）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
