{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹æ•´é«”çš„uiä»‹é¢ï¼Œè®Šæˆä¸€å€‹å•ç­”æ©Ÿå™¨äºº\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA,  ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬ä¸€æ­¥æ¸¬è©¦ï¼šè¼‰å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "ç›®çš„ï¼šå»ºç«‹langchain + openai çš„åŸºç¤ç’°å¢ƒ\n",
    "'''\n",
    "import os # ä½œæ¥­ç³»çµ±ç›¸é—œåŠŸèƒ½ï¼ˆè®€å–ç’°å¢ƒè®Šæ•¸ï¼‰\n",
    "from openai import OpenAI # openai api å®¢æˆ¶ç«¯\n",
    "from dotenv import load_dotenv, find_dotenv # dotenv æ˜¯å°ˆé–€ç”¨ä¾†è®€å–.envå¥—ä»¶çš„å¥—ä»¶ï¼Œä¸¦æ¥ä¸Šç’°å¢ƒ\n",
    "_ = load_dotenv(find_dotenv()) # è®€å–.envæª”æ¡ˆ\n",
    "client = OpenAI(\n",
    "    api_key=os.environ['OPENAI_API_KEY']\n",
    ")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Success! API key loaded\n",
      "Key starts with: sk-proj-FWUUter...\n",
      "Key length: 164 characters\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# æ¸…é™¤èˆŠçš„ç’°å¢ƒè®Šæ•¸\n",
    "if 'OPENAI_API_KEY' in os.environ:\n",
    "    del os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# é‡æ–°è¼‰å…¥\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.environ.get('OPENAI_API_KEY')\n",
    "if api_key:\n",
    "    print(\"âœ… Success! API key loaded\")\n",
    "    print(f\"Key starts with: {api_key[:15]}...\")\n",
    "    print(f\"Key length: {len(api_key)} characters\")\n",
    "else:\n",
    "    print(\"âŒ Still not working\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder exists: True\n",
      "Found 24 PDF files:\n",
      "  - 2023_LLM limitation_Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions.pdf\n",
      "  - 2025_LLM limitation_The Order Effect- Investigating Prompt Sensitivity to Input Order in LLMs.pdf\n",
      "  - 2024_RAG_Evaluation of Retrieval-Augmented Generation- A Survey.pdf\n",
      "  - 2020_scaling laws_Scaling Laws for Neural Language Models.pdf\n",
      "  - 2022_LLM limitation_Robustness of Learning from Task Instructions.pdf\n"
     ]
    }
   ],
   "source": [
    "# æª¢æŸ¥è³‡æ–™å¤¾æ˜¯å¦å­˜åœ¨\n",
    "import os\n",
    "folder_path = \"/Users/mangtinglee/Desktop/2025_gap_careerpath/RAG_LLM/pdfs\"\n",
    "\n",
    "print(f\"Folder exists: {os.path.exists(folder_path)}\")\n",
    "# çœ‹çœ‹æœ‰å“ªäº›PDFæª”æ¡ˆ\n",
    "pdf_files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
    "print(f\"Found {len(pdf_files)} PDF files:\")\n",
    "for pdf in pdf_files[:5]:  # é¡¯ç¤ºå‰5å€‹æª”å\n",
    "    print(f\"  - {pdf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2025/09/11 ç›®å‰é€²åº¦\n",
    "- å·²ç¶“å®Œæˆæ‰€æœ‰éç¨‹è·‘éä¸€é\n",
    "- ä¸Šé€±äº”ï¼ˆ9/5ï¼‰ç™¼ç¾å•é¡Œï¼šå•ã€Œçµè«–ã€ï¼Œllmå›ç­”ã€Œæ‰¾ä¸åˆ°çµè«–ã€ï¼Œä½†æ˜æ˜å…¶ä¸­ä¸€ç« å°±æ˜¯çµè«–\n",
    "- åˆ¤å®šå•é¡Œï¼šèªæ„æ±¡æŸ“èˆ‡é›œè¨Šéå¤šï¼Œllmè¢«ä¸åŒç« ç¯€çš„ã€Œçµè«–å¹²æ“¾ã€ï¼Œç„¡æ³•å›æ‡‰çµè«–é‚£ä¸€ç« çš„å…§å®¹ â†’ éœ€è¦å®šç¾©å¥½æ¯ä¸€ç¯‡ç« ç¯€åˆ†é¡\n",
    "- 9/5ï¼ŒåŸæœ¬æ‰“ç®—ç”¨llmåˆ†é¡å†æ”¾åˆ°æ¯ä¸€å¡Šä¸­ï¼Œå¯¦éš›åŸ·è¡Œå›°é›£ï¼Œå› ç‚ºllmæ¨™æ³¨æº–ç¢ºç‡è¼ƒä½\n",
    "- æ”¹ç‚ºæ‰‹å‹•å…§å®¹ç¯©é¸ï¼Œåªä¿ç•™æ¯ç¯‡è«–æ–‡çš„ç¬¬ä¸€é ï¼Œåªæƒ³è¦ Abstract + Introductioné–‹é ­\n",
    "- å¾Œä¾†æ”¹ç‚ºåªæƒ³è¦Abstractï¼Œä¸¦ä¸”æ‰‹å‹•å»ºç«‹ä¸€ä»½ï½ƒï½“ï½– meta data\n",
    "- 9/11 é€²åº¦è¦åŠƒï¼š\n",
    "    - è¼‰å…¥æª”æ¡ˆæ™‚ï¼Œè‡ªå‹•å°æ‡‰metadata\n",
    "    - é‡æ–°è·‘ä¸€éï¼Œä¸¦æ¸¬è©¦æ•ˆæœ\n",
    "    - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'title'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3806\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'title'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 76\u001b[0m\n\u001b[1;32m     73\u001b[0m csv_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/Users/mangtinglee/Desktop/2025_gap_careerpath/RAG_LLM/meta_data_correction.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m \u001b[39m# è¼‰å…¥\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m all_abstracts_with_metadata \u001b[39m=\u001b[39m load_all_first_pages_with_csv_metadata(folder_path, csv_path)\n\u001b[1;32m     78\u001b[0m \u001b[39m# æª¢æŸ¥çµæœ\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39mfor\u001b[39;00m i, doc \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(all_abstracts_with_metadata[:\u001b[39m3\u001b[39m]):\n",
      "Cell \u001b[0;32mIn[16], line 26\u001b[0m, in \u001b[0;36mload_all_first_pages_with_csv_metadata\u001b[0;34m(folder_path, csv_path)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"è¼‰å…¥PDFç¬¬ä¸€é ä¸¦å¾CSVå°æ‡‰metadata\"\"\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39m# 1. å…ˆè®€å–metadataå°æ‡‰è¡¨\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m metadata_map \u001b[39m=\u001b[39m load_metadata_mapping(csv_path)\n\u001b[1;32m     27\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mè¼‰å…¥metadataå°æ‡‰è¡¨ï¼Œå…± \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(metadata_map)\u001b[39m}\u001b[39;00m\u001b[39m ç­†è³‡æ–™\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m all_first_pages \u001b[39m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[16], line 15\u001b[0m, in \u001b[0;36mload_metadata_mapping\u001b[0;34m(csv_path)\u001b[0m\n\u001b[1;32m     12\u001b[0m metadata_map \u001b[39m=\u001b[39m {}\n\u001b[1;32m     13\u001b[0m \u001b[39mfor\u001b[39;00m _, row \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39miterrows():\n\u001b[1;32m     14\u001b[0m     metadata_map[row[\u001b[39m'\u001b[39m\u001b[39mfilename\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m {\n\u001b[0;32m---> 15\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m: row[\u001b[39m'\u001b[39;49m\u001b[39mtitle\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     16\u001b[0m         \u001b[39m'\u001b[39m\u001b[39myear\u001b[39m\u001b[39m'\u001b[39m: row[\u001b[39m'\u001b[39m\u001b[39myear\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     17\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mauthors\u001b[39m\u001b[39m'\u001b[39m: row[\u001b[39m'\u001b[39m\u001b[39mauthors\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     18\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mtopic\u001b[39m\u001b[39m'\u001b[39m: row[\u001b[39m'\u001b[39m\u001b[39mtopic\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     19\u001b[0m     }\n\u001b[1;32m     20\u001b[0m \u001b[39mreturn\u001b[39;00m metadata_map\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[1;32m   1123\u001b[0m \u001b[39m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[39m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[39mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[1;32m   1239\u001b[0m \u001b[39mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(casted_key, \u001b[39mslice\u001b[39m) \u001b[39mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[39misinstance\u001b[39m(casted_key, abc\u001b[39m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(x, \u001b[39mslice\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'title'"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. ç°¡åŒ–ç‰ˆï¼šåªè¦ç¬¬ä¸€é \n",
    "import pandas as pd\n",
    "\n",
    "def load_metadata_mapping(csv_path):\n",
    "    \"\"\"è®€å–CSVæª”æ¡ˆå»ºç«‹metadataå°æ‡‰è¡¨\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # å°‡filenameä½œç‚ºkeyï¼Œå…¶ä»–è³‡è¨Šä½œç‚ºvalue\n",
    "    metadata_map = {}\n",
    "    for _, row in df.iterrows():\n",
    "        metadata_map[row['filename']] = {\n",
    "            'title': row['title'],\n",
    "            'year': row['year'],\n",
    "            'authors': row['authors'],\n",
    "            'topic': row['topic']\n",
    "        }\n",
    "    return metadata_map\n",
    "\n",
    "def load_all_first_pages_with_csv_metadata(folder_path, csv_path):\n",
    "    \"\"\"è¼‰å…¥PDFç¬¬ä¸€é ä¸¦å¾CSVå°æ‡‰metadata\"\"\"\n",
    "    \n",
    "    # 1. å…ˆè®€å–metadataå°æ‡‰è¡¨\n",
    "    metadata_map = load_metadata_mapping(csv_path)\n",
    "    print(f\"è¼‰å…¥metadataå°æ‡‰è¡¨ï¼Œå…± {len(metadata_map)} ç­†è³‡æ–™\")\n",
    "    \n",
    "    all_first_pages = []\n",
    "    pdf_files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"è™•ç†ï¼š{pdf_file}\")\n",
    "        \n",
    "        file_path = os.path.join(folder_path, pdf_file)\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        \n",
    "        # åªå–ç¬¬ä¸€é \n",
    "        first_page_docs = filter_first_page_only(documents)\n",
    "        \n",
    "        # ğŸ¯ é—œéµï¼šå¾CSVå°æ‡‰metadata\n",
    "        for doc in first_page_docs:\n",
    "            if pdf_file in metadata_map:\n",
    "                # æ‰¾åˆ°å°æ‡‰çš„metadata\n",
    "                doc.metadata.update(metadata_map[pdf_file])\n",
    "                print(f\"  âœ… å·²æ›´æ–°metadata: {metadata_map[pdf_file]['title']}\")\n",
    "            else:\n",
    "                # æ‰¾ä¸åˆ°å°æ‡‰è³‡æ–™\n",
    "                print(f\"  âš ï¸  è­¦å‘Šï¼š{pdf_file} åœ¨CSVä¸­æ‰¾ä¸åˆ°å°æ‡‰è³‡æ–™\")\n",
    "                doc.metadata.update({\n",
    "                    'title': pdf_file.replace('.pdf', ''),\n",
    "                    'year': None,\n",
    "                    'authors': 'Unknown',\n",
    "                    'topic': 'Unknown'\n",
    "                })\n",
    "        \n",
    "        all_first_pages.extend(first_page_docs)\n",
    "    \n",
    "    print(f\"ç¸½å…±è¼‰å…¥ {len(all_first_pages)} å€‹ç¬¬ä¸€é ï¼Œmetadataå·²æ›´æ–°\")\n",
    "    return all_first_pages\n",
    "\n",
    "\n",
    "\n",
    "def filter_first_page_only(documents):\n",
    "    \"\"\"åªä¿ç•™ç¬¬ä¸€é ï¼ˆAbstractï¼‰\"\"\"\n",
    "    first_page_docs = [doc for doc in documents if doc.metadata['page'] == 0]\n",
    "    print(f\"åŸå§‹é æ•¸ï¼š{len(documents)} â†’ åªä¿ç•™ç¬¬ä¸€é ï¼š{len(first_page_docs)} é \")\n",
    "    return first_page_docs\n",
    "\n",
    "\n",
    "# åŸ·è¡Œ\n",
    "csv_path = \"/Users/mangtinglee/Desktop/2025_gap_careerpath/RAG_LLM/meta_data_correction.csv\"\n",
    "\n",
    "# è¼‰å…¥\n",
    "all_abstracts_with_metadata = load_all_first_pages_with_csv_metadata(folder_path, csv_path)\n",
    "\n",
    "# æª¢æŸ¥çµæœ\n",
    "for i, doc in enumerate(all_abstracts_with_metadata[:3]):\n",
    "    print(f\"\\n--- Document {i+1} ---\")\n",
    "    print(f\"Title: {doc.metadata.get('title', 'N/A')}\")\n",
    "    print(f\"Year: {doc.metadata.get('year', 'N/A')}\")\n",
    "    print(f\"Authors: {doc.metadata.get('authors', 'N/A')}\")\n",
    "    print(f\"Topic: {doc.metadata.get('topic', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬äºŒæ­¥æ¸¬è©¦ï¼šåˆ†å‰²æª”æ¡ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "# æ¸¬è©¦2. åˆ†å‰²æª”æ¡ˆ\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=50,\n",
    "    separators=[ \"\\n\\n\", \". \", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
    "    )    \n",
    "docs = text_splitter.split_documents(all_abstracts)\n",
    "print(len(docs)) # chunks\n",
    "print(len(all_abstracts)) # test 11 pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-05-12T00:55:20+00:00', 'author': '', 'keywords': '', 'moddate': '2025-05-12T00:55:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/mangtinglee/Desktop/2025_gap_careerpath/RAG_LLM/pdfs/2025_LLM limitation_The Order Effect- Investigating Prompt Sensitivity to Input Order in LLMs.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "print(docs[10].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ‰€ä»¥æˆ‘çš„ç†è§£æ˜¯é€™ä¸€å¤§æ®µè½å…¶å¯¦æ˜¯å…ˆæŠ“å–åŸå§‹documentsï¼ˆå·²ç¶“éloaderä½†é‚„æ²’splitsçš„æª”æ¡ˆï¼‰ï¼Œå…ˆç”¨æ­£å‰‡è¾¨è­˜å‡ºå¤§æ¦‚çš„æ¨™é¡Œsection_headersï¼Œåœ¨ç”¨llmå»è¾¨è­˜section_headerså±¬æ–¼å“ªç¨®åˆ†é¡å—ï¼Ÿ\n",
    "å› ç‚ºæˆ‘çš„æª”æ¡ˆæœ‰äº›æ¨™é¡Œç„¡æ³•è¾¨è­˜ä»–æ˜¯ç ”ç©¶çµæœæˆ–æ–¹æ³•ï¼Œä¾‹å¦‚5 Calibrating LLMs for MCQ Tasksï¼Œæ‰€ä»¥æˆ‘çš„æƒ³æ³•æ˜¯ï¼Œæˆ‘å€‘æ˜¯å¦åˆ°æ™‚å€™å¯ä»¥ç›´æ¥å…ˆåˆ†å¡Šï¼Œå‡è¨­åˆ†äº†54å¡Šï¼Œæˆ‘å€‘å†è«‹llmè®€å–54å¡Šï¼Œç„¶å¾Œè®“ä»–è²¼æ¨™ï¼ˆè¤‡æ•¸ï¼‰ï¼Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬ä¸‰æ­¥æ¸¬è©¦ï¼šembedding ä¸¦æ”¾å…¥è³‡æ–™åº«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finishï¼\n",
      "è³‡æ–™å¤¾å­˜åœ¨å—ï¼ŸTrue\n"
     ]
    }
   ],
   "source": [
    "# å»ºç«‹è³‡æ–™åº«è·¯å¾‘ï¼Œå·²æœ‰è·¯å¾‘å‰‡å¯å¿½ç•¥\n",
    "import os\n",
    "\n",
    "# å»ºç«‹è³‡æ–™å¤¾\n",
    "os.makedirs('./chroma_db', exist_ok=True)\n",
    "print(\"finishï¼\")\n",
    "\n",
    "# æª¢æŸ¥æ˜¯å¦æˆåŠŸ\n",
    "print(f\"è³‡æ–™å¤¾å­˜åœ¨å—ï¼Ÿ{os.path.exists('./chroma_db')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define embedding\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ³¨æ„ï¼Œéœ€è¦å…ˆåœ¨è‡ªå·±çš„ç’°å¢ƒä¸­å»ºç«‹è³‡æ–™åº«è·¯å¾‘\n",
    "persist_directory = './chroma_db' # æŒ‡å®šè³‡æ–™åº«è·¯å¾‘\n",
    "!rm -rf ./chroma_db  # remove old database files if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹æ–°çš„å‘é‡è³‡æ–™åº«ï¼Œä¸¦å°‡æ–‡ä»¶æ”¾é€²å»\n",
    "from langchain.vectorstores import Chroma\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      ". Many of these issues, such as bias (Talat et al., 2022; Motoki\n",
      "et al., 2023), hallucination (Chen et al., 2023; Sadat et al., 2023), consistency (Tam et al., 2023; Ye\n",
      "et al., 2023), and reliability (Shen et al., 2023b) have been extensively discussed in the literature.\n",
      "However, a more fundamental challenge to the long-term success of LLMs is their ability to reason:\n",
      "the distinguishing factor between probabilistic pattern matching and logical understanding. This\n",
      "distinction has significant implications for the future of LLMs and how we employ these models in\n",
      "decision-making.\n",
      "One necessary requirement for reasoning is order independence. A model should provide the same\n",
      "consistent response to a query regardless of the order of its content. Historically, LLMs have strug-\n",
      "gled with this issue. Swapping subsequences within semantically identical inputs often leads to\n",
      "significant changes in output, a problem that worsens as inputs grow in size and complexity (He\n",
      "et al., 2024)\n"
     ]
    }
   ],
   "source": [
    "question = \"can you explain llm limitation?\"\n",
    "ans_docs = vectordb.similarity_search(question,k=3)\n",
    "print(len(ans_docs))\n",
    "print(ans_docs[2].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_x/xcmm2w3d1hld6_nc6t0xyrch0000gn/T/ipykernel_93325/2355425439.py:2: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "# æ‰‹å‹•å„²å­˜å‰›å‰›å»ºç«‹çš„è³‡æ–™åº«\n",
    "vectordb.persist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Prompt Report: A Systematic Survey of Prompt Engineering\\nTechniques\\nSander Schulhoff1,2âˆ— Michael Ilie1âˆ—Nishant Balepur1 Konstantine Kahadze1\\nAmanda Liu1 Chenglei Si4 Yinheng Li5 Aayush Gupta1 HyoJung Han1 Sevien Schulhoff1\\nPranav Sandeep Dulepet1 Saurav Vidyadhara1 Dayeon Ki1 Sweta Agrawal12 Chau Pham13\\nGerson Kroiz Feileen Li 1 Hudson Tao1 Ashay Srivastava1 Hevander Da Costa1 Saloni Gupta1\\nMegan L'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define retriever æ”¹æˆä½¿ç”¨mmr\n",
    "retriever = vectordb.as_retriever(\n",
    "    search_type=\"mmr\", \n",
    "    search_kwargs={\n",
    "        \"k\": 3,\n",
    "        \"fetch_k\":20\n",
    "        }\n",
    "    )\n",
    "question = \"is there any author named Jean Kaddour ?\"\n",
    "docs_mmr = vectordb.max_marginal_relevance_search(question,k=3)\n",
    "docs_mmr[2].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SPANISH AND LLM B ENCHMARKS : IS MMLU L OST IN\\nTRANSLATION ?\\nA PREPRINT\\nIrene Plaza, Nina Melero, Cristina del Pozo, Javier Conde and Pedro Reviriego\\nETSI de TelecomunicaciÃ³n\\nUniversidad PolitÃ©cnica de Madrid\\n28040 Madrid, Spain\\nMarina Mayor-Rocher\\nFacultad de FilosofÃ­a y Letras\\nUniversidad AutÃ³noma de Madrid\\n28049 Madrid, Spain\\nMarÃ­a Grandury\\nSomosNLP\\n24402, Ponferrada, Spain\\nJune 27, 2024\\nABSTRACT\\nThe evaluation of Large Language Models (LLMs) is a key element in their continuous improvement\\nprocess and many benchmarks have been developed to assess the performance of LLMs in different\\ntasks and topics. As LLMs become adopted worldwide, evaluating them in languages other than\\nEnglish is increasingly important. However, most LLM benchmarks are simply translated using\\nan automated tool and then run in the target language. This means that the results depend not\\nonly on the LLM performance in that language but also on the quality of the translation'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_mmr[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. . . . . . 36\\n3.3 Computer Programming . . . . . 37\\n*Equal contribution.\\nâ€ {jean.kaddour,robert.mchardy}.20@ucl.ac.uk,\\njoshua.harris@ukhsa.gov.uk\\nDesign\\nUnfathomable \\nDatasets, \\nTokenizer-Reliance,\\nFine-Tuning \\nOverhead\\nScience\\n \\nEvaluations \\nBased \\non \\nStatic \\nHuman-Written \\nGround \\nTruth,\\nLacking \\nExperimental \\nDesigns,\\nLack \\nof \\nReproducibility\\nBehavior\\nPrompt \\nBrittleness, \\nMisaligned \\nBehavior,\\nOutdated \\nKnowledge\\nDetecting \\nGenerated \\nTexts, \\nBrittle \\nEvaluations\\nHigh \\nPre-Training \\nCosts\\nHigh \\nInference \\nLatency, \\nLimited \\nContext \\nLength, \\nHallucinations\\nTasks \\nNot \\nSolvable\\nBy \\nScale\\nFigure 1: Overview of LLM Challenges. Designing\\nLLMs relates to decisions taken before deployment. Be-\\nhaviorial challenges occur during deployment. Science\\nchallenges hinder academic progress.\\n3.4 Creative Work . . . . . . . . . . . 39\\n3.5 Knowledge Work . . . . . . . . . 40\\n3.6 Law . . . . . . . . . . . . . . . . 42\\n3.7 Medicine . . . . . . . . . . . . . 43\\n3.8 Reasoning . . . . . . . . . . . .'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_mmr[2].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_x/xcmm2w3d1hld6_nc6t0xyrch0000gn/T/ipykernel_93325/1656567243.py:12: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model_name=llm_name, temperature=0)\n",
      "/var/folders/_x/xcmm2w3d1hld6_nc6t0xyrch0000gn/T/ipykernel_93325/1656567243.py:13: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  llm.predict(\"Hello world!\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# é¸æ“‡æ¨¡å‹\n",
    "import datetime\n",
    "current_date = datetime.datetime.now().date()\n",
    "if current_date < datetime.date(2023, 9, 2):\n",
    "    llm_name = \"gpt-3.5-turbo-0301\"\n",
    "else:\n",
    "    llm_name = \"gpt-3.5-turbo\"\n",
    "print(llm_name)\n",
    "\n",
    "# åˆå§‹åŒ–èŠå¤©æ©Ÿå™¨äººè¦ç”¨åˆ°çš„llm\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=llm_name, temperature=0)\n",
    "llm.predict(\"Hello world!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_x/xcmm2w3d1hld6_nc6t0xyrch0000gn/T/ipykernel_93325/2717514122.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "# æ–°é …ç›®ï¼šè¼‰å…¥ConversationBufferMemory\n",
    "# æ­¤å¥—ä»¶èƒ½è®“å•ç­”æ©Ÿå™¨äººè¨˜ä½éå¾€æ­·å²å•ç­”\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\", # å‘Šè¨´ chain å»å“ªè£¡æ‰¾æ­·å²å°è©±\n",
    "    return_messages=True, # è¿”å›çš„æ˜¯ç‰©ä»¶ï¼ˆé•·å¾—åƒjson or meta dataï¼‰\n",
    "    output_key=\"answer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "        llm=ChatOpenAI(model_name=llm_name, temperature=0), \n",
    "        chain_type=\"map_reduce\", \n",
    "        retriever=retriever, \n",
    "        return_source_documents=True,\n",
    "        return_generated_question=True,\n",
    "        memory=memory\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation methods for Large Language Models (LLMs) include using benchmarks to assess performance in tasks such as common sense reasoning problems and mathematical questions. Some benchmarks evaluate multiple tasks to provide a more comprehensive evaluation of LLM capabilities. However, evaluations for LLMs are based on static human-written ground truth, which has been criticized for lacking experimental designs and reproducibility. Additionally, evaluations suffer from prompt brittleness, misaligned behavior, outdated knowledge, and challenges in detecting generated texts and conducting evaluations due to various factors like high pre-training costs, high inference latency, limited context length, and hallucinations.\n"
     ]
    }
   ],
   "source": [
    "question = \"What evaluation methods are used for LLMs?\"\n",
    "result = qa({\"question\": question})\n",
    "print(result['answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The names of the references for the limitations of Large Language Models (LLMs) mentioned earlier are Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, Robert McHardy, Zhao et al., 2021, and Wang et al., 2023b.\n"
     ]
    }
   ],
   "source": [
    "question = \"can you show me the reference's name of those three llm's limitations you gave me before?\"\n",
    "result = qa({\"question\": question})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
